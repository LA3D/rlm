[
  {
    "objectID": "shacl_examples.html",
    "href": "shacl_examples.html",
    "title": "SHACL Examples and Shape Indexing",
    "section": "",
    "text": "Detect sh:SPARQLExecutable content in RDF graphs.\n\n\n\n\ndef detect_sparql_executables(\n    graph:Graph\n)-&gt;dict:\n\nDetect sh:SPARQLExecutable content in a graph.\nArgs: graph: RDF graph to analyze\nReturns: Dict with: has_executables: True if any SPARQLExecutable found select_count: Count of sh:SPARQLSelectExecutable instances construct_count: Count of sh:SPARQLConstructExecutable instances ask_count: Count of sh:SPARQLAskExecutable instances total_count: Total query count\n\n\n\n\n\ndef QueryIndex(\n    queries:List=&lt;factory&gt;, comments:Dict=&lt;factory&gt;, keywords:Dict=&lt;factory&gt;, endpoints:Dict=&lt;factory&gt;,\n    query_text:Dict=&lt;factory&gt;, query_type:Dict=&lt;factory&gt;, source_file:Dict=&lt;factory&gt;\n)-&gt;None:\n\nIndex of sh:SPARQLExecutable query templates for retrieval.\nAttributes: queries: List of query URIs comments: Mapping from query URI to rdfs:comment description keywords: Inverted index from keyword to query URIs endpoints: Mapping from query URI to target endpoint URLs query_text: Mapping from query URI to sh:select/sh:construct text query_type: Mapping from query URI to type (‘select’, ‘construct’, ‘ask’) source_file: Mapping from query URI to source file path",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#query-template-detection",
    "href": "shacl_examples.html#query-template-detection",
    "title": "SHACL Examples and Shape Indexing",
    "section": "",
    "text": "Detect sh:SPARQLExecutable content in RDF graphs.\n\n\n\n\ndef detect_sparql_executables(\n    graph:Graph\n)-&gt;dict:\n\nDetect sh:SPARQLExecutable content in a graph.\nArgs: graph: RDF graph to analyze\nReturns: Dict with: has_executables: True if any SPARQLExecutable found select_count: Count of sh:SPARQLSelectExecutable instances construct_count: Count of sh:SPARQLConstructExecutable instances ask_count: Count of sh:SPARQLAskExecutable instances total_count: Total query count\n\n\n\n\n\ndef QueryIndex(\n    queries:List=&lt;factory&gt;, comments:Dict=&lt;factory&gt;, keywords:Dict=&lt;factory&gt;, endpoints:Dict=&lt;factory&gt;,\n    query_text:Dict=&lt;factory&gt;, query_type:Dict=&lt;factory&gt;, source_file:Dict=&lt;factory&gt;\n)-&gt;None:\n\nIndex of sh:SPARQLExecutable query templates for retrieval.\nAttributes: queries: List of query URIs comments: Mapping from query URI to rdfs:comment description keywords: Inverted index from keyword to query URIs endpoints: Mapping from query URI to target endpoint URLs query_text: Mapping from query URI to sh:select/sh:construct text query_type: Mapping from query URI to type (‘select’, ‘construct’, ‘ask’) source_file: Mapping from query URI to source file path",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#query-keyword-extraction",
    "href": "shacl_examples.html#query-keyword-extraction",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Query Keyword Extraction",
    "text": "Query Keyword Extraction\nExtract searchable keywords from query templates.\n\n\nextract_query_keywords\n\ndef extract_query_keywords(\n    graph:Graph, query_uri:URIRef, comment:str\n)-&gt;List:\n\nExtract keywords from schema:keywords and rdfs:comment.\nSources: - schema:keywords (explicit tags) - rdfs:comment (word extraction) - Query URI local name\nArgs: graph: RDF graph containing the query query_uri: Query URI comment: rdfs:comment text\nReturns: List of lowercase keywords",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#build-query-index",
    "href": "shacl_examples.html#build-query-index",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Build Query Index",
    "text": "Build Query Index\nBuild searchable index from sh:SPARQLExecutable templates.\n\n\nbuild_query_index\n\ndef build_query_index(\n    graph:Graph, source_path:str=None\n)-&gt;QueryIndex:\n\nBuild searchable index from sh:SPARQLExecutable templates.\nExtracts: sh:select, sh:construct, sh:ask, rdfs:comment, schema:keywords, schema:target\nArgs: graph: RDF graph containing query templates source_path: Optional source file path for tracking\nReturns: QueryIndex with indexed queries",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#query-bounded-views",
    "href": "shacl_examples.html#query-bounded-views",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Query Bounded Views",
    "text": "Query Bounded Views\nProgressive disclosure functions for exploring query templates.\n\n\nsearch_queries\n\ndef search_queries(\n    index:QueryIndex, keyword:str, limit:int=5\n)-&gt;list:\n\nFind query templates matching keyword.\nArgs: index: QueryIndex to search keyword: Search term limit: Maximum number of results\nReturns: List of dicts with: uri, comment, endpoints, matched_keyword\n\n\n\ndescribe_query\n\ndef describe_query(\n    index:QueryIndex, query_uri:str\n)-&gt;dict:\n\nGet bounded description of a query template.\nArgs: index: QueryIndex to query query_uri: URI of query\nReturns: Dict with: uri, comment, endpoints, query_type, keywords, query_preview (200 chars)\n\n\n\nget_query_text\n\ndef get_query_text(\n    index:QueryIndex, query_uri:str\n)-&gt;str:\n\nGet full SPARQL query text for execution.\nArgs: index: QueryIndex to query query_uri: URI of query\nReturns: Full SPARQL query text\n\n\n\nload_query_examples\n\ndef load_query_examples(\n    path:str, ns:dict, name:str='queries'\n)-&gt;str:\n\nLoad SPARQL example files from directory into QueryIndex.\nRecursively loads all .ttl files and builds combined index.\nArgs: path: Directory path containing .ttl example files ns: Namespace dict for storing the index name: Variable name for the index in ns\nReturns: Status message",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#shaclindex-dataclass",
    "href": "shacl_examples.html#shaclindex-dataclass",
    "title": "SHACL Examples and Shape Indexing",
    "section": "SHACLIndex Dataclass",
    "text": "SHACLIndex Dataclass\nThe SHACLIndex holds indexed SHACL shapes for efficient retrieval.\n\n\nSHACLIndex\n\ndef SHACLIndex(\n    shapes:List=&lt;factory&gt;, targets:Dict=&lt;factory&gt;, properties:Dict=&lt;factory&gt;, keywords:Dict=&lt;factory&gt;,\n    paradigm:str='unknown'\n)-&gt;None:\n\nIndex of SHACL shapes for retrieval.\nAttributes: shapes: List of shape URIs targets: Mapping from shape URI to target class URIs properties: Mapping from shape URI to property constraint dicts keywords: Inverted index from keyword to shape URIs paradigm: SHACL usage paradigm (‘validation’, ‘shacl-first’, ‘mixed’)",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#shacl-detection",
    "href": "shacl_examples.html#shacl-detection",
    "title": "SHACL Examples and Shape Indexing",
    "section": "SHACL Detection",
    "text": "SHACL Detection\nDetect whether a graph contains SHACL shapes and determine the usage paradigm.\n\n\ndetect_shacl\n\ndef detect_shacl(\n    graph:Graph\n)-&gt;dict:\n\nDetect SHACL content in a graph.\nArgs: graph: RDF graph to analyze\nReturns: Dict with: has_shacl: True if any SHACL patterns found node_shapes: Count of sh:NodeShape instances property_shapes: Count of sh:PropertyShape instances paradigm: ‘validation’, ‘shacl-first’, or ‘mixed’",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#keyword-extraction",
    "href": "shacl_examples.html#keyword-extraction",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Keyword Extraction",
    "text": "Keyword Extraction\nExtract searchable keywords from shape metadata.\n\n\nextract_keywords\n\ndef extract_keywords(\n    graph:Graph, shape:URIRef, target_classes:List, props:List\n)-&gt;List:\n\nExtract searchable keywords from a shape.\nArgs: graph: RDF graph containing the shape shape: Shape URI target_classes: Target class URIs props: Property constraint dicts\nReturns: List of lowercase keywords",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#shape-index-building",
    "href": "shacl_examples.html#shape-index-building",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Shape Index Building",
    "text": "Shape Index Building\nBuild a searchable index from SHACL shapes in a graph.\n\n\nbuild_shacl_index\n\ndef build_shacl_index(\n    graph:Graph\n)-&gt;SHACLIndex:\n\nBuild searchable index from SHACL shapes in graph.\nArgs: graph: RDF graph containing SHACL shapes\nReturns: SHACLIndex with indexed shapes",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#bounded-view-functions",
    "href": "shacl_examples.html#bounded-view-functions",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Bounded View Functions",
    "text": "Bounded View Functions\nProgressive disclosure primitives for exploring SHACL shapes.\n\n\ndescribe_shape\n\ndef describe_shape(\n    index:SHACLIndex, shape_uri:str, limit:int=10\n)-&gt;dict:\n\nGet bounded description of a SHACL shape.\nArgs: index: SHACL index to query shape_uri: URI of shape to describe limit: Maximum number of properties to return\nReturns: Dict with: uri: Shape URI targets: List of target class URIs properties: First limit property constraints property_count: Total property count truncated: True if property list was truncated\n\n\n\nsearch_shapes\n\ndef search_shapes(\n    index:SHACLIndex, keyword:str, limit:int=5\n)-&gt;list:\n\nFind shapes matching keyword.\nArgs: index: SHACL index to search keyword: Search term limit: Maximum number of results\nReturns: List of dicts with: uri: Shape URI targets: Target class URIs matched_keyword: The keyword that matched\n\n\n\nshape_constraints\n\ndef shape_constraints(\n    index:SHACLIndex, shape_uri:str\n)-&gt;str:\n\nGet human-readable property constraints for a shape.\nArgs: index: SHACL index to query shape_uri: URI of shape\nReturns: Formatted string with property constraints",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#shacl-advanced-features-evaluation",
    "href": "shacl_examples.html#shacl-advanced-features-evaluation",
    "title": "SHACL Examples and Shape Indexing",
    "section": "SHACL Advanced Features Evaluation",
    "text": "SHACL Advanced Features Evaluation\nThis project uses SHACL-AF in two ways:\n\nsh:SPARQLRule - For inferencing/materialization rules (RDFS/OWL closure)\nsh:SPARQLExecutable - For reusable query templates with examples\n\n\nsh:SPARQLRule vs sh:SPARQLExecutable\n\n\n\n\n\n\n\n\nFeature\nsh:SPARQLRule\nsh:SPARQLExecutable\n\n\n\n\nPurpose\nInference/validation\nQuery templates\n\n\nAttached to\nsh:NodeShape via sh:rule\nStandalone resource\n\n\nQuery types\nCONSTRUCT (inference)\nSELECT/CONSTRUCT/ASK\n\n\nExample files\nrdfsplus.rule.ttl (6 rules)\nuniprot/examples (1,228+ queries)\n\n\n\nowlrl-shacl.ttl (13 rules)\n\n\n\n\ndatacube.shapes.ttl (1 rule)\n\n\n\n\n\n\nExisting SHACL-AF Content\nrdfsplus.rule.ttl - Basic RDFS inference: - Subclass transitivity - Type propagation via subclass - Subproperty transitivity - Property inclusion - Domain/range typing\nowlrl-shacl.ttl - OWL RL subset inference: - All RDFS rules plus - Equivalent class expansion - Inverse properties - Transitive properties - Symmetric properties - Functional properties → sameAs (optional) - Property chain axioms (length-2)\ndatacube.shapes.ttl - RDF Data Cube validation: - sh:SPARQLRule for copying component properties - sh:sparql constraints for integrity checking - Validates observations against data structure definitions\nSources: - SHACL Advanced Features - W3C SHACL GitHub - datacube.shapes.ttl - SIB Swiss SPARQL Examples Utils",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#queryindex-dataclass",
    "href": "shacl_examples.html#queryindex-dataclass",
    "title": "SHACL Examples and Shape Indexing",
    "section": "QueryIndex Dataclass",
    "text": "QueryIndex Dataclass\nThe QueryIndex holds indexed sh:SPARQLExecutable query templates for retrieval and reuse.",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#tests",
    "href": "shacl_examples.html#tests",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Tests",
    "text": "Tests\nBasic tests for SHACL detection and indexing.\n\n# Test detect_shacl with empty graph\ng = Graph()\nresult = detect_shacl(g)\nassert result['has_shacl'] == False\nassert result['paradigm'] == 'none'\nprint(\"✓ Empty graph detection works\")\n\n✓ Empty graph detection works\n\n\n\n# Test detect_shacl with a simple NodeShape\ng = Graph()\nEX = Namespace(\"http://example.org/\")\ng.add((EX.PersonShape, RDF.type, SH.NodeShape))\ng.add((EX.PersonShape, SH.targetClass, EX.Person))\nresult = detect_shacl(g)\nassert result['has_shacl'] == True\nassert result['node_shapes'] == 1\nassert result['paradigm'] == 'validation'\nprint(\"✓ NodeShape detection works\")\n\n✓ NodeShape detection works\n\n\n\n# Test build_shacl_index\ng = Graph()\nEX = Namespace(\"http://example.org/\")\ng.add((EX.PersonShape, RDF.type, SH.NodeShape))\ng.add((EX.PersonShape, SH.targetClass, EX.Person))\ng.add((EX.PersonShape, RDFS.label, Literal(\"Person Shape\")))\n\nindex = build_shacl_index(g)\nassert len(index.shapes) == 1\nassert str(EX.PersonShape) in index.shapes\nassert str(EX.Person) in index.targets[str(EX.PersonShape)]\nassert 'person' in index.keywords or 'personshape' in index.keywords\nprint(\"✓ Index building works\")\nprint(f\"  {index.summary()}\")\n\n✓ Index building works\n  SHACLIndex: 1 shapes, 3 keywords, paradigm=validation\n\n\n\n# Test search_shapes\nresults = search_shapes(index, 'person')\nassert len(results) &gt;= 1\nassert str(EX.PersonShape) == results[0]['uri']\nprint(\"✓ Shape search works\")\nprint(f\"  Found {len(results)} shapes for 'person'\")\n\n✓ Shape search works\n  Found 1 shapes for 'person'\n\n\n\n# Test describe_shape\ndesc = describe_shape(index, str(EX.PersonShape))\nassert desc['uri'] == str(EX.PersonShape)\nassert str(EX.Person) in desc['targets']\nprint(\"✓ Shape description works\")\nprint(f\"  Targets: {desc['targets']}\")\nprint(f\"  Property count: {desc['property_count']}\")\n\n✓ Shape description works\n  Targets: ['http://example.org/Person']\n  Property count: 0\n\n\n\n# Test with DCAT-AP shapes\nfrom pathlib import Path\ndcat_path = Path('../ontology/dcat-ap/dcat-ap-SHACL.ttl')\nif dcat_path.exists():\n    g_dcat = Graph()\n    g_dcat.parse(dcat_path)\n    \n    detection = detect_shacl(g_dcat)\n    print(f\"\\n✓ DCAT-AP detection: {detection['node_shapes']} node shapes, paradigm={detection['paradigm']}\")\n    \n    index_dcat = build_shacl_index(g_dcat)\n    print(f\"  {index_dcat.summary()}\")\n    \n    # Search for Dataset shape\n    dataset_shapes = search_shapes(index_dcat, 'dataset', limit=3)\n    print(f\"  Found {len(dataset_shapes)} shapes matching 'dataset':\")\n    for s in dataset_shapes[:3]:\n        shape_name = s['uri'].split('/')[-1].split('#')[-1]\n        print(f\"    - {shape_name}\")\nelse:\n    print(\"\\n(DCAT-AP shapes not found, skipping test)\")\n\n\n(DCAT-AP shapes not found, skipping test)",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "shacl_examples.html#demonstration-query-template-workflow",
    "href": "shacl_examples.html#demonstration-query-template-workflow",
    "title": "SHACL Examples and Shape Indexing",
    "section": "Demonstration: Query Template Workflow",
    "text": "Demonstration: Query Template Workflow\nThis demonstrates the ‘retrieve example → adapt → run’ workflow for discovering how to query unfamiliar SPARQL endpoints.\n\n# Load UniProt example queries from neXtProt\nfrom pathlib import Path\n\nnxp_path = Path('../ontology/uniprot/examples/neXtProt')\nif nxp_path.exists():\n    ns = {}\n    result = load_query_examples(str(nxp_path), ns, 'nxq')\n    print(result)\n    print(f\"\\nIndex summary: {ns['nxq'].summary()}\")\nelse:\n    print(f\"neXtProt examples not found at {nxp_path}\")\n\nneXtProt examples not found at ../ontology/uniprot/examples/neXtProt\n\n\n\n# Search for protein-related queries\nif 'nxq' in ns:\n    results = search_queries(ns['nxq'], 'protein', limit=3)\n    print(f\"Found {len(results)} protein-related queries:\\n\")\n    for r in results:\n        uri_short = r['uri'].split('/')[-1]\n        print(f\"  {uri_short}\")\n        print(f\"    Comment: {r['comment']}\")\n        print(f\"    Keyword: {r['matched_keyword']}\\n\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Search for protein-related queries\n----&gt; 2 if 'nxq' in ns:\n      3     results = search_queries(ns['nxq'], 'protein', limit=3)\n      4     print(f\"Found {len(results)} protein-related queries:\\n\")\n\nNameError: name 'ns' is not defined\n\n\nNameError: name 'ns' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-dc4b1e07ccd1&gt;\", line 2, in &lt;module&gt;\n    if 'nxq' in ns:\n                ^^\n\nNameError: name 'ns' is not defined\n\n\n\n# Describe a specific query\nif 'nxq' in ns and results:\n    query_uri = results[0]['uri']\n    desc = describe_query(ns['nxq'], query_uri)\n    \n    print(f\"Query: {query_uri.split('/')[-1]}\")\n    print(f\"Type: {desc['query_type']}\")\n    print(f\"Comment: {desc['comment'][:150]}...\")\n    if desc['endpoints']:\n        print(f\"Endpoint: {desc['endpoints'][0]}\")\n    print(f\"\\nKeywords: {', '.join(desc['keywords'][:5])}\")\n    print(f\"\\nQuery preview:\\n{desc['query_preview']}\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Describe a specific query\n----&gt; 2 if 'nxq' in ns and results:\n      3     query_uri = results[0]['uri']\n      4     desc = describe_query(ns['nxq'], query_uri)\n\nNameError: name 'ns' is not defined\n\n\nNameError: name 'ns' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-858a9fb03277&gt;\", line 2, in &lt;module&gt;\n    if 'nxq' in ns and results:\n                ^^\n\nNameError: name 'ns' is not defined\n\n\n\n# Get full query text for execution\nif 'nxq' in ns and results:\n    query_uri = results[0]['uri']\n    full_query = get_query_text(ns['nxq'], query_uri)\n    \n    print(f\"Full query ({len(full_query)} chars):\\n\")\n    print(full_query[:500])\n    if len(full_query) &gt; 500:\n        print(\"\\n... (truncated for display)\")\n    \n    print(\"\\n# This query could now be adapted and executed:\")\n    print(\"# sparql_query(full_query, endpoint=desc['endpoints'][0], name='results', ns=ns)\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Get full query text for execution\n----&gt; 2 if 'nxq' in ns and results:\n      3     query_uri = results[0]['uri']\n      4     full_query = get_query_text(ns['nxq'], query_uri)\n\nNameError: name 'ns' is not defined\n\n\nNameError: name 'ns' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-54dffe882117&gt;\", line 2, in &lt;module&gt;\n    if 'nxq' in ns and results:\n                ^^\n\nNameError: name 'ns' is not defined\n\n\n\n# Search for PTM-related queries (phosphorylation)\nif 'nxq' in ns:\n    ptm_results = search_queries(ns['nxq'], 'phosphorylation', limit=5)\n    print(f\"Found {len(ptm_results)} phosphorylation-related queries:\\n\")\n    for r in ptm_results:\n        uri_short = r['uri'].split('/')[-1]\n        comment_short = r['comment'][:80] + '...' if len(r['comment']) &gt; 80 else r['comment']\n        print(f\"  • {uri_short}: {comment_short}\")\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Search for PTM-related queries (phosphorylation)\n----&gt; 2 if 'nxq' in ns:\n      3     ptm_results = search_queries(ns['nxq'], 'phosphorylation', limit=5)\n      4     print(f\"Found {len(ptm_results)} phosphorylation-related queries:\\n\")\n\nNameError: name 'ns' is not defined\n\n\nNameError: name 'ns' is not defined\nTraceback (most recent call last):\n\n  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  File \"&lt;ipython-input-1-3132b14d7a6b&gt;\", line 2, in &lt;module&gt;\n    if 'nxq' in ns:\n                ^^\n\nNameError: name 'ns' is not defined",
    "crumbs": [
      "SHACL Examples and Shape Indexing"
    ]
  },
  {
    "objectID": "test_memory_solveit.html",
    "href": "test_memory_solveit.html",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "",
    "text": "First, let’s import what we need and verify the modules load.\n\nfrom rlm.dataset import setup_dataset_context\nfrom rlm.core import rlm_run\nfrom rlm.ontology import setup_ontology_context\n\nprint(\"✓ Imports successful\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-1-import-and-setup",
    "href": "test_memory_solveit.html#step-1-import-and-setup",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "",
    "text": "First, let’s import what we need and verify the modules load.\n\nfrom rlm.dataset import setup_dataset_context\nfrom rlm.core import rlm_run\nfrom rlm.ontology import setup_ontology_context\n\nprint(\"✓ Imports successful\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-2-basic-dataset-setup",
    "href": "test_memory_solveit.html#step-2-basic-dataset-setup",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 2: Basic Dataset Setup",
    "text": "Step 2: Basic Dataset Setup\nCreate a dataset and verify it initializes correctly.\n\n# Create namespace\nns = {}\n\n# Setup dataset\nresult = setup_dataset_context(ns)\nprint(result)\n\n# Verify namespace has what we expect\nassert 'ds' in ns, \"Should have dataset\"\nassert 'ds_meta' in ns, \"Should have metadata\"\nassert 'mem_add' in ns, \"Should have mem_add function\"\nassert 'mem_query' in ns, \"Should have mem_query function\"\n\nprint(f\"✓ Dataset created with session_id: {ns['ds_meta'].session_id}\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-3-test-memory-operations-directly",
    "href": "test_memory_solveit.html#step-3-test-memory-operations-directly",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 3: Test Memory Operations Directly",
    "text": "Step 3: Test Memory Operations Directly\nBefore using with RLM, verify memory operations work.\n\n# Add a fact\nresult = ns['mem_add']('http://ex.org/alice', 'http://ex.org/age', '30',\n                       source='test', reason='Testing mem_add')\nprint(result)\n\n# Verify it was stored\nds_meta = ns['ds_meta']\nassert len(ds_meta.mem) == 1, \"Should have 1 triple in memory\"\nprint(f\"✓ Memory has {len(ds_meta.mem)} triple(s)\")\n\n# Verify provenance was recorded\nassert len(ds_meta.prov) &gt; 0, \"Should have provenance events\"\nprint(f\"✓ Provenance has {len(ds_meta.prov)} event(s)\")\n\n\n# Test query\nresults = ns['mem_query']('SELECT ?s ?age WHERE { ?s &lt;http://ex.org/age&gt; ?age }')\nprint(f\"Query results: {results}\")\n\nassert len(results) == 1, \"Should have 1 result\"\nassert results[0]['age'] == '30', \"Age should be 30\"\nprint(\"✓ Query works correctly\")\n\n\n# Test describe\ndesc = ns['mem_describe']('http://ex.org/alice')\nprint(f\"Description: {desc}\")\n\nassert len(desc['as_subject']) == 1, \"Should have 1 triple where alice is subject\"\nprint(\"✓ Describe works correctly\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-4-test-rlm-with-simple-memory-task",
    "href": "test_memory_solveit.html#step-4-test-rlm-with-simple-memory-task",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 4: Test RLM with Simple Memory Task",
    "text": "Step 4: Test RLM with Simple Memory Task\nNow let’s see if RLM can use memory operations.\n\n# Create fresh namespace for RLM test\nns_rlm = {}\nsetup_dataset_context(ns_rlm)\n\nprint(f\"Starting with {len(ns_rlm['ds_meta'].mem)} triples in memory\")\n\n\n# Give RLM a simple memory task\ncontext = \"\"\"\nYou have access to these memory operations:\n\n```python\n# Add a fact to memory\nmem_add(subject_uri, predicate_uri, object_value, source='agent', reason='...')\n\n# Query memory with SPARQL\nresults = mem_query('SELECT ?s ?p ?o WHERE { ?s ?p ?o }', limit=100)\n```\n\nStore facts as you discover them.\n\"\"\"\n\nquery = \"\"\"\nPlease store this fact in memory:\n- Subject: http://example.org/alice\n- Predicate: http://example.org/age  \n- Object: 30\n\nThen query memory to verify it was stored and tell me what you found.\n\"\"\"\n\nprint(\"Running RLM...\")\nprint(\"=\"*60)\n\n\n# Run RLM (requires API key)\nanswer, iterations, ns_rlm = rlm_run(\n    query=query,\n    context=context,\n    ns=ns_rlm,\n    max_iters=5\n)\n\nprint(\"=\"*60)\nprint(f\"Answer: {answer}\")\nprint(\"=\"*60)\n\n\n# Verify RLM used memory\nprint(f\"\\nMemory now has: {len(ns_rlm['ds_meta'].mem)} triples\")\nprint(f\"Provenance events: {len(ns_rlm['ds_meta'].prov)}\")\nprint(f\"Iterations used: {len(iterations)}\")\n\n# Show what was stored\nprint(\"\\nMemory contents:\")\nfor s, p, o in ns_rlm['ds_meta'].mem.triples((None, None, None)):\n    print(f\"  {s}\")\n    print(f\"    {p}\")\n    print(f\"      {o}\")\n\n\n# Check that RLM actually called memory operations\nprint(\"\\nCode blocks executed:\")\nfor i, iteration in enumerate(iterations):\n    print(f\"\\n--- Iteration {i} ---\")\n    for j, block in enumerate(iteration.code_blocks):\n        print(f\"\\nCode block {j}:\")\n        print(block.code)\n        \n        # Check if it used memory operations\n        if 'mem_add' in block.code:\n            print(\"  ✓ Used mem_add\")\n        if 'mem_query' in block.code:\n            print(\"  ✓ Used mem_query\")\n        \n        # Show result\n        if block.result.stdout:\n            print(f\"  Output: {block.result.stdout[:200]}\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-5-test-memory-persistence-across-runs",
    "href": "test_memory_solveit.html#step-5-test-memory-persistence-across-runs",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 5: Test Memory Persistence Across Runs",
    "text": "Step 5: Test Memory Persistence Across Runs\nVerify that memory persists when reusing the same namespace.\n\n# Remember how much memory we had\nmem_before = len(ns_rlm['ds_meta'].mem)\nprint(f\"Memory before second run: {mem_before} triples\")\n\n# Run again with same namespace\nquery2 = \"Query memory to find Alice's age. What did you find?\"\n\nanswer2, iterations2, ns_rlm = rlm_run(\n    query=query2,\n    context=context,\n    ns=ns_rlm,  # Reuse same namespace!\n    max_iters=5\n)\n\nprint(f\"\\nAnswer: {answer2}\")\nprint(f\"Memory after second run: {len(ns_rlm['ds_meta'].mem)} triples\")\nprint(f\"Session ID (same?): {ns_rlm['ds_meta'].session_id}\")\n\nassert len(ns_rlm['ds_meta'].mem) &gt;= mem_before, \"Memory should persist\"\nprint(\"✓ Memory persisted across runs\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-6-test-provenance-inspection",
    "href": "test_memory_solveit.html#step-6-test-provenance-inspection",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 6: Test Provenance Inspection",
    "text": "Step 6: Test Provenance Inspection\nLook at what provenance was recorded.\n\nfrom rdflib import RDF, Namespace\n\nRLM_PROV = Namespace('urn:rlm:prov:')\nds_meta = ns_rlm['ds_meta']\n\nprint(f\"Total provenance events: {len(ds_meta.prov)}\")\nprint(\"\\nProvenance details:\")\n\n# Show each event\nfor event in list(ds_meta.prov.subjects(RDF.type, None))[:5]:  # First 5\n    print(f\"\\nEvent: {event}\")\n    \n    # Get all properties of this event\n    for p, o in ds_meta.prov.predicate_objects(event):\n        pred_name = str(p).split('/')[-1]\n        print(f\"  {pred_name}: {o}\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-7-test-with-ontology-integration",
    "href": "test_memory_solveit.html#step-7-test-with-ontology-integration",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 7: Test with Ontology Integration",
    "text": "Step 7: Test with Ontology Integration\nNow test RLM using both ontology and memory together.\n\n# Check if ontology exists\nfrom pathlib import Path\n\nont_path = Path('ontology/prov.ttl')\nif ont_path.exists():\n    print(f\"✓ Found ontology at {ont_path}\")\nelse:\n    print(f\"⚠ Ontology not found at {ont_path}\")\n    print(\"Will skip ontology integration test\")\n\n\n# Setup namespace with both dataset and ontology\nif ont_path.exists():\n    ns_ont = {}\n    setup_dataset_context(ns_ont)\n    setup_ontology_context(str(ont_path), ns_ont, name='prov')\n    \n    print(f\"Dataset ready: {len(ns_ont['ds_meta'].mem)} triples in memory\")\n    print(f\"Ontology loaded: {len(ns_ont['prov_meta'].classes)} classes\")\n    print(f\"Ontology properties: {len(ns_ont['prov_meta'].properties)}\")\n\n\nif ont_path.exists():\n    context_ont = \"\"\"\nYou have access to:\n\n1. PROV Ontology (in prov_meta):\n   - prov_meta.classes - list of class URIs\n   - prov_meta.labels - dict of URI -&gt; label\n   - search_by_label(text) - find entities by label\n   - describe_entity(uri) - get entity details\n\n2. Memory operations:\n   - mem_add(subject, predicate, object, source='agent', reason='...')\n   - mem_query(sparql)\n\nUse the ontology to understand concepts, then store notes in memory.\n\"\"\"\n    \n    query_ont = \"\"\"\nFind the Activity class in the PROV ontology.\nGet its label and comment.\nStore a note about it in memory using:\n  subject: the Activity URI\n  predicate: http://example.org/myNote\n  object: your summary\nThen query memory to verify.\n\"\"\"\n    \n    print(\"Running RLM with ontology + memory...\")\n    print(\"=\"*60)\n\n\nif ont_path.exists():\n    answer_ont, iters_ont, ns_ont = rlm_run(\n        query=query_ont,\n        context=context_ont,\n        ns=ns_ont,\n        max_iters=8\n    )\n    \n    print(\"=\"*60)\n    print(f\"Answer: {answer_ont}\")\n    print(\"=\"*60)\n    \n    print(f\"\\nMemory: {len(ns_ont['ds_meta'].mem)} triples\")\n    print(f\"Iterations: {len(iters_ont)}\")\n    \n    # Show what was stored\n    print(\"\\nMemory contents:\")\n    for s, p, o in ns_ont['ds_meta'].mem.triples((None, None, None)):\n        print(f\"  Subject: {s}\")\n        print(f\"  Predicate: {p}\")\n        print(f\"  Object: {o[:100]}...\")  # Truncate long objects",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-8-test-work-graphs",
    "href": "test_memory_solveit.html#step-8-test-work-graphs",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 8: Test Work Graphs",
    "text": "Step 8: Test Work Graphs\nTest the scratch graph workflow.\n\n# Create fresh namespace\nns_work = {}\nsetup_dataset_context(ns_work)\n\n# Test work graph operations directly\nuri, graph = ns_work['work_create']('test_task')\nprint(f\"Created work graph: {uri}\")\n\n# Add something to it\nfrom rdflib import URIRef, Literal\ngraph.add((URIRef('http://ex.org/temp'), URIRef('http://ex.org/value'), Literal('42')))\nprint(f\"Work graph has {len(graph)} triples\")\n\n# Promote to mem\nresult = ns_work['work_to_mem']('test_task', reason='Test promotion')\nprint(result)\nprint(f\"Memory now has {len(ns_work['ds_meta'].mem)} triples\")\n\n# Cleanup\nresult = ns_work['work_cleanup'](task_id='test_task')\nprint(result)\nprint(f\"Work graphs remaining: {len(ns_work['ds_meta'].work_graphs)}\")\n\nprint(\"✓ Work graph workflow works\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-9-test-bounded-views",
    "href": "test_memory_solveit.html#step-9-test-bounded-views",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 9: Test Bounded Views",
    "text": "Step 9: Test Bounded Views\nTest the inspection functions.\n\n# Using namespace from earlier tests\nif 'ns_rlm' in locals() and len(ns_rlm['ds_meta'].mem) &gt; 0:\n    test_ns = ns_rlm\nelse:\n    # Create test data\n    test_ns = {}\n    setup_dataset_context(test_ns)\n    test_ns['mem_add']('http://ex.org/alice', 'http://ex.org/age', '30')\n    test_ns['mem_add']('http://ex.org/bob', 'http://ex.org/age', '25')\n\n# Test dataset_stats\nstats = test_ns['dataset_stats']()\nprint(\"Dataset Stats:\")\nprint(stats)\nprint()\n\n\n# Test list_graphs\ngraphs = test_ns['list_graphs']()\nprint(f\"All graphs ({len(graphs)}):\")\nfor uri, count in graphs:\n    print(f\"  {uri}: {count} triples\")\n\n\n# Test graph_sample\nmem_uri = f\"urn:rlm:{test_ns['ds_meta'].name}:mem\"\nsample = test_ns['graph_sample'](mem_uri, limit=5)\nprint(f\"\\nMemory sample ({len(sample)} triples):\")\nfor s, p, o in sample:\n    print(f\"  {s} -&gt; {p} -&gt; {o}\")",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#step-10-test-snapshots",
    "href": "test_memory_solveit.html#step-10-test-snapshots",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Step 10: Test Snapshots",
    "text": "Step 10: Test Snapshots\nTest save and restore functionality.\n\nimport tempfile\nimport os\n\n# Create test data\nns_snap = {}\nsetup_dataset_context(ns_snap)\nns_snap['mem_add']('http://ex.org/alice', 'http://ex.org/age', '30')\nns_snap['mem_add']('http://ex.org/bob', 'http://ex.org/age', '25')\n\nprint(f\"Before snapshot: {len(ns_snap['ds_meta'].mem)} triples\")\n\n# Save snapshot\nsnapshot_path = tempfile.mktemp(suffix='.trig')\nresult = ns_snap['snapshot_dataset'](path=snapshot_path)\nprint(f\"\\n{result}\")\nprint(f\"File size: {os.path.getsize(snapshot_path)} bytes\")\n\n\n# Show snapshot contents\nprint(\"\\nSnapshot preview (first 30 lines):\")\nwith open(snapshot_path, 'r') as f:\n    for i, line in enumerate(f):\n        if i &gt;= 30:\n            break\n        print(f\"  {line.rstrip()}\")\n\n\n# Restore snapshot\nfrom rlm.dataset import load_snapshot\n\nns_restored = {}\nresult = load_snapshot(snapshot_path, ns_restored, name='restored')\nprint(f\"\\n{result}\")\n\nprint(f\"Restored memory: {len(ns_restored['restored_meta'].mem)} triples\")\nprint(f\"Restored provenance: {len(ns_restored['restored_meta'].prov)} events\")\n\n# Verify contents match\nassert len(ns_restored['restored_meta'].mem) == len(ns_snap['ds_meta'].mem)\nprint(\"\\n✓ Snapshot restore successful\")\n\n# Cleanup\nos.unlink(snapshot_path)",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "test_memory_solveit.html#summary",
    "href": "test_memory_solveit.html#summary",
    "title": "Testing RLM Memory Step-by-Step",
    "section": "Summary",
    "text": "Summary\nThis notebook tested:\n✓ Step 1: Module imports\n✓ Step 2: Dataset setup\n✓ Step 3: Direct memory operations (add, query, describe)\n✓ Step 4: RLM using memory operations\n✓ Step 5: Memory persistence across RLM runs\n✓ Step 6: Provenance inspection\n✓ Step 7: Ontology + memory integration\n✓ Step 8: Work graph workflow\n✓ Step 9: Bounded view functions\n✓ Step 10: Snapshot/restore\nAll components working correctly!",
    "crumbs": [
      "Testing RLM Memory Step-by-Step"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLM",
    "section": "",
    "text": "This project implements the Recursive Language Model (RLM) architecture for querying RDF ontologies through progressive disclosure. The implementation follows the protocol from Zhang et al. (2025) while using claudette as the LLM backend.\nThe work is part of an ongoing investigation into how LLM agents can navigate large knowledge graphs without overwhelming their context windows. Rather than loading entire ontologies into the prompt, the agent iteratively explores through bounded REPL operations, delegating heavy summarization tasks to sub-LLMs.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "RLM",
    "section": "Background",
    "text": "Background\nThe RLM architecture addresses a fundamental tension in using LLMs for knowledge graph tasks: ontologies and query results are often too large to fit in context, yet the model needs semantic understanding to construct correct queries.\nThe solution externalizes the large context to a REPL environment. The root LLM emits small code blocks that execute against the graph, receiving truncated results. When more detail is needed, it delegates to sub-LLMs via llm_query() calls that summarize specific chunks. The process iterates until the model returns a final answer.\nThis implementation extends RLM with two complementary memory systems and a four-layer context injection strategy:\nDataset memory (RDF quads) stores domain facts discovered during exploration. An RDF Dataset provides named graphs for working memory (mem), provenance tracking (prov), and scratch space (work/*). Facts persist across queries and can be snapshotted for session continuity.\nProcedural memory (ReasoningBank-style) stores reusable exploration strategies extracted from past trajectories. The system bootstraps with 7 universal strategies (describe entity, navigate hierarchy, find properties) stored as MemoryItem objects. After each RLM run, a judge evaluates success or failure, and an extractor distills new procedural insights. These are retrieved via BM25 for similar future tasks, allowing the agent to improve over time.\nStructured sense data provides compact ontology metadata (~600 chars) with 100% URI grounding validation. Instead of loading full ontologies into context, the system injects targeted sense cards with key classes, properties, and exploration hints—achieving 83% iteration reduction on entity queries.\nAdditional components include SPARQL result handles that expose metadata without materializing full result sets, SHACL shape indexing for schema discovery, and query template retrieval from sh:SPARQLExecutable examples.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#context-engineering-ont-sense-memory-based-architecture",
    "href": "index.html#context-engineering-ont-sense-memory-based-architecture",
    "title": "RLM",
    "section": "Context Engineering: Ont-Sense & Memory-Based Architecture",
    "text": "Context Engineering: Ont-Sense & Memory-Based Architecture\nTo enable effective ontology exploration, this implementation uses a four-layer context injection strategy that provides the LLM with just enough information without overwhelming its context window:\n\nLayer 0: Structured Sense Data\nOnt-Sense provides compact, programmatically-extracted ontology metadata with 100% URI grounding validation. Instead of loading full ontologies, the system injects ~600 character sense cards containing:\n\nKey classes and properties (with URIs)\nAvailable indexes (hierarchy, domains, ranges)\nLabel/description predicates\nQuick exploration hints\n\nThe sense card is auto-generated from GraphMeta scaffolding and validated to ensure all URIs exist in the ontology (zero hallucinations). Progressive disclosure automatically injects detailed sections (hierarchy overview, common patterns) when query keywords trigger them.\n\n\nLayer 1: General Strategies (Procedural Memory)\nUniversal exploration patterns are bootstrapped as procedural memories and retrieved via BM25 when relevant to the query. These 7 general strategies include:\n\nDescribe Entity by Label\nFind Subclasses/Superclasses Using GraphMeta\nFind Properties by Domain/Range\nPattern-Based Entity Search\nFind Relationship Paths\nNavigate Class Hierarchy from Roots\n\nThese strategies are stored as MemoryItem objects (not hardcoded), enabling the system to learn new patterns over time and update success rates based on actual performance.\n\n\nLayer 2: Ontology-Specific Recipes\nDomain-specific patterns (PROV Activity-Entity relationships, SIO measurement patterns) can be authored as Recipe objects and injected when working with specific ontologies. This layer is currently a placeholder, reserved for future ontology-specific guidance.\n\n\nLayer 3: Base Context\nGraphMeta summary and ontology statistics provide foundational context about triple counts, class/property distributions, and index availability.\n\n\nPerformance Results\nThis architecture achieves 83% iteration reduction on entity description queries:\n\nBaseline (no enhancements): 6 iterations\nWith sense + memory: 1 iteration\n\nThe four-layer approach maintains bounded context size (~1800 chars total) while providing targeted, relevant guidance for each query type.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "RLM",
    "section": "Installation",
    "text": "Installation\nThis project uses uv for package management with a shared environment:\nsource ~/uvws/.venv/bin/activate\nuv pip install fastcore claudette rdflib rank-bm25\nuv pip install -e .\nFor development, you also need nbdev:\nuv pip install nbdev\nnbdev_install_hooks",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "RLM",
    "section": "Example",
    "text": "Example\nThe following demonstrates loading an ontology into the dataset memory and using bounded view functions to explore it.\n\nfrom rlm.dataset import setup_dataset_context\n\n# Initialize dataset with mem/prov graphs\nns = {}\nsetup_dataset_context(ns)\nprint(ns['dataset_stats']())\n\nDataset 'ds' (session: ae4334a3)\nmem: 0 triples\nprov: 0 events\nwork graphs: 0\nonto graphs: 0\n\n\n\n# Mount an ontology (SHACL shapes are auto-indexed)\nns['mount_ontology']('ontology/dcat-ap/dcat-ap-SHACL.ttl', 'dcat')\n\n# The SHACL index is now available\nprint(ns['dcat_shacl'].summary())\n\n\nfrom rlm.shacl_examples import search_shapes, describe_shape\n\n# Search for shapes related to datasets\nresults = search_shapes(ns['dcat_shacl'], 'dataset', limit=3)\nfor r in results:\n    print(f\"{r['uri'].split('#')[-1]}: targets {r['targets']}\")\n\n\n# Get bounded description of a shape (first 10 properties)\ndesc = describe_shape(ns['dcat_shacl'], results[0]['uri'], limit=10)\nprint(f\"Properties: {desc['property_count']} (showing {len(desc['properties'])})\")\nfor p in desc['properties'][:5]:\n    print(f\"  {p['path'].split('/')[-1]}: min={p.get('minCount')}\")\n\nQuery templates can be loaded from SHACL-AF examples and searched by keyword:\n\nfrom rlm.shacl_examples import load_query_examples, search_queries, get_query_text\n\n# Load neXtProt SPARQL examples\nload_query_examples('ontology/uniprot/examples/neXtProt', ns, 'nxq')\nprint(ns['nxq'].summary())\n\n# Find queries about phosphorylation\nqueries = search_queries(ns['nxq'], 'phosphorylation', limit=2)\nfor q in queries:\n    print(f\"{q['uri'].split('/')[-1]}: {q['comment'][:60]}...\")",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#tutorial",
    "href": "index.html#tutorial",
    "title": "RLM",
    "section": "Tutorial",
    "text": "Tutorial\nFor a complete walkthrough with working examples, see 91_tutorial.ipynb. The tutorial demonstrates:\n\nCore RLM loop with llm_query() and rlm_run()\nOntology loading with bounded views\nProgressive disclosure over RDF graphs\nStructured sense data with 100% URI grounding\nFour-layer context injection (sense + memory + recipes + base)\nMemory-based general strategies and BM25 retrieval\nDataset memory for fact persistence\nSPARQL result handles\nProcedural memory closed loop (judge + extract)\nSHACL shape indexing\nMulti-ontology integration\n\nAll cells are executed with real Claude API calls showing actual outputs.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#testing",
    "href": "index.html#testing",
    "title": "RLM",
    "section": "Testing",
    "text": "Testing\nThe project includes a comprehensive test suite with 110+ tests covering all components:\ntests/\n├── unit/                    # Component-level tests\n│   ├── test_sparql_handles.py\n│   ├── test_session_tracking.py\n│   ├── test_memory_store.py\n│   ├── test_bootstrap_strategies.py      # NEW: Bootstrap validation\n│   ├── test_memory_recipe_separation.py  # NEW: Architecture separation\n│   └── test_sense_structured.py          # NEW: Sense data validation\n├── integration/             # Cross-component tests\n│   ├── test_dataset_memory.py\n│   ├── test_sparql_dataset.py\n│   ├── test_memory_closed_loop.py\n│   └── test_full_stack.py\n├── live/                    # API-required tests\n│   └── test_memory_integration.py        # NEW: Memory-based architecture\n└── test_quick_e2e.py        # End-to-end validation\n\nRunning Tests\n# Activate environment\nsource ~/uvws/.venv/bin/activate\n\n# Run unit tests (no API calls)\npytest tests/unit/ -v\n\n# Run integration tests (no API calls)\npytest tests/integration/ -v\n\n# Run live tests (requires ANTHROPIC_API_KEY)\nANTHROPIC_API_KEY=sk-... pytest tests/live/ -v\n\n# Run quick end-to-end test (with API calls)\npython tests/test_quick_e2e.py\n\n# Run notebook tests\nnbdev_test\nAll tests pass, validating: - Core RLM loop with Claude API - Ontology loading and exploration - Structured sense data with URI grounding ✅ - Bootstrap general strategies (7 universal patterns) ✅ - Memory-recipe separation validation ✅ - Four-layer context injection ✅ - Dataset memory persistence - SPARQL result handles - Procedural memory closed loop - SHACL shape indexing - End-to-end integration workflows\nSee tests/README.md for detailed test documentation.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#testing-1",
    "href": "index.html#testing-1",
    "title": "RLM",
    "section": "Testing",
    "text": "Testing\nThe project includes a comprehensive test suite with 100+ tests covering all components:\ntests/\n├── unit/                    # Component-level tests\n│   ├── test_sparql_handles.py\n│   ├── test_session_tracking.py\n│   └── test_memory_store.py\n├── integration/             # Cross-component tests\n│   ├── test_dataset_memory.py\n│   ├── test_sparql_dataset.py\n│   ├── test_memory_closed_loop.py\n│   └── test_full_stack.py\n└── test_quick_e2e.py        # End-to-end validation\n\nRunning Tests\n# Activate environment\nsource ~/uvws/.venv/bin/activate\n\n# Run all tests (no API calls)\npytest tests/unit/ tests/integration/ -v\n\n# Run quick end-to-end test (with API calls)\npython tests/test_quick_e2e.py\n\n# Run notebook tests\nnbdev_test\nAll tests pass, validating: - Core RLM loop with Claude API - Ontology loading and exploration - Dataset memory persistence - SPARQL result handles - Procedural memory closed loop - SHACL shape indexing - End-to-end integration workflows\nSee tests/README.md for detailed test documentation.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#status",
    "href": "index.html#status",
    "title": "RLM",
    "section": "Status",
    "text": "Status\nThis is preliminary research code under active development. The current implementation covers stages 1-5 of the trajectory:\n\nStage 1: Core RLM loop with claudette backend ✅\nStage 2: Bounded view primitives for progressive disclosure ✅\nStage 3: SPARQL handles with work-bound query execution ✅\nStage 4: SHACL shape indexing and query template retrieval ✅\nStage 5: Ont-Sense improvements & ReasoningBank integration ✅\n\nStructured sense data with 100% URI grounding\nFour-layer context injection (sense, memory, recipes, base context)\nMemory-based general strategies (bootstrap + learning)\nValidation pipeline and comprehensive test suite\n83% iteration reduction on entity queries\n\n\nStage 6 (evaluation framework) is in progress with task-based eval system in evals/.\nThe code is developed through exploratory programming in Jupyter notebooks using nbdev. It targets integration with the Solveit platform but can run standalone.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "RLM",
    "section": "References",
    "text": "References\n\nZhang, A., et al. (2025). Recursive Language Models. The reference implementation this project follows.\nWang, B., et al. (2025). ReasoningBank: Self-Evolving Procedural Knowledge for Adaptive Reasoning. Procedural memory approach for learning from trajectories.\nAnthropic. (2025). Building Effective Agents. Context engineering patterns for agentic systems.\nHoward, J. & Gugger, S. nbdev. Literate programming framework.\nHoward, J. claudette. Claude API wrapper used as the LLM backend.",
    "crumbs": [
      "RLM"
    ]
  },
  {
    "objectID": "ontology.html",
    "href": "ontology.html",
    "title": "ontology",
    "section": "",
    "text": "This module implements Stages 1-2 of the trajectory: Define the Ontology “Context Model” and provide bounded view primitives for progressive disclosure.\nStage 1: Meta-graph scaffolding with navigation indexes\nStage 2: Bounded view primitives for safe graph exploration\n\n\n\nHandles, not dumps: Return graph handles with bounded view operations\nMeta-graph scaffolding: Build navigation indexes (labels, hierarchy, properties)\nProgressive disclosure: Small summaries guide exploration\nRLM-compatible: Works with namespace-explicit rlm_run()\n\n\n\n\nFrom the trajectory document: &gt; The root model never gets a graph dump. It gets a handle name (e.g. ont, res_0) and uses bounded view operations.",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#overview",
    "href": "ontology.html#overview",
    "title": "ontology",
    "section": "",
    "text": "This module implements Stages 1-2 of the trajectory: Define the Ontology “Context Model” and provide bounded view primitives for progressive disclosure.\nStage 1: Meta-graph scaffolding with navigation indexes\nStage 2: Bounded view primitives for safe graph exploration\n\n\n\nHandles, not dumps: Return graph handles with bounded view operations\nMeta-graph scaffolding: Build navigation indexes (labels, hierarchy, properties)\nProgressive disclosure: Small summaries guide exploration\nRLM-compatible: Works with namespace-explicit rlm_run()\n\n\n\n\nFrom the trajectory document: &gt; The root model never gets a graph dump. It gets a handle name (e.g. ont, res_0) and uses bounded view operations.",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#imports",
    "href": "ontology.html#imports",
    "title": "ontology",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#graph-loading",
    "href": "ontology.html#graph-loading",
    "title": "ontology",
    "section": "Graph Loading",
    "text": "Graph Loading\n\n\nload_ontology\n\ndef load_ontology(\n    path:str | pathlib.Path, ns:dict, name:str='ont'\n)-&gt;str:\n\nLoad an RDF ontology file into namespace as a Graph handle.\nArgs: path: Path to ontology file (.ttl, .rdf, .owl) ns: Namespace dict where Graph will be stored name: Variable name for the Graph handle\nReturns: Summary string describing what was loaded\n\n# Test loading prov.ttl\ntest_ns = {}\nresult = load_ontology('ontology/prov.ttl', test_ns, name='prov_ont')\nprint(result)\nassert 'prov_ont' in test_ns\nassert isinstance(test_ns['prov_ont'], Graph)\nassert len(test_ns['prov_ont']) &gt; 0\nprint(f\"✓ Loaded {len(test_ns['prov_ont'])} triples\")\n\nLoaded 1664 triples from prov.ttl into 'prov_ont'\n✓ Loaded 1664 triples",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#meta-graph-navigation",
    "href": "ontology.html#meta-graph-navigation",
    "title": "ontology",
    "section": "Meta-Graph Navigation",
    "text": "Meta-Graph Navigation\nBuild navigation scaffolding from a Graph to enable progressive disclosure. This is what goes in the REPL environment, not the graph itself.\n\n\nGraphMeta\n\ndef GraphMeta(\n    graph:Graph, name:str='ont'\n)-&gt;None:\n\nMeta-graph navigation scaffolding for an RDF Graph.\nThis is REPL-resident and provides bounded views over the graph. Indexes discovered in dialogs/inspect_tools.ipynb exploration.\n\n# Test GraphMeta with prov ontology\nprov_g = test_ns['prov_ont']\nmeta = GraphMeta(prov_g, name='prov')\n\nprint(meta.summary())\nprint()\nprint(f\"Sample classes (first 5): {meta.classes[:5]}\")\nprint(f\"Sample properties (first 5): {meta.properties[:5]}\")\nprint(f\"Namespaces: {list(meta.namespaces.keys())}\")\n\nGraph 'prov': 1,664 triples\nClasses: 59\nProperties: 89\nIndividuals: 1\nNamespaces: brick, csvw, dc, dcat, dcmitype, dcterms, dcam, doap, foaf, geo, odrl, org, prof, qb, schema, sh, skos, sosa, ssn, time, vann, void, wgs, owl, rdf, rdfs, xsd, xml, prov\n\nSample classes (first 5): ['http://www.w3.org/2002/07/owl#Thing', 'http://www.w3.org/ns/prov#Accept', 'http://www.w3.org/ns/prov#Activity', 'http://www.w3.org/ns/prov#ActivityInfluence', 'http://www.w3.org/ns/prov#Agent']\nSample properties (first 5): ['http://www.w3.org/2000/01/rdf-schema#comment', 'http://www.w3.org/2000/01/rdf-schema#isDefinedBy', 'http://www.w3.org/2000/01/rdf-schema#label', 'http://www.w3.org/2000/01/rdf-schema#seeAlso', 'http://www.w3.org/2002/07/owl#topObjectProperty']\nNamespaces: ['brick', 'csvw', 'dc', 'dcat', 'dcmitype', 'dcterms', 'dcam', 'doap', 'foaf', 'geo', 'odrl', 'org', 'prof', 'qb', 'schema', 'sh', 'skos', 'sosa', 'ssn', 'time', 'vann', 'void', 'wgs', 'owl', 'rdf', 'rdfs', 'xsd', 'xml', 'prov']",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#bounded-view-functions-stage-1",
    "href": "ontology.html#bounded-view-functions-stage-1",
    "title": "ontology",
    "section": "Bounded View Functions (Stage 1)",
    "text": "Bounded View Functions (Stage 1)\nBasic operations on GraphMeta that return small, bounded summaries:\n\ngraph_stats(): Overall graph statistics\nsearch_by_label(): Simple label-based search\ndescribe_entity(): Get entity description with sample triples\n\nThese provide the foundation for progressive disclosure.\n\n\ngraph_stats\n\ndef graph_stats(\n    meta:GraphMeta\n)-&gt;str:\n\nGet graph statistics summary.\n\n\n\nsearch_by_label\n\ndef search_by_label(\n    meta:GraphMeta, search:str, limit:int=10\n)-&gt;list:\n\nSearch for entities by label substring (case-insensitive).\nBackward-compatible wrapper around search_entity().\nArgs: meta: GraphMeta to search search: Substring to search for in labels limit: Maximum results to return\nReturns: List of (URI, label) tuples\n\n\n\nsearch_entity\n\ndef search_entity(\n    meta:GraphMeta, query:str, limit:int=10, search_in:str='all'\n)-&gt;list:\n\nSearch for entities by label, IRI, or localname.\nArgs: meta: GraphMeta to search query: Search string (case-insensitive substring match) limit: Maximum results to return search_in: Where to search - ‘label’, ‘iri’, ‘localname’, or ‘all’\nReturns: List of dicts: [{‘uri’: str, ‘label’: str, ‘match_type’: str}, …]\n\n# Test search_entity\nresults = search_entity(meta, 'activity', limit=5)\nprint(f\"Found {len(results)} matches for 'activity':\")\nfor r in results:\n    print(f\"  {r['label']}: {r['uri']} ({r['match_type']})\")\n\n# Test different search modes\nprint(\"\\nSearch by IRI only:\")\niri_results = search_entity(meta, 'prov', search_in='iri', limit=3)\nfor r in iri_results:\n    print(f\"  {r['label']}: {r['uri']}\")\n\n# Test backward compatibility\nprint(\"\\nBackward compatibility test:\")\nlegacy_results = search_by_label(meta, 'activity', limit=5)\nprint(f\"Found {len(legacy_results)} matches using search_by_label():\")\nfor uri, label in legacy_results:\n    print(f\"  {label}: {uri}\")\n\nFound 5 matches for 'activity':\n  Activity: http://www.w3.org/ns/prov#Activity (label)\n  ActivityInfluence: http://www.w3.org/ns/prov#ActivityInfluence (label)\n  activity: http://www.w3.org/ns/prov#activity (label)\n  hadActivity: http://www.w3.org/ns/prov#hadActivity (label)\n  activityOfInfluence: http://www.w3.org/ns/prov#activityOfInfluence (label)\n\nSearch by IRI only:\n  Attribution: http://www.w3.org/ns/prov#Attribution\n  invalidatedAtTime: http://www.w3.org/ns/prov#invalidatedAtTime\n  Derivation: http://www.w3.org/ns/prov#Derivation\n\nBackward compatibility test:\nFound 5 matches using search_by_label():\n  Activity: http://www.w3.org/ns/prov#Activity\n  ActivityInfluence: http://www.w3.org/ns/prov#ActivityInfluence\n  activity: http://www.w3.org/ns/prov#activity\n  hadActivity: http://www.w3.org/ns/prov#hadActivity\n  activityOfInfluence: http://www.w3.org/ns/prov#activityOfInfluence\n\n\n\n\n\ndescribe_entity\n\ndef describe_entity(\n    meta:GraphMeta, uri:str, limit:int=20\n)-&gt;dict:\n\nGet bounded description of an entity.\nArgs: meta: GraphMeta containing the entity uri: URI of entity to describe (supports prefixed forms like ‘prov:Activity’) limit: Max number of triples to include\nReturns: Dict with label, types, and sample triples\n\n# Test describe_entity\n# Find the Activity class\nactivity_uri = 'http://www.w3.org/ns/prov#Activity'\ndesc = describe_entity(meta, activity_uri)\n\nprint(f\"Label: {desc['label']}\")\nprint(f\"Types: {desc['types']}\")\nprint(f\"Comment: {desc['comment'][:100]}...\" if desc['comment'] else \"No comment\")\nprint(f\"Outgoing triples: {len(desc['outgoing_sample'])}\")\n\nLabel: Activity\nTypes: ['http://www.w3.org/2002/07/owl#Class']\nNo comment\nOutgoing triples: 10\n\n\n\n\nStage 2: Progressive Disclosure Primitives\nAdvanced bounded view operations that enable root models to explore graphs iteratively:\n\nsearch_entity(): Multi-mode entity search (label/IRI/localname)\nprobe_relationships(): One-hop neighbor exploration with filtering\nfind_path(): BFS path finding between entities\npredicate_frequency(): Usage analysis for understanding graph structure\n\nThese primitives answer questions like: - “Is X defined?” → search_entity() - “What connects A to B?” → find_path() - “What are the most important predicates?” → predicate_frequency() - “What does X relate to?” → probe_relationships()\n\n\n\nprobe_relationships\n\ndef probe_relationships(\n    meta:GraphMeta, uri:str, predicate:str=None, direction:str='both', limit:int=20\n)-&gt;dict:\n\nGet one-hop neighbors of an entity, optionally filtered by predicate.\nArgs: meta: GraphMeta containing the entity uri: URI of entity to probe (supports prefixed forms like ‘prov:Activity’) predicate: Optional predicate URI to filter by (supports prefixed forms) direction: ‘out’, ‘in’, or ‘both’ (default: ‘both’) limit: Maximum neighbors to return per direction\nReturns: { ‘uri’: str, ‘label’: str, ‘outgoing’: [{‘predicate’: str, ‘pred_label’: str, ‘object’: str, ‘obj_label’: str}, …], ‘incoming’: [{‘subject’: str, ‘subj_label’: str, ‘predicate’: str, ‘pred_label’: str}, …], ‘outgoing_count’: int, ‘incoming_count’: int }\n\n\n\nfind_path\n\ndef find_path(\n    meta:GraphMeta, source:str, target:str, max_depth:int=2, limit:int=10\n)-&gt;list:\n\nFind predicates connecting two entities using BFS.\nAnswers “What predicates connect A to B?”\nArgs: meta: GraphMeta to search source: Source entity URI (supports prefixed forms like ‘prov:Activity’) target: Target entity URI (supports prefixed forms like ‘prov:Entity’) max_depth: Maximum path length (default: 2) limit: Maximum paths to return\nReturns: List of paths, each path is list of steps: [{‘from’: uri, ‘predicate’: uri, ‘to’: uri, ‘direction’: ‘out’|‘in’}, …]\n\n\n\npredicate_frequency\n\ndef predicate_frequency(\n    meta:GraphMeta, limit:int=20, predicate_type:str=None\n)-&gt;list:\n\nGet predicates ranked by frequency of use.\nArgs: meta: GraphMeta to analyze limit: Maximum predicates to return predicate_type: Optional filter - ‘object’, ‘datatype’, ‘annotation’\nReturns: List of dicts: [{‘predicate’: str, ‘label’: str, ‘count’: int, ‘sample_subject’: str, ‘sample_object’: str}, …]\n\n# Test probe_relationships\nactivity_uri = 'http://www.w3.org/ns/prov#Activity'\nprobe_result = probe_relationships(meta, activity_uri, limit=5)\n\nprint(f\"Probing: {probe_result['label']}\")\nprint(f\"Outgoing relationships: {probe_result['outgoing_count']} total, showing {len(probe_result['outgoing'])}\")\nfor rel in probe_result['outgoing'][:3]:\n    print(f\"  --{rel['pred_label']}--&gt; {rel['obj_label']}\")\n\nprint(f\"\\nIncoming relationships: {probe_result['incoming_count']} total, showing {len(probe_result['incoming'])}\")\nfor rel in probe_result['incoming'][:3]:\n    print(f\"  &lt;--{rel['pred_label']}-- {rel['subj_label']}\")\n\n# Test find_path\n# Find path between two PROV classes\nentity_uri = 'http://www.w3.org/ns/prov#Entity'\npaths = find_path(meta, activity_uri, entity_uri, max_depth=2, limit=3)\n\nprint(f\"\\n\\nPaths from Activity to Entity:\")\nif paths:\n    for i, path in enumerate(paths, 1):\n        print(f\"Path {i}:\")\n        for step in path:\n            direction_sym = '--&gt;' if step['direction'] == 'out' else '&lt;--'\n            pred_label = meta.labels.get(step['predicate'], step['predicate'])\n            print(f\"  {direction_sym} {pred_label}\")\nelse:\n    print(\"  No paths found\")\n\nProbing: Activity\nOutgoing relationships: 10 total, showing 5\n  --http://www.w3.org/1999/02/22-rdf-syntax-ns#type--&gt; http://www.w3.org/2002/07/owl#Class\n  --http://www.w3.org/2000/01/rdf-schema#isDefinedBy--&gt; W3C PROVenance Interchange Ontology (PROV-O)\n  --http://www.w3.org/2000/01/rdf-schema#label--&gt; Activity\n\nIncoming relationships: 34 total, showing 5\n  &lt;--http://www.w3.org/2000/01/rdf-schema#range-- activity\n  &lt;--http://www.w3.org/1999/02/22-rdf-syntax-ns#first-- n0fe42a034f254bbc9cc97fe482231e2cb5\n  &lt;--http://www.w3.org/2000/01/rdf-schema#domain-- endedAtTime\n\n\nPaths from Activity to Entity:\nPath 1:\n  --&gt; http://www.w3.org/2002/07/owl#disjointWith\n\n\n\n# Test predicate_frequency\nprint(\"Top 10 predicates by frequency:\")\nfreq_results = predicate_frequency(meta, limit=10)\nfor r in freq_results:\n    print(f\"  {r['count']:4d} uses - {r['label']}\")\n\n# Test filtering by predicate type\nprint(\"\\nTop 5 object properties:\")\nobj_props = predicate_frequency(meta, limit=5, predicate_type='object')\nfor r in obj_props:\n    print(f\"  {r['count']:4d} uses - {r['label']}\")\n\nTop 10 predicates by frequency:\n   184 uses - http://www.w3.org/2000/01/rdf-schema#isDefinedBy\n   175 uses - http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n   161 uses - http://www.w3.org/2000/01/rdf-schema#label\n   107 uses - http://www.w3.org/2000/01/rdf-schema#comment\n   104 uses - http://www.w3.org/ns/prov#category\n    85 uses - http://www.w3.org/ns/prov#component\n    64 uses - http://www.w3.org/2000/01/rdf-schema#domain\n    63 uses - http://www.w3.org/ns/prov#definition\n    60 uses - http://www.w3.org/2000/01/rdf-schema#range\n    55 uses - http://www.w3.org/2000/01/rdf-schema#subClassOf\n\nTop 5 object properties:\n     7 uses - wasDerivedFrom\n     3 uses - wasRevisionOf\n     3 uses - specializationOf",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#additional-exploration-functions",
    "href": "ontology.html#additional-exploration-functions",
    "title": "ontology",
    "section": "Additional Exploration Functions",
    "text": "Additional Exploration Functions\nFunctions discovered in dialogs/inspect_tools.ipynb for deeper ontology exploration.\n\n\nont_describe\n\ndef ont_describe(\n    ont:str, uri:str, name:str='desc', ns:dict=None, limit:int=100\n)-&gt;str:\n\nGet triples about a URI, store in namespace.\nReturns both triples where URI is subject and where it’s object.\nArgs: ont: Name of ontology variable in namespace uri: URI to describe name: Variable name for storing result ns: Namespace dict limit: Maximum triples to return per direction (default: 100)\nReturns: Summary string\n\n\n\nont_meta\n\ndef ont_meta(\n    ont:str, name:str='meta', ns:dict=None\n)-&gt;str:\n\nExtract ontology metadata (prefixes, annotation predicates, imports).\nArgs: ont: Name of ontology variable in namespace name: Variable name for storing result ns: Namespace dict\nReturns: Summary string\n\n\n\nont_roots\n\ndef ont_roots(\n    ont:str, name:str='roots', ns:dict=None\n)-&gt;str:\n\nFind root classes (no declared superclass), store in namespace.\nArgs: ont: Name of ontology variable in namespace name: Variable name for storing result ns: Namespace dict\nReturns: Summary string\n\n\n\nsetup_ontology_context\n\ndef setup_ontology_context(\n    path:str | pathlib.Path, ns:dict, name:str='ont', dataset_meta:NoneType=None\n)-&gt;str:\n\nLoad ontology and create meta-graph for RLM use.\nThis sets up both the Graph and GraphMeta in the namespace.\nNEW: Dataset integration - if dataset_meta provided, automatically mounts the ontology into the dataset as onto/ graph.\nArgs: path: Path to ontology file ns: Namespace dict name: Base name for graph handle dataset_meta: Optional DatasetMeta for auto-mounting\nReturns: Summary string",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#ontology-sense-building",
    "href": "ontology.html#ontology-sense-building",
    "title": "ontology",
    "section": "Ontology Sense Building",
    "text": "Ontology Sense Building\n\nWhat is a “Sense Document”?\nWhen an LLM needs to work with an ontology, loading the entire graph into context is wasteful and may exceed limits. Instead, we build a sense document - a compact summary that captures:\n\nFormalism: Which OWL/RDFS/SKOS constructs are used\nMetadata structure: Which annotation properties exist (labels, descriptions, etc.)\nDomain/scope: What the ontology is about\nNavigation hints: How to effectively search and traverse\n\nThis approach was developed through experiments in dialogs/inspect_tools.ipynb exploring progressive disclosure patterns.\n\n\nWhy Sense Building Matters\nDesign Decision Response (from ISSUE_ANALYSIS.md): &gt; GraphMeta.labels only uses rdfs:label - This is a limitation because different ontologies use different annotation properties: &gt; - rdfs:label, skos:prefLabel, skos:altLabel for labels &gt; - rdfs:comment, skos:definition, dcterms:description for descriptions\n&gt; - vann:preferredNamespacePrefix, owl:versionInfo for metadata\nRather than hardcode support for all possible properties, build_sense() detects which annotation properties this specific ontology uses, enabling intelligent search.\n\n\nReferences\n\nWidoco Metadata Guide - Recommended ontology metadata properties\nAnthropic: Building Effective Agents - Orchestrator-workers pattern\nAnthropic: Progressive Disclosure - Context engineering strategy\n\n\n\nImplementation Pattern\nThe sense-building workflow (not agentic): 1. Metadata collection - Extract prefixes, detect annotation predicates, find ontology-level metadata 2. Structural exploration - Build hierarchy, property signatures, detect OWL axioms 3. LLM synthesis - One LLM call to identify domain, patterns, navigation hints 4. Structured storage - Store as retrievable AttrDict in REPL namespace\n\n\n\nbuild_sense\n\ndef build_sense(\n    path:str, name:str='sense', ns:dict=None\n)-&gt;str:\n\nBuild ontology sense document using workflow + LLM synthesis.\nDetects annotation properties per Widoco metadata guide: - Label properties: rdfs:label, skos:prefLabel, skos:altLabel, dcterms:title - Description properties: rdfs:comment, skos:definition, dcterms:description - Ontology metadata: vann:preferredNamespacePrefix, owl:versionInfo, etc.\nThis function: 1. Loads ontology and extracts metadata/roots programmatically 2. Detects which annotation properties are actually used 3. Builds hierarchy (2 levels), property info, characteristics 4. Makes one LLM call to synthesize domain/scope/patterns/hints 5. Returns structured AttrDict stored in namespace\nArgs: path: Path to ontology file name: Variable name for sense document (default: ‘sense’) ns: Namespace dict\nReturns: Summary string\n\n# Test build_sense with PROV ontology\n# Note: Requires API key, marked eval:false to avoid CI failures\n\ntest_ns = {}\nresult = build_sense('ontology/prov.ttl', name='prov_sense', ns=test_ns)\nprint(result)\nprint()\n\n# Inspect the sense document\nsense = test_ns['prov_sense']\nprint(f\"Ontology: {sense.ont}\")\nprint(f\"Ontology Metadata: {sense.ont_metadata}\")\nprint(f\"Stats: {sense.stats}\")\nprint()\n\n# NEW: Show detected annotation properties\nprint(f\"Label properties detected: {sense.label_properties}\")\nprint(f\"Description properties detected: {sense.description_properties}\")\nprint()\n\nprint(f\"Roots: {sense.roots}\")\nprint(f\"Root branches: {list(sense.hier.keys())}\")\nprint(f\"Top properties (first 3): {sense.top_props[:3]}\")\nprint(f\"Property characteristics: {sense.prop_chars}\")\nprint(f\"OWL constructs: {sense.owl_constructs}\")\nprint(f\"URI pattern: {sense.uri_pattern}\")\nprint()\nprint(\"LLM Summary:\")\nprint(sense.summary)",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#structured-sense-data",
    "href": "ontology.html#structured-sense-data",
    "title": "ontology",
    "section": "Structured Sense Data",
    "text": "Structured Sense Data\nNEW: JSON-schemaed sense data for ReasoningBank integration.\nThe original build_sense() produces free-form prose in the summary field. This new system creates: - sense_card: Compact, always-injected structured data (~500 chars) - sense_brief: Detailed sections retrieved when needed (~2000 chars) - Grounding validation: All URIs must exist in the ontology\nSee docs/ont-sense-improvements.md for full specification.\n\n\nvalidate_sense_grounding\n\ndef validate_sense_grounding(\n    sense:dict, meta:GraphMeta\n)-&gt;dict:\n\nValidate all URIs in sense exist in the ontology.\nArgs: sense: Sense document with sense_card (and optional sense_brief) meta: GraphMeta to validate against\nReturns: {‘valid’: bool, ‘errors’: list[str], ‘error_count’: int}\n\n\n\nbuild_sense_structured\n\ndef build_sense_structured(\n    path:str, name:str='sense', ns:dict=None\n)-&gt;dict:\n\nBuild structured sense document with card and brief.\nReturns JSON-schemaed output instead of free-form prose.\nArgs: path: Path to ontology file name: Variable name for sense document ns: Namespace dict\nReturns: Dict with ‘sense_card’, ‘sense_brief’, and ’_validation’ keys",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#integration-with-rlm",
    "href": "ontology.html#integration-with-rlm",
    "title": "ontology",
    "section": "Integration with RLM",
    "text": "Integration with RLM\nHelper to setup ontology context for rlm_run().\n\n# Test RLM with structured sense context\n# Note: Requires API key, marked eval:false to avoid CI failures\n\nfrom rlm.core import rlm_run\n\nprint(\"=\" * 70)\nprint(\" RLM INTEGRATION TEST: Structured Sense as Context\")\nprint(\"=\" * 70)\n\n# Setup: Build structured sense for PROV ontology\nns = {}\nsense_result = build_sense_structured('ontology/prov.ttl', name='prov_sense', ns=ns)\n\n# Get formatted sense card as context\nsense_context = format_sense_card(sense_result['sense_card'])\n\nprint(f\"\\n📋 Context Type: Structured Sense Card\")\nprint(f\"   Size: {len(sense_context)} chars\")\nprint(f\"   Grounding: {'PASS' if sense_result['_validation']['valid'] else 'FAIL'}\")\n\n# Test query\nquery = \"What is the Activity class in PROV?\"\n\nprint(f\"\\n❓ Query: {query}\")\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Running RLM with sense card context...\")\nprint(\"-\" * 70)\n\n# Run RLM with sense context\nanswer, iterations, final_ns = rlm_run(\n    query,\n    sense_context,\n    ns=ns,\n    max_iters=5\n)\n\nprint(f\"\\n✓ Answer: {answer}\")\nprint(f\"\\n📊 Iterations: {len(iterations)}\")\nprint(f\"   Max allowed: 5\")\n\n# Show iteration details\nprint(f\"\\n🔍 Iteration Breakdown:\")\nfor i, iteration in enumerate(iterations, 1):\n    print(f\"   {i}. {iteration.get('action', 'unknown action')}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\" TEST RESULT\")\nprint(\"=\" * 70)\n\nif len(iterations) &lt;= 5:\n    print(f\"\\n✓ PASS: RLM converged in {len(iterations)} iterations\")\n    print(f\"  The structured sense card provides sufficient context for RLM\")\nelse:\n    print(f\"\\n✗ FAIL: RLM did not converge within iteration limit\")\n\nprint(\"\\n💡 Benefits of Structured Sense:\")\nprint(\"  • Compact context (~600 chars vs full ontology)\")\nprint(\"  • 100% grounded (no hallucinated URIs)\")\nprint(\"  • Ontology-aware (detects label/description predicates)\")\nprint(\"  • Progressive disclosure ready (can add hierarchy brief)\")\n\n\nTest RLM Integration with Structured Sense\nTest if rlm_run() works with the new structured sense documents as context.\n\n# Test formatting functions\nprint(\"=\" * 60)\nprint(\"FORMATTING FUNCTIONS TEST\")\nprint(\"=\" * 60)\n\n# Test format_sense_card\nformatted_card = format_sense_card(card)\nprint(f\"\\n✓ Formatted Sense Card ({len(formatted_card)} chars):\")\nprint(\"-\" * 60)\nprint(formatted_card)\nprint(\"-\" * 60)\n\n# Test format_sense_brief_section\nformatted_hier = format_sense_brief_section(brief, 'hierarchy_overview')\nprint(f\"\\n✓ Formatted Hierarchy Overview ({len(formatted_hier)} chars):\")\nprint(\"-\" * 60)\nprint(formatted_hier)\nprint(\"-\" * 60)\n\n# Test get_sense_context\nquery = \"What are the subclasses of Activity?\"\ncontext = get_sense_context(query, result)\nprint(f\"\\n✓ Auto-detected Context for: '{query}'\")\nprint(f\"  Context length: {len(context)} chars\")\nprint(f\"  Includes hierarchy: {('Hierarchy Overview' in context)}\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(\"FORMATTING TESTS PASSED\")\nprint(\"=\" * 60)\n\n\n# Test build_sense_structured with PROV ontology\ntest_ns = {}\nresult = build_sense_structured('ontology/prov.ttl', name='prov_sense_structured', ns=test_ns)\n\nprint(\"=\" * 60)\nprint(\"STRUCTURED SENSE TEST\")\nprint(\"=\" * 60)\n\n# Check validation\nprint(f\"\\n✓ Validation: {'PASS' if result['_validation']['valid'] else 'FAIL'}\")\nif not result['_validation']['valid']:\n    print(f\"  Errors: {result['_validation']['errors']}\")\nelse:\n    print(\"  All URIs grounded in ontology\")\n\n# Check sense_card\ncard = result['sense_card']\nprint(f\"\\n✓ Sense Card:\")\nprint(f\"  Ontology ID: {card['ontology_id']}\")\nprint(f\"  Triple count: {card['triple_count']:,}\")\nprint(f\"  Class count: {card['class_count']}\")\nprint(f\"  Property count: {card['property_count']}\")\nprint(f\"  Label predicates: {len(card['label_predicates'])}\")\nprint(f\"  Key classes: {len(card['key_classes'])}\")\nprint(f\"  Key properties: {len(card['key_properties'])}\")\nprint(f\"  Quick hints: {len(card['quick_hints'])}\")\n\n# Verify key_classes are grounded\nprint(f\"\\n✓ Key Classes (grounded URIs):\")\nfor cls in card['key_classes'][:3]:\n    print(f\"  - {cls['label']}\")\n    print(f\"    URI: {cls['uri'][:50]}...\")\n\n# Verify key_properties are grounded\nprint(f\"\\n✓ Key Properties (grounded URIs):\")\nfor prop in card['key_properties'][:3]:\n    print(f\"  - {prop['label']}: {prop['role']}\")\n    print(f\"    URI: {prop['uri'][:50]}...\")\n\n# Check sense_brief\nbrief = result['sense_brief']\nprint(f\"\\n✓ Sense Brief:\")\nprint(f\"  Hierarchy roots: {len(brief['hierarchy_overview']['root_classes'])}\")\nprint(f\"  Max depth: {brief['hierarchy_overview']['max_depth']}\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(\"ALL TESTS PASSED\")\nprint(\"=\" * 60)\n\n\n\n\nget_sense_context\n\ndef get_sense_context(\n    query:str, sense:dict\n)-&gt;str:\n\nAuto-detect and return relevant sense sections for a query.\nArgs: query: User query sense: Full sense document (with sense_card and sense_brief)\nReturns: Formatted context string\n\n\n\nformat_sense_brief_section\n\ndef format_sense_brief_section(\n    brief:dict, section:str\n)-&gt;str:\n\nFormat a specific brief section.\nArgs: brief: sense_brief dict section: Section name (e.g., ‘hierarchy_overview’, ‘patterns’)\nReturns: Formatted markdown string\n\n\n\nformat_sense_card\n\ndef format_sense_card(\n    card:dict\n)-&gt;str:\n\nFormat sense card for context injection (~500 chars).\nArgs: card: sense_card dict\nReturns: Formatted markdown string\n\n# NOTE: setup_ontology_context() is defined above in cell-27\n# This cell previously contained a duplicate definition\n\n\n# Test setup for RLM\ntest_ns = {}\nresult = setup_ontology_context('ontology/prov.ttl', test_ns, name='prov')\nprint(result)\nprint()\nprint(\"Namespace contains:\")\nfor k in test_ns.keys():\n    print(f\"  {k}: {type(test_ns[k]).__name__}\")\n\nLoaded 1664 triples from prov.ttl into 'prov'\nCreated meta-graph 'prov_meta' with 59 classes, 89 properties\n\nNamespace contains:\n  prov: Graph\n  prov_meta: GraphMeta\n  prov_graph_stats: partial\n  prov_search_by_label: partial\n  prov_describe_entity: partial\n  prov_search_entity: partial\n  prov_probe_relationships: partial\n  prov_find_path: partial\n  prov_predicate_frequency: partial\n  graph_stats: partial\n  search_by_label: partial\n  describe_entity: partial\n  search_entity: partial\n  probe_relationships: partial\n  find_path: partial\n  predicate_frequency: partial\n\n\n\n# Test new exploration functions\n# Reuse the test_ns from previous cell with loaded prov ontology\n# Note: prov_meta is a GraphMeta object in test_ns\n\n# Test that new indexes work\nmeta = test_ns['prov_meta']\nassert len(meta.by_label) &gt; 0  # inverted label index\nassert len(meta.subs) &gt; 0 or len(meta.supers) &gt; 0  # class hierarchy\nprint(f\"✓ New GraphMeta indexes work: by_label has {len(meta.by_label)} entries\")\n\n# Test ont_describe (need to pass GraphMeta object as namespace entry)\nresult = ont_describe('prov_meta', 'http://www.w3.org/ns/prov#Activity', name='activity_desc', ns=test_ns)\nassert 'activity_desc' in test_ns\nprint(f\"✓ ont_describe works: {result}\")\n\n# Test ont_meta  \nresult = ont_meta('prov_meta', name='prov_metadata', ns=test_ns)\nassert 'prov_metadata' in test_ns\nprint(f\"✓ ont_meta works: {result}\")\n\n# Test ont_roots\nresult = ont_roots('prov_meta', name='prov_roots', ns=test_ns)\nassert 'prov_roots' in test_ns\nprint(f\"✓ ont_roots works: {result}\")\n\n✓ New GraphMeta indexes work: by_label has 156 entries\n✓ ont_describe works: Stored 10 + 34 triples about 'http://www.w3.org/ns/prov#Activity' into 'activity_desc'\n✓ ont_meta works: Stored metadata into 'prov_metadata': 29 prefixes, 16 annotation predicates, 9 imports\n✓ ont_roots works: Stored 10 root classes into 'prov_roots'",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#test-with-rlm",
    "href": "ontology.html#test-with-rlm",
    "title": "ontology",
    "section": "Test with RLM",
    "text": "Test with RLM\nNow let’s test asking a question about the PROV ontology using rlm_run().\n\nfrom rlm.core import rlm_run\n\n# Setup namespace with PROV ontology\nns = {}\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\n\n# Ask a question\n# The context is the GraphMeta summary, not the full graph\ncontext = ns['prov_meta'].summary()\n\nanswer, iterations, ns = rlm_run(\n    \"What is the Activity class in the PROV ontology?\",\n    context,\n    ns=ns,\n    max_iters=3\n)\n\nprint(f\"Answer: {answer}\")\nprint(f\"Iterations: {len(iterations)}\")",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "ontology.html#sense-validation-gate",
    "href": "ontology.html#sense-validation-gate",
    "title": "ontology",
    "section": "Sense Validation Gate",
    "text": "Sense Validation Gate\nValidate sense data before RLM operations (precondition check).\n\n\nvalidate_sense_precondition\n\ndef validate_sense_precondition(\n    sense:dict, meta\n)-&gt;dict:\n\nGate 0: Validate sense data before RLM operations.\nChecks: - URI grounding (all URIs exist in ontology) - Card size (under 800 chars) - Required fields present\nArgs: sense: Sense document from build_sense_structured() meta: GraphMeta object for grounding validation\nReturns: Dictionary with proceed flag and validation details\n\n# Test sense validation gate (requires real ontology)\nfrom rlm.ontology import setup_ontology_context, build_sense_structured\n\nprint(\"Test: validate_sense_precondition()\")\nprint(\"=\" * 60)\n\nns = {}\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\nsense = build_sense_structured('ontology/prov.ttl', name='prov_sense', ns=ns)\n\nresult = validate_sense_precondition(sense, ns['prov_meta'])\n\nprint(f\"Proceed: {result['proceed']}\")\nprint(f\"Grounding valid: {result['grounding_valid']}\")\nprint(f\"Card size: {result['card_size']} chars (ok: {result['card_size_ok']})\")\nprint(f\"Has required fields: {result['has_required_fields']}\")\n\nif result['proceed']:\n    print(\"\\n✓ Sense validation gate passed\")\nelse:\n    print(f\"\\n✗ Validation failed: {result['reason']}\")",
    "crumbs": [
      "ontology"
    ]
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "",
    "text": "# Shared namespace for the entire notebook - demonstrating REPL persistence\nimport sys\nimport os\nfrom pathlib import Path\n\nns = {}  # This single namespace persists throughout the tutorial\n\ndef require_anthropic_api_key():\n    \"\"\"Fail fast if the Claude API key is not configured.\"\"\"\n    if not os.getenv('ANTHROPIC_API_KEY'):\n        raise RuntimeError(\n            \"Missing ANTHROPIC_API_KEY. Set it in your environment to run llm_query()/rlm_run() cells.\"\n        )",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#core-rlm-loop",
    "href": "tutorial.html#core-rlm-loop",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "1. Core RLM Loop",
    "text": "1. Core RLM Loop\nThe llm_query() function delegates a question to Claude and stores the result.\n\nfrom rlm.core import llm_query\n\nrequire_anthropic_api_key()\n\n# Use shared ns - result will persist\nresult = llm_query(\"What is 2+2? Answer with just the number.\", ns, name='math')\nprint(f\"Result: {result}\")\nprint(f\"Stored as: ns['math'] = {ns.get('math', 'not found')}\")\n\nResult: 4\nStored as: ns['math'] = 4\n\n\nThe rlm_run() function runs the full RLM loop: the model emits code, executes it in a REPL, and iterates until it finds an answer.\n\nfrom rlm.core import rlm_run\n\nrequire_anthropic_api_key()\n\n# Continue using shared ns\nanswer, iterations, ns = rlm_run(\n    \"Calculate the sum of squares of 1, 2, and 3.\",\n    \"You can use Python to calculate.\",\n    ns=ns,\n    max_iters=3\n)\nprint(f\"Answer: {answer}\")\nprint(f\"Iterations: {len(iterations)}\")\nprint(f\"ns still has 'math': {ns.get('math', 'not found')}\")\n\nAnswer: 14\nIterations: 1\nns still has 'math': 4",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#ontology-loading",
    "href": "tutorial.html#ontology-loading",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "2. Ontology Loading",
    "text": "2. Ontology Loading\nLoad RDF ontologies and explore them with bounded view functions. The key insight: we never dump the full graph into context.\n\nfrom rlm.ontology import setup_ontology_context\n\n# Add PROV ontology to shared ns\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\nprint(ns['prov_meta'].summary())\nprint(f\"\\nns now contains: {[k for k in ns.keys() if not k.startswith('_')]}\")\n\nGraph 'prov': 1,664 triples\nClasses: 59\nProperties: 89\nIndividuals: 1\nNamespaces: brick, csvw, dc, dcat, dcmitype, dcterms, dcam, doap, foaf, geo, odrl, org, prof, qb, schema, sh, skos, sosa, ssn, time, vann, void, wgs, owl, rdf, rdfs, xsd, xml, prov\n\nns now contains: ['math', 'context', 'llm_query', 'llm_query_batched', 'FINAL_VAR', 'llm_res', 'analysis', 'sum_of_squares', 'prov', 'prov_meta', 'prov_graph_stats', 'prov_search_by_label', 'prov_describe_entity', 'prov_search_entity', 'prov_probe_relationships', 'prov_find_path', 'prov_predicate_frequency', 'graph_stats', 'search_by_label', 'describe_entity', 'search_entity', 'probe_relationships', 'find_path', 'predicate_frequency']\n\n\n\n# Search for classes related to \"Activity\"\nresults = ns['prov_search_by_label']('Activity', limit=5)\nfor uri, label in results:\n    print(f\"{label}: {uri}\")\n\nActivity: http://www.w3.org/ns/prov#Activity\nActivityInfluence: http://www.w3.org/ns/prov#ActivityInfluence\nactivity: http://www.w3.org/ns/prov#activity\nhadActivity: http://www.w3.org/ns/prov#hadActivity\nactivityOfInfluence: http://www.w3.org/ns/prov#activityOfInfluence\n\n\n\n# Get bounded description of Activity class\ndesc = ns['prov_describe_entity']('http://www.w3.org/ns/prov#Activity', limit=10)\nprint(f\"Label: {desc['label']}\")\nprint(f\"Types: {desc['types']}\")\nprint(f\"Comment: {desc['comment'][:100] if desc['comment'] else 'None'}...\")\nprint(f\"Outgoing triples (sample): {len(desc['outgoing_sample'])}\")\n\nLabel: Activity\nTypes: ['http://www.w3.org/2002/07/owl#Class']\nComment: None...\nOutgoing triples (sample): 10",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#rlm-with-ontology-exploration",
    "href": "tutorial.html#rlm-with-ontology-exploration",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "3. RLM with Ontology Exploration",
    "text": "3. RLM with Ontology Exploration\nCombine the RLM loop with ontology tools for intelligent exploration. The model uses bounded views to progressively discover information.\n\nfrom rlm.core import rlm_run\nfrom rlm.ontology import setup_ontology_context\n\nrequire_anthropic_api_key()\n\n# PROV is already loaded in ns from previous section\nquery = \"What is prov:Activity? Use search_by_label and describe_entity.\"\ncontext = ns['prov_meta'].summary()\n\nanswer, iterations, ns = rlm_run(\n    query,\n    context,\n    ns=ns,\n    max_iters=3,\n    verbose=False\n)\n\nprint(f\"Answer: {answer[:500] if answer else 'No answer'}...\")\nprint(f\"Iterations: {len(iterations)}\")\n\nAnswer: [Max iterations] Last output: Description of prov:Activity:\n{'uri': 'http://www.w3.org/ns/prov#Activity', 'label': 'Activity', 'types': ['http://www.w3.org/2002/07/owl#Class'], 'comment': None, 'outgoing_sample': [('http://www.w3.org/1999/02/22-rdf-syntax-ns#type', 'http://www.w3.org/2002/07/owl#Class'), ('http://www.w3.org/2000/01/rdf-schema#isDefinedBy', 'http://www.w3.org/ns/prov-o#'), ('http://www.w3.org/2000/01/rdf-schema#label', 'Activity'), ('http://www.w3.org/2002/07/owl#disjointWith', '...\nIterations: 3\n\n\n\n# Show what code the LLM executed\nfor i, it in enumerate(iterations):\n    if it.code_blocks:\n        print(f\"Iteration {i}:\")\n        for cb in it.code_blocks:\n            print(f\"  Code: {cb.code[:100]}...\")\n\nIteration 0:\n  Code: print(\"Context content:\")\nprint(context)\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Context type: {type(context)}\"...\nIteration 1:\n  Code: # Search for \"Activity\" using search_by_label\nactivity_search = search_by_label(\"Activity\")\nprint(\"S...\nIteration 2:\n  Code: # Describe the prov:Activity entity\nactivity_description = describe_entity(\"http://www.w3.org/ns/pro...",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#dataset-memory",
    "href": "tutorial.html#dataset-memory",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "4. Dataset Memory",
    "text": "4. Dataset Memory\nStore discovered facts in an RDF Dataset with provenance tracking. Facts persist within the same namespace/session. Use snapshot_dataset() and load_snapshot() APIs for persistence across sessions.\n\nfrom rlm.dataset import setup_dataset_context\n\n# Add dataset to shared ns (alongside previously loaded ontology)\nsetup_dataset_context(ns)\nprint(ns['dataset_stats']())\nprint(f\"\\nPROV ontology still accessible: 'prov_meta' in ns = {'prov_meta' in ns}\")\n\nDataset 'ds' (session: f7322b86)\nmem: 0 triples\nprov: 0 events\nwork graphs: 0\nonto graphs: 0\n\nPROV ontology still accessible: 'prov_meta' in ns = True\n\n\n\n# Add a fact we discovered\nns['mem_add'](\n    'http://example.org/myAnalysis',\n    'http://www.w3.org/ns/prov#wasGeneratedBy',\n    'http://example.org/rlmSession1'\n)\n\n# Check stats\nprint(ns['dataset_stats']())\n\nDataset 'ds' (session: f7322b86)\nmem: 1 triples\nprov: 7 events\nwork graphs: 0\nonto graphs: 0\n\n\n\n# Query the memory graph\nresults = ns['mem_query'](\"\"\"\n    SELECT ?s ?p ?o WHERE { ?s ?p ?o }\n\"\"\")\nfor r in results:\n    print(r)\n\n{'s': 'http://example.org/myAnalysis', 'p': 'http://www.w3.org/ns/prov#wasGeneratedBy', 'o': 'http://example.org/rlmSession1'}",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#sparql-result-handles",
    "href": "tutorial.html#sparql-result-handles",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "5. SPARQL Result Handles",
    "text": "5. SPARQL Result Handles\nQuery results return handles with metadata, not raw data dumps. Handles support bounded sampling (e.g., rows[:n]) and summary statistics.\nNote: Results are still fetched into memory; handles provide metadata-first access patterns rather than true server-side pagination.\n\nfrom rlm.sparql_handles import SPARQLResultHandle\n\n# Simulating a large result set\nhandle = SPARQLResultHandle(\n    rows=[{'name': f'Item{i}', 'value': i} for i in range(100)],\n    result_type='select',\n    query='SELECT ?name ?value WHERE { ... }',\n    endpoint='local',\n    columns=['name', 'value'],\n    total_rows=100\n)\n\nprint(handle.summary())\nprint(f\"First 3 rows: {handle.rows[:3]}\")\n\nSELECT: 100 rows, columns=['name', 'value']\nFirst 3 rows: [{'name': 'Item0', 'value': 0}, {'name': 'Item1', 'value': 1}, {'name': 'Item2', 'value': 2}]",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#procedural-memory",
    "href": "tutorial.html#procedural-memory",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "6. Procedural Memory",
    "text": "6. Procedural Memory\nStore and retrieve methods learned from past trajectories. Uses BM25 for similarity-based retrieval.\n\nfrom rlm.procedural_memory import MemoryStore, MemoryItem, retrieve_memories\nfrom datetime import datetime, timezone\nimport uuid\n\nstore = MemoryStore()\n\n# Add a learned procedure\nitem = MemoryItem(\n    id=str(uuid.uuid4()),\n    title='Find Activity classes in PROV',\n    description='How to discover Activity-related classes',\n    content='1. Use search_by_label(\"Activity\")\\n2. Use describe_entity() on results',\n    source_type='success',\n    task_query='find activities in PROV',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['prov', 'ontology', 'exploration']\n)\nstore.add(item)\n\nprint(f\"Store has {len(store.memories)} memories\")\n\nStore has 1 memories\n\n\n\n# Retrieve relevant memories for a new task\nretrieved = retrieve_memories(store, 'how to explore PROV ontology activities', k=1)\nfor mem in retrieved:\n    print(f\"Title: {mem.title}\")\n    print(f\"Content:\\n{mem.content}\")\n\nTitle: Find Activity classes in PROV\nContent:\n1. Use search_by_label(\"Activity\")\n2. Use describe_entity() on results",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#shacl-shape-indexing",
    "href": "tutorial.html#shacl-shape-indexing",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "7. SHACL Shape Indexing",
    "text": "7. SHACL Shape Indexing\nDetect and index SHACL shapes for schema discovery and constraint inspection.\nNote: This provides shape detection and constraint inspection (targets, properties, cardinalities), not runtime validation. Use a SHACL validator for actual data validation.\n\nfrom rlm.shacl_examples import detect_shacl, build_shacl_index, search_shapes\nfrom rdflib import Graph\n\n# Load DCAT-AP shapes\ng = Graph()\ng.parse('ontology/dcat-ap/dcat-ap-SHACL.ttl')\n\n# Detect SHACL content\ndetection = detect_shacl(g)\nprint(f\"Node shapes: {detection['node_shapes']}\")\nprint(f\"Property shapes: {detection['property_shapes']}\")\n\nNode shapes: 42\nProperty shapes: 0\n\n\n\n# Build index and search\nindex = build_shacl_index(g)\nresults = search_shapes(index, 'dataset', limit=3)\n\nfor r in results:\n    print(f\"{r['uri'].split('#')[-1]}: targets {r['targets']}\")\n\ndcat:CatalogShape: targets ['http://www.w3.org/ns/dcat#Catalog']\ndcat:DatasetShape: targets ['http://www.w3.org/ns/dcat#Dataset']\ndcat:DataServiceShape: targets ['http://www.w3.org/ns/dcat#DataService']",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#full-integration-multi-ontology-comparison",
    "href": "tutorial.html#full-integration-multi-ontology-comparison",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "8. Full Integration: Multi-Ontology Comparison",
    "text": "8. Full Integration: Multi-Ontology Comparison\nPutting it all together: load multiple ontologies, build sense documents, and use RLM to answer complex questions.\n\nfrom rlm.ontology import build_sense\n\nrequire_anthropic_api_key()\n\n# Build PROV sense document in shared ns\nbuild_sense('ontology/prov.ttl', name='prov_sense', ns=ns)\nprint(\"PROV sense document built\")\nprint(f\"Summary length: {len(ns['prov_sense'].summary)} chars\")\n\nPROV sense document built\nSummary length: 6275 chars\n\n\n\nfrom rlm.core import rlm_run\n\nrequire_anthropic_api_key()\n\n# Build sense for SIO in shared ns\nbuild_sense('ontology/sio/sio-release.owl', name='sio_sense', ns=ns)\n\n# Context as dict - model can inspect context['prov'] / context['sio'] directly\ncontext = {\n    'prov': ns['prov_sense'].summary[:2000],  # Truncate for demo\n    'sio': ns['sio_sense'].summary[:2000]\n}\n\nquery = \"What are the key differences between PROV and SIO ontologies?\"\n\n# Pass dict context directly (not str(context)) for progressive disclosure\nanswer, iterations, ns = rlm_run(\n    query,\n    context,  # Keep dict structure for model inspection\n    ns=ns,\n    max_iters=3,\n    verbose=False\n)\n\nprint(f\"Answer:\\n{answer[:800] if answer else 'No answer'}...\")\nprint(f\"\\nIterations: {len(iterations)}\")\n\nAnswer:\n[Max iterations] Last output: # Comprehensive Comparison: PROV vs SIO Ontologies\n\n## 1. Primary Purposes and Domains\n\n### **PROV Ontology**\n- **Purpose**: Specialized ontology for **provenance tracking** - documenting the history, lineage, and accountability of resources\n- **Focus**: Temporal chains of causation, responsibility, and influence\n- **Domain**: Cross-domain provenance (applicable to any field requiring audit trails)\n- **Philosophical stance**: Process-centric view of how things came to be\n\n### **SIO Ontology**\n- ......\n\nIterations: 3",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "tutorial.html#summary",
    "href": "tutorial.html#summary",
    "title": "RLM Tutorial: Progressive Disclosure Over RDF Graphs",
    "section": "Summary",
    "text": "Summary\nThis tutorial demonstrated:\n\nCore RLM loop: llm_query() and rlm_run() for LLM-driven exploration\nOntology loading: Bounded views prevent context overflow\nProgressive disclosure: Start small, explore as needed\nDataset memory: Persist discovered facts with provenance (within a session/namespace)\nSPARQL handles: Metadata-first result handling with bounded sampling\nProcedural memory: Learn and reuse exploration strategies\nSHACL indexing: Schema discovery and constraint inspection through shape search\n\nEnvironment requirements: Sections using llm_query() or rlm_run() require network access and ANTHROPIC_API_KEY set in your environment. Non-LLM sections work offline.",
    "crumbs": [
      "RLM Tutorial: Progressive Disclosure Over RDF Graphs"
    ]
  },
  {
    "objectID": "eval_reports.html",
    "href": "eval_reports.html",
    "title": "Evaluation Reports",
    "section": "",
    "text": "This notebook loads and visualizes results from the RLM evaluation framework. Eval tasks are defined in evals/tasks/ and executed via the CLI:\nResults are saved to evals/results/ as JSON files and loaded here for analysis.\nfrom pathlib import Path\nimport json\nfrom datetime import datetime",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "eval_reports.html#load-results",
    "href": "eval_reports.html#load-results",
    "title": "Evaluation Reports",
    "section": "Load Results",
    "text": "Load Results\nLoad all result JSON files from the results directory:\n\nresults_dir = Path('../evals/results')\n\ndef load_results(results_dir: Path) -&gt; list:\n    \"\"\"Load all evaluation results from directory.\"\"\"\n    if not results_dir.exists():\n        return []\n    \n    results = []\n    for result_file in results_dir.glob('*.json'):\n        try:\n            with open(result_file) as f:\n                data = json.load(f)\n                results.append(data)\n        except Exception as e:\n            print(f\"Warning: Could not load {result_file}: {e}\")\n    \n    return results\n\nresults = load_results(results_dir)\nprint(f\"Loaded {len(results)} result files\")",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "eval_reports.html#summary-statistics",
    "href": "eval_reports.html#summary-statistics",
    "title": "Evaluation Reports",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nOverall pass rates across all tasks:\n\nif results:\n    total_trials = sum(r.get('total_trials', 0) for r in results)\n    total_passed = sum(r.get('passed_trials', 0) for r in results)\n    avg_pass_rate = total_passed / total_trials if total_trials &gt; 0 else 0\n    \n    print(f\"Tasks evaluated: {len(results)}\")\n    print(f\"Total trials: {total_trials}\")\n    print(f\"Total passed: {total_passed}\")\n    print(f\"Overall pass rate: {avg_pass_rate:.1%}\")\nelse:\n    print(\"No results found. Run evals first:\")\n    print(\"  python -m evals.cli run\")",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "eval_reports.html#results-by-category",
    "href": "eval_reports.html#results-by-category",
    "title": "Evaluation Reports",
    "section": "Results by Category",
    "text": "Results by Category\nBreak down performance by task category:\n\nif results:\n    from collections import defaultdict\n    \n    by_category = defaultdict(list)\n    \n    for r in results:\n        task_id = r.get('task_id', 'unknown')\n        category = task_id.split('_')[0] if '_' in task_id else 'unknown'\n        by_category[category].append(r)\n    \n    print(\"\\nResults by Category:\")\n    print(\"=\" * 60)\n    \n    for category in sorted(by_category.keys()):\n        cat_results = by_category[category]\n        cat_trials = sum(r.get('total_trials', 0) for r in cat_results)\n        cat_passed = sum(r.get('passed_trials', 0) for r in cat_results)\n        cat_rate = cat_passed / cat_trials if cat_trials &gt; 0 else 0\n        \n        print(f\"\\n{category.upper()}\")\n        print(\"-\" * 40)\n        \n        for r in cat_results:\n            task_id = r.get('task_id', 'unknown')\n            passed = r.get('passed_trials', 0)\n            total = r.get('total_trials', 0)\n            pass_at_k = r.get('pass_at_k', 0)\n            status = \"PASS\" if passed == total else \"FAIL\"\n            print(f\"  [{status}] {task_id}: {passed}/{total} ({pass_at_k:.0%})\")\n        \n        print(f\"  Category total: {cat_passed}/{cat_trials} ({cat_rate:.0%})\")",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "eval_reports.html#detailed-results",
    "href": "eval_reports.html#detailed-results",
    "title": "Evaluation Reports",
    "section": "Detailed Results",
    "text": "Detailed Results\nShow full details for each task:\n\nif results:\n    print(\"\\nDetailed Results:\")\n    print(\"=\" * 60)\n    \n    for r in results:\n        task_id = r.get('task_id', 'unknown')\n        print(f\"\\n{task_id}\")\n        print(\"-\" * 40)\n        print(f\"  pass@{r.get('total_trials', 0)}: {r.get('pass_at_k', 0):.1%}\")\n        print(f\"  pass^{r.get('total_trials', 0)}: {r.get('pass_power_k', 0):.1%}\")\n        print(f\"  Passed: {r.get('passed_trials', 0)}/{r.get('total_trials', 0)}\")\n        print(f\"  Avg iterations: {r.get('avg_iterations', 0):.1f}\")\n        \n        if 'trials' in r:\n            print(f\"  Trial details:\")\n            for i, trial in enumerate(r['trials'], 1):\n                passed = trial.get('passed', False)\n                status = \"✓\" if passed else \"✗\"\n                print(f\"    {status} Trial {i}: {trial.get('iterations', 0)} iterations\")",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "eval_reports.html#next-steps",
    "href": "eval_reports.html#next-steps",
    "title": "Evaluation Reports",
    "section": "Next Steps",
    "text": "Next Steps\nTo run evaluations and generate results:\n# Run all tasks\npython -m evals.cli run\n\n# Execute this notebook to show results\nnbdev exec_nb --path nbs/eval_reports.ipynb\n\n# Regenerate documentation\nnbdev_docs",
    "crumbs": [
      "Evaluation Reports"
    ]
  },
  {
    "objectID": "dataset_memory.html",
    "href": "dataset_memory.html",
    "title": "dataset",
    "section": "",
    "text": "This module implements RDF Dataset-based memory for RLM sessions using named graphs:\n\nonto/&lt;name&gt; - Read-only ontology graphs\nmem - Mutable working memory for current session\nprov - Provenance/audit trail\nwork/&lt;task_id&gt; - Scratch graphs for intermediate results\n\n\n\n\nSession-scoped: mem is working memory for current RLM run\nHandle-based access: Model sees bounded views, never raw quads\nProvenance tracking: All mem changes recorded with timestamp/source/reason\nLazy indexing: Caches invalidated on mutation",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#overview",
    "href": "dataset_memory.html#overview",
    "title": "dataset",
    "section": "",
    "text": "This module implements RDF Dataset-based memory for RLM sessions using named graphs:\n\nonto/&lt;name&gt; - Read-only ontology graphs\nmem - Mutable working memory for current session\nprov - Provenance/audit trail\nwork/&lt;task_id&gt; - Scratch graphs for intermediate results\n\n\n\n\nSession-scoped: mem is working memory for current RLM run\nHandle-based access: Model sees bounded views, never raw quads\nProvenance tracking: All mem changes recorded with timestamp/source/reason\nLazy indexing: Caches invalidated on mutation",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#imports",
    "href": "dataset_memory.html#imports",
    "title": "dataset",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#datasetmeta",
    "href": "dataset_memory.html#datasetmeta",
    "title": "dataset",
    "section": "DatasetMeta",
    "text": "DatasetMeta\nMeta-graph navigation for RDF Dataset with lazy-cached indexes.\n\n\nDatasetMeta\n\ndef DatasetMeta(\n    dataset:Dataset, name:str='ds', session_id:str=&lt;factory&gt;\n)-&gt;None:\n\nMeta-graph navigation for RDF Dataset.\nProvides lazy-cached indexes and bounded views over named graphs. Indexes are invalidated on any mutation to mem graph.",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#setup-function",
    "href": "dataset_memory.html#setup-function",
    "title": "dataset",
    "section": "Setup Function",
    "text": "Setup Function",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#memory-operations",
    "href": "dataset_memory.html#memory-operations",
    "title": "dataset",
    "section": "Memory Operations",
    "text": "Memory Operations\n\n\nmem_add\n\ndef mem_add(\n    ds_meta:DatasetMeta, subject, predicate, obj, source:str='agent', reason:str=None\n)-&gt;str:\n\nAdd fact to mem with provenance tracking.\nArgs: ds_meta: DatasetMeta containing the dataset subject: Subject URI or literal predicate: Predicate URI obj: Object URI or literal source: Source of this fact (default: ‘agent’) reason: Optional reason for adding\nReturns: Summary string\n\n\n\nmem_query\n\ndef mem_query(\n    ds_meta:DatasetMeta, sparql:str, limit:int=100\n)-&gt;list:\n\nQuery mem graph, return bounded results.\nArgs: ds_meta: DatasetMeta containing the dataset sparql: SPARQL query string limit: Maximum results to return\nReturns: List of result rows (as dicts)\n\n\n\nmem_retract\n\ndef mem_retract(\n    ds_meta:DatasetMeta, subject:NoneType=None, predicate:NoneType=None, obj:NoneType=None, source:str='agent',\n    reason:str=None\n)-&gt;str:\n\nRemove triples with provenance.\nArgs: ds_meta: DatasetMeta containing the dataset subject: Subject URI or None (wildcard) predicate: Predicate URI or None (wildcard) obj: Object URI/literal or None (wildcard) source: Source of this retraction reason: Optional reason for removing\nReturns: Summary string\n\n\n\nmem_describe\n\ndef mem_describe(\n    ds_meta:DatasetMeta, uri:str, limit:int=20\n)-&gt;dict:\n\nGet bounded entity description from mem.\nArgs: ds_meta: DatasetMeta containing the dataset uri: URI of entity to describe limit: Maximum triples to include\nReturns: Dict with ‘as_subject’ and ‘as_object’ triple lists",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#scratch-graph-operations",
    "href": "dataset_memory.html#scratch-graph-operations",
    "title": "dataset",
    "section": "Scratch Graph Operations",
    "text": "Scratch Graph Operations\n\n\nwork_create\n\ndef work_create(\n    ds_meta:DatasetMeta, task_id:str=None\n)-&gt;tuple:\n\nCreate a scratch graph for intermediate results.\nArgs: ds_meta: DatasetMeta containing the dataset task_id: Task identifier (default: auto-generated)\nReturns: (graph_uri, graph) tuple\n\n\n\nwork_cleanup\n\ndef work_cleanup(\n    ds_meta:DatasetMeta, task_id:str=None, all:bool=False\n)-&gt;str:\n\nRemove scratch graph(s).\nArgs: ds_meta: DatasetMeta containing the dataset task_id: Specific task to clean up, or None all: If True, remove all work/* graphs\nReturns: Summary string\n\n\n\nwork_to_mem\n\ndef work_to_mem(\n    ds_meta:DatasetMeta, task_id:str, source:str='work', reason:str=None\n)-&gt;str:\n\nPromote triples from scratch graph to mem with provenance.\nArgs: ds_meta: DatasetMeta containing the dataset task_id: Task identifier for work graph source: Source label for provenance reason: Optional reason for promotion\nReturns: Summary string",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#snapshot-functions",
    "href": "dataset_memory.html#snapshot-functions",
    "title": "dataset",
    "section": "Snapshot Functions",
    "text": "Snapshot Functions\n\n\nsnapshot_dataset\n\ndef snapshot_dataset(\n    ds_meta:DatasetMeta, path:str=None, format:str='trig'\n)-&gt;str:\n\nSerialize dataset to TriG/N-Quads for debugging.\nArgs: ds_meta: DatasetMeta to snapshot path: Output path (default: auto-generated with timestamp) format: ‘trig’ or ‘nquads’\nReturns: Path to snapshot file\n\n\n\nload_snapshot\n\ndef load_snapshot(\n    path:str, ns:dict, name:str='ds'\n)-&gt;str:\n\nLoad dataset from TriG/N-Quads snapshot.\nUseful for debugging/replay. Note: The snapshot preserves the original dataset name in graph URIs, so if you want to use the original name, extract it from the graph URIs.\nArgs: path: Path to snapshot file ns: Namespace dict where Dataset will be stored name: Variable name for the Dataset handle\nReturns: Summary string",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#bounded-view-functions",
    "href": "dataset_memory.html#bounded-view-functions",
    "title": "dataset",
    "section": "Bounded View Functions",
    "text": "Bounded View Functions\n\n\nres_distinct\n\ndef res_distinct(\n    result, column:str, limit:int=50\n)-&gt;list:\n\nGet distinct values in a column.\nArgs: result: ResultTable or list of dicts column: Column to get distinct values from limit: Maximum distinct values to return\nReturns: List of distinct values\n\n\n\nres_group\n\ndef res_group(\n    result, column:str, limit:int=20\n)-&gt;list:\n\nGet counts grouped by column value.\nArgs: result: ResultTable or list of dicts column: Column to group by limit: Maximum groups to return\nReturns: List of (value, count) tuples, sorted by count descending\n\n\n\nres_where\n\ndef res_where(\n    result, column:str, pattern:str=None, value:str=None, limit:int=100\n)-&gt;list:\n\nFilter result rows by column value or regex pattern.\nArgs: result: ResultTable or list of dicts column: Column name to filter on pattern: Optional regex pattern to match value: Optional exact value to match limit: Maximum matching rows to return (default: 100)\nReturns: List of matching rows\n\n\n\nres_head\n\ndef res_head(\n    result, n:int=10\n)-&gt;list:\n\nGet first N rows of a result set.\nArgs: result: ResultTable, list of dicts, or list of tuples n: Number of rows to return\nReturns: List of rows (same format as input)\n\n\n\nResultTable\n\ndef ResultTable(\n    rows:list, columns:list, query:str, total_rows:int\n)-&gt;None:\n\nWrapper for SPARQL query results with bounded view operations.",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#result-table-views-stage-2",
    "href": "dataset_memory.html#result-table-views-stage-2",
    "title": "dataset",
    "section": "Result Table Views (Stage 2)",
    "text": "Result Table Views (Stage 2)\nBounded view operations over SPARQL query results enable iterative exploration without overwhelming context:\n\nres_head(): Preview first N rows\nres_where(): Filter by column value or regex\nres_group(): Aggregate and count by column\nres_distinct(): Find unique values\n\nThese work with ResultTable wrapper or plain list-of-dicts from mem_query().\n\nUse Cases\n\nPreviewing large result sets: res_head(results, 10)\nFinding specific entities: res_where(results, 'name', pattern='Alice')\nUnderstanding data distribution: res_group(results, 'category')\nExploring unique values: res_distinct(results, 'author')\n\n\n\n\ndataset_stats\n\ndef dataset_stats(\n    ds_meta:DatasetMeta\n)-&gt;str:\n\nGet dataset statistics summary.\n\n\n\nlist_graphs\n\ndef list_graphs(\n    ds_meta:DatasetMeta, pattern:str=None\n)-&gt;list:\n\nList named graphs, optionally filtered.\nArgs: ds_meta: DatasetMeta containing the dataset pattern: Optional substring to filter graph URIs\nReturns: List of (graph_uri, triple_count) tuples\n\n\n\ngraph_sample\n\ndef graph_sample(\n    ds_meta:DatasetMeta, graph_uri:str, limit:int=10\n)-&gt;list:\n\nGet sample triples from a graph.\nArgs: ds_meta: DatasetMeta containing the dataset graph_uri: URI of graph to sample limit: Maximum triples to return\nReturns: List of (s, p, o) tuples as strings",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#ontology-integration",
    "href": "dataset_memory.html#ontology-integration",
    "title": "dataset",
    "section": "Ontology Integration",
    "text": "Ontology Integration\n\n\nmount_ontology\n\ndef mount_ontology(\n    ds_meta:DatasetMeta, ns:dict, path:str, ont_name:str, index_shacl:bool=True, index_queries:bool=True\n)-&gt;str:\n\nMount ontology into dataset as read-only onto/ graph.\nIf index_shacl=True and SHACL content detected, also builds SHACLIndex and stores in ns[’{ont_name}_shacl’].\nIf index_queries=True and sh:SPARQLExecutable detected, also builds QueryIndex and stores in ns[’{ont_name}_queries’].\nArgs: ds_meta: DatasetMeta containing the dataset ns: Namespace dict (for compatibility with setup_ontology_context) path: Path to ontology file ont_name: Name for the ontology index_shacl: Whether to detect and index SHACL shapes (default: True) index_queries: Whether to detect and index query templates (default: True)\nReturns: Summary string\n\n\n\nsetup_dataset_context\n\ndef setup_dataset_context(\n    ns:dict, name:str='ds'\n)-&gt;str:\n\nInitialize Dataset with mem/prov graphs, bind helper functions.\nArgs: ns: Namespace dict where Dataset will be stored name: Variable name for the Dataset handle\nReturns: Summary string describing what was created\n\n# Test result table views\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Add test data\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\nmem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\nmem_add(ds_meta, 'http://ex.org/charlie', 'http://ex.org/age', '30')\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/city', 'Boston')\nmem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/city', 'NYC')\n\n# Query and get results as list\nresults = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s &lt;http://ex.org/age&gt; ?age }')\n\n# Test res_head\nhead = res_head(results, n=2)\nassert len(head) == 2\nprint(f\"✓ res_head works: {len(head)} rows\")\n\n# Test res_where with exact value\nfiltered = res_where(results, 'age', value='30')\nassert len(filtered) == 2\nprint(f\"✓ res_where (exact) works: {len(filtered)} rows with age=30\")\n\n# Test res_where with pattern\nfiltered_pattern = res_where(results, 's', pattern='alice')\nassert len(filtered_pattern) == 1\nprint(f\"✓ res_where (pattern) works: {len(filtered_pattern)} rows matching 'alice'\")\n\n# Test res_group\ngroups = res_group(results, 'age')\nassert len(groups) == 2  # Two distinct ages\nassert groups[0][1] == 2  # Age '30' appears twice\nprint(f\"✓ res_group works: {groups}\")\n\n# Test res_distinct\ndistinct_ages = res_distinct(results, 'age')\nassert len(distinct_ages) == 2\nassert '25' in distinct_ages and '30' in distinct_ages\nprint(f\"✓ res_distinct works: {distinct_ages}\")\n\n# Test ResultTable wrapper\nresult_table = ResultTable(\n    rows=results,\n    columns=['s', 'age'],\n    query='SELECT ?s ?age WHERE { ?s &lt;http://ex.org/age&gt; ?age }',\n    total_rows=len(results)\n)\nassert len(result_table) == 3\nprint(f\"✓ ResultTable works: {result_table}\")\n\n# Test result table views work with ResultTable\nhead_from_table = res_head(result_table, n=2)\nassert len(head_from_table) == 2\nprint(f\"✓ res_head works with ResultTable\")\n\n✓ res_head works: 2 rows\n✓ res_where (exact) works: 2 rows with age=30\n✓ res_where (pattern) works: 1 rows matching 'alice'\n✓ res_group works: [('30', 2), ('25', 1)]\n✓ res_distinct works: ['25', '30']\n✓ ResultTable works: ResultTable(3 rows, columns=['s', 'age'])\n✓ res_head works with ResultTable\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test dataset creation\ntest_ns = {}\nresult = setup_dataset_context(test_ns, name='test_ds')\nassert 'test_ds' in test_ns\nassert 'test_ds_meta' in test_ns\nassert len(test_ns['test_ds_meta'].session_id) == 8\nprint(\"✓ Dataset creation works\")\nprint(result)\n\n✓ Dataset creation works\nCreated dataset 'test_ds' with session_id=cb576fb5\n\n\n\n# Test mem_add with provenance\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\nresult = mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob', \n                 source='test', reason='Testing')\nassert len(ds_meta.mem) == 1\nassert len(ds_meta.prov) &gt; 0\nprint(\"✓ mem_add works\")\nprint(result)\n\n✓ mem_add works\nAdded triple to mem: (http://ex.org/alice, http://ex.org/knows, http://ex.org/bob)\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test mem_query\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\nmem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\n\nresults = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s &lt;http://ex.org/age&gt; ?age }')\nassert len(results) == 2\nassert all('s' in r and 'age' in r for r in results)\nprint(\"✓ mem_query works\")\nprint(results)\n\n✓ mem_query works\n[{'s': 'http://ex.org/alice', 'age': '30'}, {'s': 'http://ex.org/bob', 'age': '25'}]\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test mem_retract\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\nassert len(ds_meta.mem) == 1\n\nresult = mem_retract(ds_meta, predicate='http://ex.org/age', source='test', reason='Correction')\nassert len(ds_meta.mem) == 0\nassert 'Removed 1 triples' in result\nprint(\"✓ mem_retract works\")\nprint(result)\n\n✓ mem_retract works\nRemoved 1 triples from mem\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n&lt;ipython-input-1-e3f77e94d507&gt;:36: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test mem_describe\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n\ndesc = mem_describe(ds_meta, 'http://ex.org/alice')\nassert 'as_subject' in desc\nassert 'as_object' in desc\nassert len(desc['as_subject']) == 2\nprint(\"✓ mem_describe works\")\nprint(desc)\n\n✓ mem_describe works\n{'uri': 'http://ex.org/alice', 'as_subject': [('http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob'), ('http://ex.org/alice', 'http://ex.org/age', '30')], 'as_object': []}\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test index invalidation\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Access cached property\ninitial_version = ds_meta._version\n_ = ds_meta.graph_stats\n\n# Mutate\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n\n# Check version incremented\nassert ds_meta._version &gt; initial_version\nprint(\"✓ Index invalidation works\")\n\n✓ Index invalidation works\n\n\nDeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n  for ctx in self.dataset.contexts():\n&lt;ipython-input-1-3a8dafc08295&gt;:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test work graph lifecycle\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Create work graph\nuri, graph = work_create(ds_meta, task_id='test_task')\nassert 'work/test_task' in uri\nassert len(ds_meta.work_graphs) == 1\n\n# Add some triples to work graph\ngraph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/temp'), Literal('value')))\nassert len(graph) == 1\n\n# Promote to mem\nresult = work_to_mem(ds_meta, 'test_task', reason='Test promotion')\nassert len(ds_meta.mem) == 1\nassert 'Promoted 1 triples' in result\n\n# Cleanup\nresult = work_cleanup(ds_meta, task_id='test_task')\nassert 'Removed 1 work' in result\nassert len(ds_meta.work_graphs) == 0\n\nprint(\"✓ Work graph lifecycle works\")\n\n✓ Work graph lifecycle works\n\n\nDeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n  return [str(ctx.identifier) for ctx in self.dataset.contexts()\n&lt;ipython-input-1-661dda18a793&gt;:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n\n\n\n# Test snapshot\nimport tempfile\nimport os\n\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Add some data\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n\n# Take snapshot\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.trig', delete=False) as f:\n    snapshot_path = f.name\n\nresult = snapshot_dataset(ds_meta, path=snapshot_path)\nassert os.path.exists(snapshot_path)\nassert 'Snapshot saved' in result\n\n# Load snapshot (let it auto-detect the name 'ds' from graph URIs)\ntest_ns2 = {}\nresult = load_snapshot(snapshot_path, test_ns2, name='restored')\nassert 'restored' in test_ns2\nassert 'restored_meta' in test_ns2\n# Should auto-detect original name 'ds' and use it for URIs\nassert len(test_ns2['restored_meta'].mem) == 1\n\n# Also test loading with same name\ntest_ns3 = {}\nresult = load_snapshot(snapshot_path, test_ns3, name='ds')\nassert 'ds' in test_ns3\nassert 'ds_meta' in test_ns3\nassert len(test_ns3['ds_meta'].mem) == 1\n\n# Cleanup\nos.unlink(snapshot_path)\n\nprint(\"✓ Snapshot roundtrip works\")\n\n✓ Snapshot roundtrip works\n\n\n\n# Test bounded view functions\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Add some data\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\nwork_create(ds_meta, 'task1')\nwork_create(ds_meta, 'task2')\n\n# Test dataset_stats\nstats = dataset_stats(ds_meta)\nassert 'mem: 1 triples' in stats\nassert 'work graphs: 2' in stats\n\n# Test list_graphs\ngraphs = list_graphs(ds_meta)\nassert len(graphs) &gt;= 4  # mem, prov, work/task1, work/task2\n\n# Test list_graphs with pattern\nwork_graphs = list_graphs(ds_meta, pattern='work/')\nassert len(work_graphs) == 2\n\n# Test graph_sample\nmem_uri = f'urn:rlm:{ds_meta.name}:mem'\nsample = graph_sample(ds_meta, mem_uri)\nassert len(sample) == 1\n\nprint(\"✓ Bounded view functions work\")\n\n✓ Bounded view functions work\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n&lt;ipython-input-1-338468221890&gt;:54: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n  return [str(ctx.identifier) for ctx in self.dataset.contexts()\n&lt;ipython-input-1-338468221890&gt;:44: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n  for ctx in self.dataset.contexts():\n&lt;ipython-input-1-0d2d1cee68f5&gt;:13: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n  for ctx in ds_meta.dataset.contexts():",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#tests",
    "href": "dataset_memory.html#tests",
    "title": "dataset",
    "section": "Tests",
    "text": "Tests",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "dataset_memory.html#usage-examples",
    "href": "dataset_memory.html#usage-examples",
    "title": "dataset",
    "section": "Usage Examples",
    "text": "Usage Examples\n\n# Basic usage in RLM context\nns = {}\nsetup_dataset_context(ns)\n\n# RLM can now use: mem_add, mem_query, mem_describe, etc.\nns['mem_add']('http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\nresults = ns['mem_query']('SELECT ?s ?p ?o WHERE { ?s ?p ?o }')\nprint(results)\n\n\n# Integration with ontology\nfrom rlm.ontology import setup_ontology_context\n\nns = {}\nsetup_dataset_context(ns)\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\n\n# Mount ontology into dataset\nns['mount_ontology']('ontology/prov.ttl', 'prov')\n\n# Now ontology is in dataset as onto/prov graph\ngraphs = ns['list_graphs']()\nprint(graphs)",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "sparql_handles.html",
    "href": "sparql_handles.html",
    "title": "sparql_handles",
    "section": "",
    "text": "This module implements Stage 3 from the trajectory: SPARQL query execution with first-class result handles.\n\n\nEvery SPARQL execution produces a SPARQLResultHandle with: - meta: query, endpoint/local, timestamp, row count, columns - rows: stored internally as list of dicts (SELECT) or Graph (CONSTRUCT/DESCRIBE) - Bounded view operations: res_head(), res_where(), res_group(), res_sample()\n\n\n\nResult handles enable the root model to refine queries by inspecting metadata and small slices, not rerunning blind queries.\n\n\n\nSPARQL results can optionally be stored in dataset work graphs with full provenance tracking.",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#overview",
    "href": "sparql_handles.html#overview",
    "title": "sparql_handles",
    "section": "",
    "text": "This module implements Stage 3 from the trajectory: SPARQL query execution with first-class result handles.\n\n\nEvery SPARQL execution produces a SPARQLResultHandle with: - meta: query, endpoint/local, timestamp, row count, columns - rows: stored internally as list of dicts (SELECT) or Graph (CONSTRUCT/DESCRIBE) - Bounded view operations: res_head(), res_where(), res_group(), res_sample()\n\n\n\nResult handles enable the root model to refine queries by inspecting metadata and small slices, not rerunning blind queries.\n\n\n\nSPARQL results can optionally be stored in dataset work graphs with full provenance tracking.",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#imports",
    "href": "sparql_handles.html#imports",
    "title": "sparql_handles",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#sparqlresulthandle",
    "href": "sparql_handles.html#sparqlresulthandle",
    "title": "sparql_handles",
    "section": "SPARQLResultHandle",
    "text": "SPARQLResultHandle\nUnified wrapper for all SPARQL result types with metadata and bounded view operations.\n\n\nSPARQLResultHandle\n\ndef SPARQLResultHandle(\n    rows:list | rdflib.graph.Graph, result_type:str, query:str, endpoint:str, timestamp:str=&lt;factory&gt;,\n    columns:list=None, total_rows:int=0, triple_count:int=0, total_triples:int=0\n)-&gt;None:\n\nWrapper for SPARQL results with metadata and bounded view operations.\nTest SPARQLResultHandle with different result types:\n\n# Test SELECT result\nselect_handle = SPARQLResultHandle(\n    rows=[{'s': 'http://ex.org/alice', 'age': '30'}],\n    result_type='select',\n    query='SELECT ?s ?age WHERE { ?s :age ?age }',\n    endpoint='local',\n    columns=['s', 'age'],\n    total_rows=1\n)\nassert select_handle.summary() == \"SELECT: 1 rows, columns=['s', 'age']\"\nassert len(select_handle) == 1\nprint(f\"✓ SELECT handle: {select_handle}\")\n\n# Test SELECT with truncation\ntruncated_select = SPARQLResultHandle(\n    rows=[{'s': 'http://ex.org/alice', 'age': '30'}],\n    result_type='select',\n    query='SELECT ?s ?age WHERE { ?s :age ?age }',\n    endpoint='local',\n    columns=['s', 'age'],\n    total_rows=100  # More than stored\n)\nassert '(of 100 total)' in truncated_select.summary()\nprint(f\"✓ Truncated SELECT handle: {truncated_select}\")\n\n# Test ASK result\nask_handle = SPARQLResultHandle(\n    rows=True,\n    result_type='ask',\n    query='ASK { ?s ?p ?o }',\n    endpoint='local'\n)\nassert ask_handle.summary() == \"ASK: True\"\nprint(f\"✓ ASK handle: {ask_handle}\")\n\n# Test CONSTRUCT result\ng = Graph()\ng.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/age'), Literal('30')))\nconstruct_handle = SPARQLResultHandle(\n    rows=g,\n    result_type='construct',\n    query='CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }',\n    endpoint='local',\n    triple_count=1,\n    total_triples=1\n)\nassert construct_handle.summary() == \"CONSTRUCT: 1 triples\"\nprint(f\"✓ CONSTRUCT handle: {construct_handle}\")\n\n# Test CONSTRUCT with truncation\ntruncated_construct = SPARQLResultHandle(\n    rows=g,\n    result_type='construct',\n    query='CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }',\n    endpoint='local',\n    triple_count=1,\n    total_triples=500  # More than stored\n)\nassert '(of 500 total)' in truncated_construct.summary()\nprint(f\"✓ Truncated CONSTRUCT handle: {truncated_construct}\")\n\n✓ SELECT handle: SPARQLResultHandle(SELECT: 1 rows, columns=['s', 'age'])\n✓ Truncated SELECT handle: SPARQLResultHandle(SELECT: 1 rows (of 100 total), columns=['s', 'age'])\n✓ ASK handle: SPARQLResultHandle(ASK: True)\n✓ CONSTRUCT handle: SPARQLResultHandle(CONSTRUCT: 1 triples)\n✓ Truncated CONSTRUCT handle: SPARQLResultHandle(CONSTRUCT: 1 triples (of 500 total))\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#remote-sparql-query",
    "href": "sparql_handles.html#remote-sparql-query",
    "title": "sparql_handles",
    "section": "Remote SPARQL Query",
    "text": "Remote SPARQL Query\nExecute SPARQL queries against remote endpoints and return result handles.",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#query-rewriting-helper",
    "href": "sparql_handles.html#query-rewriting-helper",
    "title": "sparql_handles",
    "section": "Query Rewriting Helper",
    "text": "Query Rewriting Helper\nHelper to inject LIMIT clauses into SELECT queries to bound server-side work.\n\n# Test LIMIT injection\nq1 = \"SELECT ?s ?p ?o WHERE { ?s ?p ?o }\"\nmodified, injected = _inject_limit(q1, 100)\nassert injected == True\nassert 'LIMIT 100' in modified\nprint(f\"✓ Basic injection: {modified}\")\n\n# Test with existing LIMIT (should not modify)\nq2 = \"SELECT ?s WHERE { ?s ?p ?o } LIMIT 50\"\nmodified, injected = _inject_limit(q2, 100)\nassert injected == False\nassert modified == q2\nprint(f\"✓ Existing LIMIT preserved: {modified}\")\n\n# Test with ORDER BY (inject before it)\nq3 = \"SELECT ?s ?o WHERE { ?s ?p ?o } ORDER BY ?s\"\nmodified, injected = _inject_limit(q3, 100)\nassert injected == True\nassert 'LIMIT 100' in modified\nassert modified.index('LIMIT') &lt; modified.index('ORDER')\nprint(f\"✓ Injection before ORDER BY: {modified}\")\n\n# Test CONSTRUCT (should not inject)\nq4 = \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\"\nmodified, injected = _inject_limit(q4, 100)\nassert injected == False\nprint(f\"✓ CONSTRUCT not modified: {modified}\")\n\n✓ Basic injection: SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 100\n✓ Existing LIMIT preserved: SELECT ?s WHERE { ?s ?p ?o } LIMIT 50\n✓ Injection before ORDER BY: SELECT ?s ?o WHERE { ?s ?p ?o }  LIMIT 100 ORDER BY ?s\n✓ CONSTRUCT not modified: CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\n\n\n\n\nsparql_query\n\ndef sparql_query(\n    query:str, endpoint:str='https://query.wikidata.org/sparql', max_results:int=100, name:str='res', ns:dict=None,\n    timeout:float=30.0, ds_meta:NoneType=None, # Dataset integration\n    store_in_work:bool=False, work_task_id:str=None\n)-&gt;str:\n\nExecute SPARQL query, store SPARQLResultHandle in namespace.\nFor SELECT: Stores SPARQLResultHandle with rows as list of dicts For CONSTRUCT/DESCRIBE: Stores SPARQLResultHandle with rdflib.Graph For ASK: Stores SPARQLResultHandle with boolean result\nIMPORTANT - Work Bounds: - For SELECT: Automatically injects LIMIT clause to bound server-side work - For CONSTRUCT/DESCRIBE: max_results only truncates locally; full results still fetched from endpoint (SPARQL 1.1 has no standard LIMIT for graphs)\nIf ds_meta provided and store_in_work=True: - CONSTRUCT results stored in work/ graph - Query logged to prov graph\nArgs: query: SPARQL query string endpoint: SPARQL endpoint URL max_results: Maximum results to return (for SELECT/CONSTRUCT) name: Variable name to store result handle ns: Namespace dict (defaults to globals()) timeout: Query timeout in seconds ds_meta: Optional DatasetMeta for dataset integration store_in_work: If True and ds_meta provided, store CONSTRUCT results in work graph work_task_id: Task ID for work graph (auto-generated if None)\nReturns: Summary string describing the result\nTest against Wikidata:\n\n# Test SELECT query against Wikidata\ntest_ns = {}\nresult = sparql_query(\n    \"SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 5\",\n    ns=test_ns,\n    name='wikidata_test'\n)\nprint(result)\nassert 'wikidata_test' in test_ns\nassert isinstance(test_ns['wikidata_test'], SPARQLResultHandle)\nassert test_ns['wikidata_test'].result_type == 'select'\nassert len(test_ns['wikidata_test'].rows) == 5\nprint(f\"✓ SELECT query works: {test_ns['wikidata_test'].summary()}\")\n\n# Test CONSTRUCT query\nresult = sparql_query(\n    \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o } LIMIT 3\",\n    ns=test_ns,\n    name='graph_test'\n)\nprint(result)\nassert test_ns['graph_test'].result_type == 'construct'\nassert isinstance(test_ns['graph_test'].rows, Graph)\nprint(f\"✓ CONSTRUCT query works: {test_ns['graph_test'].summary()}\")",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#local-graph-query",
    "href": "sparql_handles.html#local-graph-query",
    "title": "sparql_handles",
    "section": "Local Graph Query",
    "text": "Local Graph Query\nExecute SPARQL queries against local rdflib graphs (mounted ontologies or work graphs).\n\n\nsparql_local\n\ndef sparql_local(\n    query:str, graph:rdflib.graph.Graph | str, max_results:int=100, name:str='res', ns:dict=None\n)-&gt;str:\n\nExecute SPARQL query on local rdflib Graph.\nUseful for querying mounted ontologies or work graphs. Returns SPARQLResultHandle same as sparql_query().\nIMPORTANT - Work Bounds: - max_results is output truncation only; full result set is materialized - For large local graphs, consider filtering in the SPARQL query itself\nArgs: query: SPARQL query string graph: rdflib.Graph object or name of graph in namespace max_results: Maximum results to return name: Variable name to store result handle ns: Namespace dict (defaults to globals())\nReturns: Summary string describing the result\nTest with local graph:\n\n# Create test graph\ntest_graph = Graph()\ntest_graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/age'), Literal('30')))\ntest_graph.add((URIRef('http://ex.org/bob'), URIRef('http://ex.org/age'), Literal('25')))\ntest_graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/city'), Literal('Boston')))\n\ntest_ns = {'my_graph': test_graph}\n\n# Test SELECT query on local graph\nresult = sparql_local(\n    \"SELECT ?s ?age WHERE { ?s &lt;http://ex.org/age&gt; ?age }\",\n    'my_graph',\n    ns=test_ns,\n    name='local_res'\n)\nprint(result)\nassert 'local_res' in test_ns\nassert test_ns['local_res'].result_type == 'select'\nassert len(test_ns['local_res'].rows) == 2\nassert test_ns['local_res'].total_rows == 2\nprint(f\"✓ Local SELECT query works: {test_ns['local_res'].rows}\")\n\n# Test CONSTRUCT on local graph\nresult = sparql_local(\n    \"CONSTRUCT { ?s &lt;http://ex.org/age&gt; ?age } WHERE { ?s &lt;http://ex.org/age&gt; ?age }\",\n    test_graph,\n    ns=test_ns,\n    name='local_graph'\n)\nprint(result)\nassert test_ns['local_graph'].result_type == 'construct'\nassert len(test_ns['local_graph'].rows) == 2\nassert test_ns['local_graph'].triple_count == 2\nassert test_ns['local_graph'].total_triples == 2\nprint(f\"✓ Local CONSTRUCT query works\")\n\n# Test truncation\nresult = sparql_local(\n    \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\",\n    test_graph,\n    max_results=2,\n    ns=test_ns,\n    name='truncated'\n)\nprint(result)\nassert len(test_ns['truncated'].rows) == 2\nassert test_ns['truncated'].triple_count == 2\nassert test_ns['truncated'].total_triples == 3  # Original had 3\nassert '(of 3 total)' in test_ns['truncated'].summary()\nprint(f\"✓ Truncation works correctly: {test_ns['truncated'].summary()}\")\n\nSELECT result with 2 rows, columns: ['s', 'age'], stored in 'local_res'\n✓ Local SELECT query works: [{'s': rdflib.term.URIRef('http://ex.org/alice'), 'age': rdflib.term.Literal('30')}, {'s': rdflib.term.URIRef('http://ex.org/bob'), 'age': rdflib.term.Literal('25')}]\nGraph with 2 triples stored in 'local_graph'\n✓ Local CONSTRUCT query works\nGraph with 2 triples stored in 'truncated' (of 3 total)\n✓ Truncation works correctly: CONSTRUCT: 2 triples (of 3 total)\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#view-operations",
    "href": "sparql_handles.html#view-operations",
    "title": "sparql_handles",
    "section": "View Operations",
    "text": "View Operations\nBounded view functions for progressive disclosure over result sets.\nThese functions work with SPARQLResultHandle, ResultTable, or plain lists.\n\n\nres_sample\n\ndef res_sample(\n    result, n:int=10, seed:int=None\n)-&gt;list:\n\nGet random sample of N rows from result.\nArgs: result: SPARQLResultHandle, ResultTable, or list n: Number of rows to sample seed: Optional random seed for reproducibility\nReturns: List of sampled rows\nTest res_sample:\n\n# Test with list\ntest_list = [{'x': i} for i in range(20)]\nsample = res_sample(test_list, n=5, seed=42)\nassert len(sample) == 5\nassert all(isinstance(item, dict) for item in sample)\nprint(f\"✓ res_sample works with list: {len(sample)} items\")\n\n# Test with SPARQLResultHandle\nhandle = SPARQLResultHandle(\n    rows=[{'s': f'http://ex.org/item{i}'} for i in range(15)],\n    result_type='select',\n    query='SELECT ?s WHERE { ?s ?p ?o }',\n    endpoint='local',\n    columns=['s'],\n    total_rows=15\n)\nsample = res_sample(handle, n=3, seed=42)\nassert len(sample) == 3\nprint(f\"✓ res_sample works with SPARQLResultHandle\")\n\n# Test with small result (no sampling needed)\nsmall_list = [1, 2, 3]\nsample = res_sample(small_list, n=10)\nassert len(sample) == 3\nprint(f\"✓ res_sample handles small results correctly\")\n\n✓ res_sample works with list: 5 items\n✓ res_sample works with SPARQLResultHandle\n✓ res_sample handles small results correctly\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#setup-function",
    "href": "sparql_handles.html#setup-function",
    "title": "sparql_handles",
    "section": "Setup Function",
    "text": "Setup Function\nInitialize SPARQL tools in namespace for RLM sessions.\n\n\nsetup_sparql_context\n\ndef setup_sparql_context(\n    ns:dict, default_endpoint:str='https://query.wikidata.org/sparql', ds_meta:NoneType=None\n)-&gt;str:\n\nInitialize SPARQL tools in namespace.\nBinds: - sparql_query() with default endpoint - sparql_local() if ds_meta provided - res_head(), res_where(), res_group(), res_distinct(), res_sample()\nArgs: ns: Namespace dict where functions will be bound default_endpoint: Default SPARQL endpoint URL ds_meta: Optional DatasetMeta for dataset integration\nReturns: Status message\nTest setup function:\n\n# Test basic setup\ntest_ns = {}\nresult = setup_sparql_context(test_ns)\nprint(result)\nassert 'sparql_query' in test_ns\nassert 'sparql_local' in test_ns\nassert 'res_sample' in test_ns\nprint(f\"✓ Setup function works\")\n\n# Test with dataset integration\ntry:\n    from rlm.dataset import DatasetMeta\n    from rdflib import Dataset\n    \n    ds = Dataset()\n    ds_meta = DatasetMeta(ds, name='test')\n    \n    test_ns2 = {}\n    result = setup_sparql_context(test_ns2, ds_meta=ds_meta)\n    print(result)\n    assert 'session:' in result\n    print(f\"✓ Setup with dataset integration works\")\nexcept ImportError:\n    print(\"⊘ Dataset module not available, skipping integration test\")\n\nSPARQL context initialized with endpoint: https://query.wikidata.org/sparql\nBound functions: sparql_query, sparql_local, res_sample, res_head, res_where, res_group, res_distinct\n✓ Setup function works\nSPARQL context initialized with endpoint: https://query.wikidata.org/sparql\nDataset integration enabled (session: d6379b48)\nBound functions: sparql_query, sparql_local, res_sample, res_head, res_where, res_group, res_distinct\n✓ Setup with dataset integration works",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "sparql_handles.html#usage-examples",
    "href": "sparql_handles.html#usage-examples",
    "title": "sparql_handles",
    "section": "Usage Examples",
    "text": "Usage Examples\nEnd-to-end examples showing SPARQL handles in RLM context.\n\n# Example 1: Basic SPARQL workflow\nns = {}\nsetup_sparql_context(ns)\n\n# Execute query (LLM would do this)\nns['sparql_query']('SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 10', name='results')\n\n# Inspect results\nprint(ns['results'].summary())\nprint(ns['res_head'](ns['results'], 5))\nprint(ns['res_sample'](ns['results'], 3))\n\n\n# Example 2: Dataset integration\nfrom rlm.dataset import setup_dataset_context\n\nns = {}\nsetup_dataset_context(ns)\nsetup_sparql_context(ns, ds_meta=ns['ds_meta'])\n\n# Query and store in work graph\nns['sparql_query'](\n    'CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o } LIMIT 5',\n    name='discovered_triples',\n    store_in_work=True,\n    work_task_id='discovery_1'\n)\n\n# Check provenance\nprint(ns['dataset_stats']())\n\n\n# Example 3: Local graph queries\nfrom rlm.ontology import setup_ontology_context\n\nns = {}\nsetup_sparql_context(ns)\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\n\n# Query mounted ontology\nns['sparql_local'](\n    'SELECT ?c WHERE { ?c a &lt;http://www.w3.org/2002/07/owl#Class&gt; }',\n    'prov',\n    name='classes'\n)\n\nprint(f\"Found {len(ns['classes'].rows)} classes\")\nprint(ns['res_head'](ns['classes'], 10))",
    "crumbs": [
      "sparql_handles"
    ]
  },
  {
    "objectID": "procedural_memory.html",
    "href": "procedural_memory.html",
    "title": "procedural_memory",
    "section": "",
    "text": "This module implements Stage 2.5: Procedural Memory Loop inspired by the ReasoningBank paper. The goal is to enable an RLM agent to improve over time by accumulating procedural knowledge (strategies, templates, debugging moves) without replacing evidence-based retrieval.\n\n\n┌──────────┐    ┌──────────┐    ┌──────────┐\n│ RETRIEVE │───▶│ INTERACT │───▶│ EXTRACT  │\n│ (BM25)   │    │ (rlm_run)│    │ (Judge + │\n└────▲─────┘    └──────────┘    │ Extractor)│\n     │                          └─────┬─────┘\n     │                                │\n     │          ┌──────────┐          │\n     └──────────│  STORE   │◀─────────┘\n                │ (JSON)   │\n                └──────────┘\n\n\n\n\nProcedural, not episodic: Memories are strategies/checklists, not retellings\nBounded injection: Only title + description + 3 key bullets in prompts\nEvidence-sensitive judgment: Success requires grounding in retrieved evidence\nKeyword retrieval: BM25 over title/description/tags (deterministic, offline)\nAppend-only storage: Simple JSON file for experimentation\n\n\n\n\n\nReasoningBank Paper",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#overview",
    "href": "procedural_memory.html#overview",
    "title": "procedural_memory",
    "section": "",
    "text": "This module implements Stage 2.5: Procedural Memory Loop inspired by the ReasoningBank paper. The goal is to enable an RLM agent to improve over time by accumulating procedural knowledge (strategies, templates, debugging moves) without replacing evidence-based retrieval.\n\n\n┌──────────┐    ┌──────────┐    ┌──────────┐\n│ RETRIEVE │───▶│ INTERACT │───▶│ EXTRACT  │\n│ (BM25)   │    │ (rlm_run)│    │ (Judge + │\n└────▲─────┘    └──────────┘    │ Extractor)│\n     │                          └─────┬─────┘\n     │                                │\n     │          ┌──────────┐          │\n     └──────────│  STORE   │◀─────────┘\n                │ (JSON)   │\n                └──────────┘\n\n\n\n\nProcedural, not episodic: Memories are strategies/checklists, not retellings\nBounded injection: Only title + description + 3 key bullets in prompts\nEvidence-sensitive judgment: Success requires grounding in retrieved evidence\nKeyword retrieval: BM25 over title/description/tags (deterministic, offline)\nAppend-only storage: Simple JSON file for experimentation\n\n\n\n\n\nReasoningBank Paper",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#imports",
    "href": "procedural_memory.html#imports",
    "title": "procedural_memory",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#memory-schema",
    "href": "procedural_memory.html#memory-schema",
    "title": "procedural_memory",
    "section": "Memory Schema",
    "text": "Memory Schema\nA MemoryItem represents a reusable procedural insight extracted from an RLM trajectory.\nConstraints: - Items must be small enough to inject into prompts - content should be procedural (steps/checklist), not a retelling - Up to 3 items extracted per trajectory\n\n\nMemoryItem\n\ndef MemoryItem(\n    id:str, title:str, description:str, content:str, source_type:str, task_query:str, created_at:str,\n    access_count:int=0, tags:Optional=None, session_id:Optional=None\n)-&gt;None:\n\nA reusable procedural memory extracted from an RLM trajectory.\nAttributes: id: Unique identifier (UUID) title: Concise identifier (≤10 words) description: One-sentence summary content: Procedural steps/checklist/template (Markdown) source_type: ‘success’ or ‘failure’ task_query: Original task that produced this memory created_at: ISO timestamp access_count: Number of times retrieved (for future consolidation) tags: Keywords for BM25 retrieval session_id: Optional session ID from DatasetMeta (links to dataset session)\n\n# Test MemoryItem creation and serialization\ntest_item = MemoryItem(\n    id='test-uuid',\n    title='SPARQL Query Pattern',\n    description='Template for searching entities by label.',\n    content='- Use `rdfs:label` for human-readable names\\n- Add FILTER for case-insensitive search',\n    source_type='success',\n    task_query='Find entities named \"Activity\"',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['sparql', 'search', 'rdfs']\n)\n\n# Test roundtrip\ndata = test_item.to_dict()\nrestored = MemoryItem.from_dict(data)\nassert restored.title == test_item.title\nassert restored.tags == test_item.tags\nprint(\"✓ MemoryItem serialization works\")\n\n✓ MemoryItem serialization works\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#memory-store",
    "href": "procedural_memory.html#memory-store",
    "title": "procedural_memory",
    "section": "Memory Store",
    "text": "Memory Store\nPersistent storage for procedural memories using a simple JSON file format.\n\n\nMemoryStore\n\ndef MemoryStore(\n    memories:list=&lt;factory&gt;, path:Optional=None\n)-&gt;None:\n\nPersistent storage for procedural memories.\nAttributes: memories: List of MemoryItem objects path: Path to JSON file\n\n# Test MemoryStore save/load roundtrip\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    test_path = Path(tmpdir) / 'test_memories.json'\n    \n    # Create store and add items\n    store = MemoryStore(path=test_path)\n    item1 = MemoryItem(\n        id=str(uuid.uuid4()),\n        title='Test Memory 1',\n        description='First test memory',\n        content='- Step 1\\n- Step 2',\n        source_type='success',\n        task_query='test task 1',\n        created_at=datetime.now(timezone.utc).isoformat(),\n        tags=['test', 'example']\n    )\n    item2 = MemoryItem(\n        id=str(uuid.uuid4()),\n        title='Test Memory 2',\n        description='Second test memory',\n        content='- Action A\\n- Action B',\n        source_type='failure',\n        task_query='test task 2',\n        created_at=datetime.now(timezone.utc).isoformat(),\n        tags=['test']\n    )\n    \n    store.add(item1)\n    store.add(item2)\n    store.save()\n    \n    # Load and verify\n    loaded = MemoryStore.load(test_path)\n    assert len(loaded.memories) == 2\n    assert loaded.memories[0].title == 'Test Memory 1'\n    assert loaded.memories[1].source_type == 'failure'\n    assert loaded.memories[0].tags == ['test', 'example']\n    \n    # Test corpus generation\n    corpus = loaded.get_corpus_for_bm25()\n    assert len(corpus) == 2\n    assert 'test' in corpus[0]  # From title and tags\n    \n    print(\"✓ MemoryStore save/load/corpus works\")\n\n✓ MemoryStore save/load/corpus works",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#trajectory-artifact",
    "href": "procedural_memory.html#trajectory-artifact",
    "title": "procedural_memory",
    "section": "Trajectory Artifact",
    "text": "Trajectory Artifact\nExtract a bounded representation of an RLM run for the judge and extractor.\nPurpose: Summarize iterations into key steps (~10 max) with actions and outcomes.\n\n\nextract_trajectory_artifact\n\ndef extract_trajectory_artifact(\n    task:str, answer:str, iterations:list, ns:dict\n)-&gt;dict:\n\nCreate bounded trajectory artifact for judge/extractor.\nSummarizes each iteration’s code blocks into 1-2 line “action + outcome”, limiting to ~10 most informative key steps.\nArgs: task: Original task query answer: Final answer from rlm_run iterations: List of RLMIteration objects ns: Final namespace dict\nReturns: Dictionary with keys: - task: str - final_answer: str - iteration_count: int - converged: bool (whether final_answer was set) - key_steps: List of {iteration, action, outcome} - variables_created: List of variable names in ns - errors_encountered: List of error messages from stderr\n\n# Test with mock iterations\nfrom rlm._rlmpaper_compat import CodeBlock, REPLResult\n\nmock_block1 = CodeBlock(\n    code=\"search('Activity')\",\n    result=REPLResult(stdout=\"Found 3 entities\", stderr=None, locals={})\n)\nmock_block2 = CodeBlock(\n    code=\"describe_entity('prov:Activity')\",\n    result=REPLResult(stdout=\"prov:Activity is a class\", stderr=None, locals={})\n)\nmock_iteration = RLMIteration(\n    prompt=\"test prompt\",\n    response=\"test response\",\n    code_blocks=[mock_block1, mock_block2],\n    final_answer=None,\n    iteration_time=0.5\n)\n\nartifact = extract_trajectory_artifact(\n    task=\"What is prov:Activity?\",\n    answer=\"prov:Activity is a class\",\n    iterations=[mock_iteration],\n    ns={'result': 'prov:Activity is a class'}\n)\n\nassert artifact['task'] == \"What is prov:Activity?\"\nassert artifact['iteration_count'] == 1\nassert artifact['converged'] == True\nassert len(artifact['key_steps']) == 2\nassert 'search' in artifact['key_steps'][0]['action'].lower()\nassert len(artifact['variables_created']) == 1\nprint(\"✓ Trajectory artifact extraction works\")\n\n✓ Trajectory artifact extraction works",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#judge",
    "href": "procedural_memory.html#judge",
    "title": "procedural_memory",
    "section": "Judge",
    "text": "Judge\nClassify trajectory as success or failure with evidence-sensitivity.\nSuccess criteria: 1. Answer directly addresses the task 2. Answer is grounded in retrieved evidence (not hallucinated) 3. Reasoning shows systematic exploration\nFailure indicators: 1. No answer produced (didn’t converge) 2. Answer doesn’t address the task 3. Answer makes claims without supporting evidence\n\n\njudge_trajectory\n\ndef judge_trajectory(\n    artifact:dict, ns:dict=None\n)-&gt;dict:\n\nJudge trajectory success using llm_query.\nEvidence-sensitive: success requires grounding in retrieved evidence.\nArgs: artifact: Trajectory artifact from extract_trajectory_artifact() ns: Optional namespace for additional context\nReturns: Dictionary with keys: - is_success: bool - reason: str - confidence: str (‘high’, ‘medium’, ‘low’) - missing: list[str] (what evidence was lacking if failure)\n\n# Test judge with real LLM (requires API key)\ntest_artifact = {\n    'task': 'What is prov:Activity?',\n    'final_answer': 'prov:Activity is a class representing activities in PROV ontology',\n    'iteration_count': 2,\n    'converged': True,\n    'key_steps': [\n        {'iteration': 1, 'action': \"search('Activity')\", 'outcome': 'Found 3 entities'},\n        {'iteration': 2, 'action': \"describe_entity('prov:Activity')\", 'outcome': 'A class in PROV'}\n    ],\n    'variables_created': ['result'],\n    'errors_encountered': []\n}\n\njudgment = judge_trajectory(test_artifact)\nprint(f\"Success: {judgment['is_success']}\")\nprint(f\"Reason: {judgment['reason']}\")\nprint(f\"Confidence: {judgment['confidence']}\")",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#extractor",
    "href": "procedural_memory.html#extractor",
    "title": "procedural_memory",
    "section": "Extractor",
    "text": "Extractor\nExtract 1-3 reusable memory items from a trajectory.\nFor successes: Emphasize why the approach worked\nFor failures: Emphasize what to avoid and recovery strategies\nOutput format: Procedural (steps/checklist/template), NOT a retelling\n\n\nextract_memories\n\ndef extract_memories(\n    artifact:dict, judgment:dict, ns:dict=None\n)-&gt;list:\n\nExtract up to 3 reusable memory items from trajectory.\nArgs: artifact: Trajectory artifact from extract_trajectory_artifact() judgment: Judgment dict from judge_trajectory() ns: Optional namespace for additional context\nReturns: List of MemoryItem objects (0-3 items)\n\n# Test extractor with real LLM\ntest_artifact = {\n    'task': 'Find properties of prov:Activity',\n    'final_answer': 'prov:Activity has properties: prov:startedAtTime, prov:endedAtTime',\n    'iteration_count': 3,\n    'converged': True,\n    'key_steps': [\n        {'iteration': 1, 'action': \"search('Activity')\", 'outcome': 'Found prov:Activity'},\n        {'iteration': 2, 'action': \"describe_entity('prov:Activity')\", 'outcome': 'A class'},\n        {'iteration': 3, 'action': \"get_properties('prov:Activity')\", 'outcome': 'Listed properties'}\n    ],\n    'variables_created': ['activity_props'],\n    'errors_encountered': []\n}\n\ntest_judgment = {\n    'is_success': True,\n    'reason': 'Answer grounded in ontology data',\n    'confidence': 'high',\n    'missing': []\n}\n\nmemories = extract_memories(test_artifact, test_judgment)\nprint(f\"Extracted {len(memories)} memories:\")\nfor m in memories:\n    print(f\"  - {m.title}\")\n    print(f\"    Tags: {m.tags}\")",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#bm25-retrieval",
    "href": "procedural_memory.html#bm25-retrieval",
    "title": "procedural_memory",
    "section": "BM25 Retrieval",
    "text": "BM25 Retrieval\nFind relevant memories for new tasks using keyword-based BM25 retrieval.\nSearches over: title + description + tags\n\n\nretrieve_memories\n\ndef retrieve_memories(\n    store:MemoryStore, task:str, k:int=3\n)-&gt;list:\n\nRetrieve top-k relevant memories using BM25.\nTokenizes task and searches over title + description + tags.\nArgs: store: MemoryStore instance task: Task query string k: Number of memories to retrieve\nReturns: List of top-k MemoryItem objects (may be fewer if scores ≤ 0)\n\n# Test BM25 retrieval\ntest_store = MemoryStore()\n\n# Add diverse memories\ntest_store.add(MemoryItem(\n    id=str(uuid.uuid4()),\n    title='SPARQL query pattern for entity search',\n    description='Use rdfs:label with FILTER for case-insensitive search.',\n    content='- Step 1\\n- Step 2',\n    source_type='success',\n    task_query='Find entities by name',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['sparql', 'search', 'entity']\n))\n\ntest_store.add(MemoryItem(\n    id=str(uuid.uuid4()),\n    title='Property exploration strategy',\n    description='Systematically explore properties using describe then probe.',\n    content='- Action A\\n- Action B',\n    source_type='success',\n    task_query='What properties does X have?',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['properties', 'exploration']\n))\n\ntest_store.add(MemoryItem(\n    id=str(uuid.uuid4()),\n    title='Debugging failed SPARQL queries',\n    description='Check syntax, namespaces, and endpoint first.',\n    content='- Check 1\\n- Check 2',\n    source_type='failure',\n    task_query='Query failed with error',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['sparql', 'debugging', 'error']\n))\n\n# Test retrieval for different queries\nresults1 = retrieve_memories(test_store, 'How do I search for entities?', k=2)\nassert len(results1) &lt;= 2\nassert any('search' in r.title.lower() or 'search' in r.tags for r in results1)\nprint(f\"✓ Retrieved {len(results1)} memories for 'search for entities'\")\n\nresults2 = retrieve_memories(test_store, 'My SPARQL query is broken', k=2)\nassert len(results2) &lt;= 2\nassert any('sparql' in r.tags for r in results2)\nprint(f\"✓ Retrieved {len(results2)} memories for 'SPARQL query broken'\")\n\nresults3 = retrieve_memories(test_store, 'What properties does prov:Activity have?', k=2)\nprint(f\"✓ Retrieved {len(results3)} memories for 'properties question'\")\n\n# Test access count increment\nassert results1[0].access_count &gt; 0\nprint(\"✓ Access count tracking works\")\n\n✓ Retrieved 2 memories for 'search for entities'\n✓ Retrieved 2 memories for 'SPARQL query broken'\n✓ Retrieved 2 memories for 'properties question'\n✓ Access count tracking works\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),\n&lt;ipython-input-1-c9306d916f1d&gt;:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),\n&lt;ipython-input-1-c9306d916f1d&gt;:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#injection-formatting",
    "href": "procedural_memory.html#injection-formatting",
    "title": "procedural_memory",
    "section": "Injection Formatting",
    "text": "Injection Formatting\nFormat retrieved memories for bounded prompt injection.\nOutput includes: - Assessment instruction - Title + description + up to 3 key bullets from content\nNever injects full content to maintain bounded prompt size.\n\n\nformat_memories_for_injection\n\ndef format_memories_for_injection(\n    memories:list, max_bullets:int=3\n)-&gt;str:\n\nFormat memories for bounded prompt injection.\nReturns string with: - Assessment instruction - Title + description + key bullets from content (up to max_bullets)\nArgs: memories: List of MemoryItem objects to format max_bullets: Maximum bullets to extract from content\nReturns: Formatted string for prompt injection\n\n# Test injection formatting\ntest_memories = [\n    MemoryItem(\n        id='test-1',\n        title='SPARQL Search Pattern',\n        description='Template for searching entities by label.',\n        content=\"\"\"- Use rdfs:label for human-readable names\n- Add FILTER for case-insensitive matching\n- Include LIMIT to avoid timeout\n- Check for alternative label properties\"\"\",\n        source_type='success',\n        task_query='test',\n        created_at=datetime.now(timezone.utc).isoformat(),\n        tags=['sparql']\n    ),\n    MemoryItem(\n        id='test-2',\n        title='Property Discovery',\n        description='Systematic approach to finding properties.',\n        content=\"\"\"1. Start with describe_entity() for overview\n2. Use get_properties() for full list\n3. Check both domain and range\n4. Look for inverse properties\"\"\",\n        source_type='success',\n        task_query='test',\n        created_at=datetime.now(timezone.utc).isoformat(),\n        tags=['properties']\n    )\n]\n\nformatted = format_memories_for_injection(test_memories, max_bullets=3)\n\n# Verify format\nassert '## Relevant Prior Experience' in formatted\nassert 'assess which of these strategies' in formatted\nassert '### 1. SPARQL Search Pattern' in formatted\nassert '### 2. Property Discovery' in formatted\nassert 'Use rdfs:label' in formatted\nassert 'Start with describe_entity' in formatted\n\n# Verify bullet limiting (should have max 3 bullets per memory)\nlines = formatted.split('\\n')\nbullet_count_mem1 = sum(1 for l in lines[lines.index('### 1. SPARQL Search Pattern'):lines.index('### 2. Property Discovery')] if l.strip().startswith('-'))\nassert bullet_count_mem1 &lt;= 3\n\nprint(\"✓ Injection formatting works\")\nprint(\"\\nFormatted output:\")\nprint(formatted[:300] + \"...\")\n\n✓ Injection formatting works\n\nFormatted output:\n## Relevant Prior Experience\n\nBefore taking action, briefly assess which of these strategies apply to your current task and which do not.\n\n### 1. SPARQL Search Pattern\nTemplate for searching entities by label.\nKey points:\n- Use rdfs:label for human-readable names\n- Add FILTER for case-insensitive ma...\n\n\nDeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),\n&lt;ipython-input-1-2dc0c9d48ca1&gt;:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  created_at=datetime.utcnow().isoformat(),",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#integration",
    "href": "procedural_memory.html#integration",
    "title": "procedural_memory",
    "section": "Integration",
    "text": "Integration\nComplete closed-loop: RETRIEVE → INJECT → INTERACT → EXTRACT → STORE\n\n\nrlm_run_with_memory\n\ndef rlm_run_with_memory(\n    query:str, context:str, memory_store:MemoryStore, ns:dict=None, enable_memory_extraction:bool=True,\n    persist_dataset:bool=False, # NEW: Dataset persistence\n    dataset_path:Path=None, kwargs:VAR_KEYWORD\n)-&gt;tuple:\n\nRLM run with procedural memory loop.\nClosed-loop cycle: 1. RETRIEVE: Get relevant memories via BM25 2. INJECT: Add to context/prompt 3. INTERACT: Run rlm_run() 4. EXTRACT: Judge + extract new memories 5. STORE: Persist new memories\nNEW: Dataset persistence: - If persist_dataset=True and dataset_path provided, loads snapshot before run - After run, if dataset was modified, saves snapshot - Stores snapshot path in extracted MemoryItem for lineage\nArgs: query: Task query string context: Context string (e.g., ontology summary) memory_store: MemoryStore instance for retrieval/storage ns: Optional namespace dict enable_memory_extraction: Whether to extract and store new memories (default True) persist_dataset: Whether to persist dataset snapshots (default False) dataset_path: Optional path for dataset snapshot **kwargs: Additional arguments for rlm_run()\nReturns: Tuple of (answer, iterations, ns, new_memories)\n\n# Integration test (requires full RLM setup)\nfrom rlm.ontology import setup_ontology_context\nimport tempfile\n\ndef test_memory_improves_convergence():\n    \"\"\"Second attempt should benefit from first attempt's memory.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        store = MemoryStore(path=Path(tmpdir) / 'test_integration.json')\n        \n        # First run - no memories\n        ns = {}\n        setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n        \n        answer1, iters1, ns1, mems1 = rlm_run_with_memory(\n            \"What is prov:Activity and what properties does it have?\",\n            ns['prov_meta'].summary(),\n            store,\n            ns=ns\n        )\n        print(f\"\\nFirst run: {len(iters1)} iterations, {len(mems1)} memories extracted\")\n        for mem in mems1:\n            print(f\"  - {mem.title}\")\n        \n        # Second run - similar task, should retrieve memories\n        ns2 = {}\n        setup_ontology_context('ontology/prov.ttl', ns2, name='prov')\n        \n        answer2, iters2, ns2, mems2 = rlm_run_with_memory(\n            \"What is prov:Entity and what properties does it have?\",\n            ns2['prov_meta'].summary(),\n            store,\n            ns=ns2\n        )\n        print(f\"\\nSecond run: {len(iters2)} iterations\")\n        print(f\"Total memories in store: {len(store.memories)}\")\n        \n        # Verify memories were retrieved\n        retrieved_for_second = retrieve_memories(\n            store,\n            \"What is prov:Entity and what properties does it have?\",\n            k=3\n        )\n        print(f\"Memories that would be retrieved for second run: {len(retrieved_for_second)}\")\n        for mem in retrieved_for_second:\n            print(f\"  - {mem.title} (accessed {mem.access_count} times)\")\n\n# Run test\n# test_memory_improves_convergence()",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#usage-examples",
    "href": "procedural_memory.html#usage-examples",
    "title": "procedural_memory",
    "section": "Usage Examples",
    "text": "Usage Examples\nEnd-to-end examples with PROV ontology.\n\n# Full example: Build up procedural memory over multiple queries\nfrom rlm.ontology import setup_ontology_context\nfrom pathlib import Path\n\n# Initialize memory store\nstore = MemoryStore(path=Path('memories/prov_memories.json'))\n\n# If store exists, load it\nif store.path.exists():\n    store = MemoryStore.load(store.path)\n    print(f\"Loaded {len(store.memories)} existing memories\")\n\n# Setup ontology context\nns = {}\nsetup_ontology_context('ontology/prov.ttl', ns, name='prov')\n\n# Series of queries\nqueries = [\n    \"What is prov:Activity?\",\n    \"What properties does prov:Activity have?\",\n    \"How are prov:Activity and prov:Entity related?\",\n]\n\nfor i, query in enumerate(queries, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"Query {i}: {query}\")\n    print('='*60)\n    \n    answer, iterations, ns, new_memories = rlm_run_with_memory(\n        query,\n        ns['prov_meta'].summary(),\n        store,\n        ns=ns\n    )\n    \n    print(f\"\\nAnswer: {answer}\")\n    print(f\"Iterations: {len(iterations)}\")\n    print(f\"New memories extracted: {len(new_memories)}\")\n    for mem in new_memories:\n        print(f\"  - {mem.title}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Final memory store: {len(store.memories)} memories\")\nprint('='*60)\n\n# Show all memories with access counts\nfor mem in store.memories:\n    print(f\"\\n{mem.title}\")\n    print(f\"  Source: {mem.source_type}\")\n    print(f\"  Accessed: {mem.access_count} times\")\n    print(f\"  Tags: {mem.tags}\")",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#bootstrap-general-strategies",
    "href": "procedural_memory.html#bootstrap-general-strategies",
    "title": "procedural_memory",
    "section": "Bootstrap General Strategies",
    "text": "Bootstrap General Strategies\nArchitectural Role (2026-01-19 Refactor):\nUniversal ontology exploration patterns that should be loaded into memory on startup. These strategies were previously in reasoning_bank.CORE_RECIPES but were moved here to align with the ReasoningBank paper’s architecture.\nKey Insight: General strategies are LEARNED (procedural memory), not AUTHORED (recipes).\n\nWhy Bootstrap?\n\nCorrect conceptual layer: Universal patterns belong in procedural_memory (Layer 1), not reasoning_bank (Layer 2)\nEnable learning: Stored as MemoryItems, these can be:\n\nRetrieved via BM25 (not always injected)\nUpdated with success_rate over time\nMerged/consolidated with newly extracted patterns\nRemoved if ineffective\n\nFuture extensibility: New strategies extracted from successful runs can be added to memory_store automatically\n\n\n\nUsage\n# One-time bootstrap at startup\nmemory_store = MemoryStore()\nfor strategy in bootstrap_general_strategies():\n    memory_store.add(strategy)\n\n# Use in RLM runs\nfrom rlm.reasoning_bank import rlm_run_enhanced\nanswer, iters, ns = rlm_run_enhanced(\n    query=\"What is Activity?\",\n    context=meta.summary(),\n    sense=sense,\n    memory_store=memory_store  # General strategies retrieved via BM25\n)\nNote: These are seed strategies - the system can learn and add more over time via the memory extraction loop.\n\n\n\nbootstrap_general_strategies\n\ndef bootstrap_general_strategies(\n    \n)-&gt;list:\n\nCreate general strategy memories for bootstrapping.\nThese are universal patterns extracted from successful RLM runs that apply to all ontologies.\nReturns: List of MemoryItem objects representing general strategies\n\n# Test bootstrap\nstrategies = bootstrap_general_strategies()\nprint(f\"Bootstrapped {len(strategies)} general strategies:\")\nfor s in strategies:\n    print(f\"  - {s.title}\")\n    print(f\"    Tags: {s.tags}\")\n    print(f\"    Task: {s.task_query}\")\n\n# Test that they can be stored\nimport tempfile\nwith tempfile.TemporaryDirectory() as tmpdir:\n    test_path = Path(tmpdir) / 'bootstrap_test.json'\n    store = MemoryStore(path=test_path)\n    \n    for strategy in strategies:\n        store.add(strategy)\n    \n    store.save()\n    \n    # Reload and verify\n    loaded = MemoryStore.load(test_path)\n    assert len(loaded.memories) == len(strategies)\n    print(f\"\\n✓ Bootstrap strategies can be saved and loaded\")\n    print(f\"✓ Total: {len(loaded.memories)} strategies\")\n\nBootstrapped 7 general strategies:\n  - Describe Entity by Label\n    Tags: ['entity', 'search', 'describe', 'universal']\n    Task: entity_description\n  - Find Subclasses Using GraphMeta\n    Tags: ['hierarchy', 'subclass', 'graphmeta', 'universal']\n    Task: hierarchy\n  - Find Superclasses Using GraphMeta\n    Tags: ['hierarchy', 'superclass', 'graphmeta', 'universal']\n    Task: hierarchy\n  - Find Properties by Domain/Range\n    Tags: ['properties', 'domain', 'range', 'universal']\n    Task: property_discovery\n  - Pattern-Based Entity Search\n    Tags: ['search', 'pattern', 'multiple', 'universal']\n    Task: pattern_search\n  - Find Relationship Path Between Entities\n    Tags: ['relationships', 'path', 'connection', 'universal']\n    Task: relationship_discovery\n  - Navigate Class Hierarchy from Roots\n    Tags: ['hierarchy', 'exploration', 'roots', 'universal']\n    Task: hierarchy\n\n✓ Bootstrap strategies can be saved and loaded\n✓ Total: 7 strategies",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "procedural_memory.html#validation-functions",
    "href": "procedural_memory.html#validation-functions",
    "title": "procedural_memory",
    "section": "Validation Functions",
    "text": "Validation Functions\nValidation gates to ensure quality and consistency of procedural memory.\n\n\nvalidate_no_hardcoded_uris\n\ndef validate_no_hardcoded_uris(\n    strategies:list\n)-&gt;bool:\n\nEnsure strategies don’t reference specific ontology URIs.\nUniversal strategies should use placeholders like {ontology}_meta instead of hardcoded ontology prefixes.\n\n\n\nvalidate_bootstrap_strategies\n\ndef validate_bootstrap_strategies(\n    \n)-&gt;dict:\n\nValidate bootstrap creates valid, non-conflicting strategies.\nChecks: - Correct count (7 strategies) - All are valid MemoryItem objects - Unique titles (no duplicates) - All tagged as ‘universal’ - No hardcoded ontology-specific URIs\nReturns: Dictionary with ‘valid’ flag and detailed checks\n\n\n\ncheck_memory_deduplication\n\ndef check_memory_deduplication(\n    new_memory:MemoryItem, store:MemoryStore, threshold:float=0.7\n)-&gt;str:\n\nGate 1: Check for duplicate memories.\nUses title similarity to detect duplicates and decide action: - add: No similar memories, safe to add - merge: Similar memory exists, should combine insights - skip: Similar memory exists and is better, don’t add - replace: New memory is better, replace existing\nArgs: new_memory: MemoryItem to check store: MemoryStore to check against threshold: Similarity threshold (0-1) for considering duplicate\nReturns: Action string: ‘add’, ‘merge’, ‘skip’, or ‘replace’\n\n\n\nscore_generalization\n\ndef score_generalization(\n    memory:MemoryItem\n)-&gt;float:\n\nGate 3: Score how generalizable a memory is (0-1).\nHigher score = more general/reusable across ontologies. Lower score = specific to one ontology or situation.\nScoring factors: - Penalize hardcoded URIs (prov:, sio:, http://) - Reward procedural language (use, check, try, if/then) - Reward ‘universal’ tag\nArgs: memory: MemoryItem to score\nReturns: Score between 0.0 and 1.0\n\n\n\nvalidate_retrieval_quality\n\ndef validate_retrieval_quality(\n    memory_store:MemoryStore, test_cases:list\n)-&gt;dict:\n\nValidate BM25 retrieves relevant memories for known queries.\nArgs: memory_store: MemoryStore with strategies test_cases: List of (query, expected_tags) tuples\nReturns: Dictionary with validation results including success_rate\n\n# Test validation functions\nprint(\"Test 1: Validate bootstrap strategies\")\nresult = validate_bootstrap_strategies()\nprint(f\"  Valid: {result['valid']}\")\nprint(f\"  Checks: {result['checks']}\")\n\nprint(\"\\nTest 2: Score generalization\")\ntest_mem = bootstrap_general_strategies()[0]\nscore = score_generalization(test_mem)\nprint(f\"  Strategy '{test_mem.title}' generalization score: {score:.2f}\")\n\nprint(\"\\nTest 3: Check memory deduplication\")\nstore = MemoryStore()\nstrategies = bootstrap_general_strategies()\nfor s in strategies:\n    store.add(s)\n\n# Try adding a duplicate\nduplicate = MemoryItem(\n    id=str(uuid.uuid4()),\n    title='Describe Entity by Label',  # Same as first strategy\n    description='Test duplicate',\n    content='Test content',\n    source_type='success',\n    task_query='test',\n    created_at=datetime.now(timezone.utc).isoformat(),\n    tags=['test']\n)\naction = check_memory_deduplication(duplicate, store, threshold=0.7)\nprint(f\"  Action for duplicate: {action}\")\n\nprint(\"\\nTest 4: Validate retrieval quality\")\ntest_cases = [\n    (\"What is Activity?\", ['entity', 'describe']),\n    (\"Find subclasses\", ['hierarchy', 'subclass']),\n    (\"What properties does it have?\", ['properties', 'domain'])\n]\nretrieval_result = validate_retrieval_quality(store, test_cases)\nprint(f\"  Valid: {retrieval_result['valid']}\")\nprint(f\"  Success rate: {retrieval_result['success_rate']:.1%}\")\n\nprint(\"\\n✓ All validation functions work\")",
    "crumbs": [
      "procedural_memory"
    ]
  },
  {
    "objectID": "logger.html",
    "href": "logger.html",
    "title": "logger",
    "section": "",
    "text": "This module provides two complementary logging mechanisms for RLM, following the pattern established in rlmpaper:\n\nRLMLogger: Structured JSON-lines logging for analysis and debugging\nVerbosePrinter: Beautiful console output using Rich for real-time visibility\n\n\n\n\nOptional: Both logger and verbose mode are opt-in\nNon-invasive: Minimal coupling to core RLM logic\nDual output: Machine-readable JSONL + human-readable Rich output\nAligned with rlmpaper: Same concepts, adapted for claudette backend\n\n\n\n\n\nrlmpaper logging - Reference implementation\nRich library - Beautiful terminal formatting",
    "crumbs": [
      "logger"
    ]
  },
  {
    "objectID": "logger.html#overview",
    "href": "logger.html#overview",
    "title": "logger",
    "section": "",
    "text": "This module provides two complementary logging mechanisms for RLM, following the pattern established in rlmpaper:\n\nRLMLogger: Structured JSON-lines logging for analysis and debugging\nVerbosePrinter: Beautiful console output using Rich for real-time visibility\n\n\n\n\nOptional: Both logger and verbose mode are opt-in\nNon-invasive: Minimal coupling to core RLM logic\nDual output: Machine-readable JSONL + human-readable Rich output\nAligned with rlmpaper: Same concepts, adapted for claudette backend\n\n\n\n\n\nrlmpaper logging - Reference implementation\nRich library - Beautiful terminal formatting",
    "crumbs": [
      "logger"
    ]
  },
  {
    "objectID": "logger.html#imports",
    "href": "logger.html#imports",
    "title": "logger",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "logger"
    ]
  },
  {
    "objectID": "logger.html#rlmlogger---structured-json-lines-logging",
    "href": "logger.html#rlmlogger---structured-json-lines-logging",
    "title": "logger",
    "section": "RLMLogger - Structured JSON-Lines Logging",
    "text": "RLMLogger - Structured JSON-Lines Logging\nWrites iteration data to JSON-lines files for post-hoc analysis.\nEach line is a JSON object representing either: - Metadata (first line): Configuration and setup - Iteration: Complete iteration with prompt, response, code blocks, results\n\n\nRLMLogger\n\ndef RLMLogger(\n    log_dir:str | pathlib.Path, file_name:str='rlm'\n):\n\nLogger that writes RLM iteration data to JSON-lines files.\nCreates timestamped JSONL files with complete iteration history. Following rlmpaper’s logging pattern.\nExample: logger = RLMLogger(log_dir=‘./logs’) logger.log_metadata({‘query’: ‘What is X?’, ‘max_iters’: 5}) logger.log(iteration)\nTest RLMLogger:\n\nimport tempfile\n\n# Create logger in temp directory\nwith tempfile.TemporaryDirectory() as tmpdir:\n    logger = RLMLogger(tmpdir, file_name='test')\n    \n    # Log metadata\n    logger.log_metadata({'query': 'Test query', 'max_iters': 5})\n    \n    # Create mock iteration\n    result = REPLResult(stdout='Hello', stderr='', locals={}, execution_time=0.1)\n    block = CodeBlock(code='print(\"Hello\")', result=result)\n    iteration = RLMIteration(prompt='Test prompt', response='Test response', code_blocks=[block])\n    \n    # Log iteration\n    logger.log(iteration, 1)\n    \n    # Verify file exists and has content\n    assert logger.log_file_path.exists()\n    \n    # Read and parse\n    lines = logger.log_file_path.read_text().strip().split('\\n')\n    assert len(lines) == 2  # metadata + 1 iteration\n    \n    metadata_entry = json.loads(lines[0])\n    assert metadata_entry['type'] == 'metadata'\n    assert metadata_entry['query'] == 'Test query'\n    \n    iteration_entry = json.loads(lines[1])\n    assert iteration_entry['type'] == 'iteration'\n    assert iteration_entry['iteration'] == 1\n    assert iteration_entry['response'] == 'Test response'\n    assert len(iteration_entry['code_blocks']) == 1\n    \n    print(f\"✓ RLMLogger works: {logger.log_file_path.name}\")\n    print(f\"  Logged {logger.iteration_count} iteration(s)\")\n\n✓ RLMLogger works: test_2026-01-29_18-45-55_e9bae748.jsonl\n  Logged 1 iteration(s)",
    "crumbs": [
      "logger"
    ]
  },
  {
    "objectID": "logger.html#verboseprinter---rich-console-output",
    "href": "logger.html#verboseprinter---rich-console-output",
    "title": "logger",
    "section": "VerbosePrinter - Rich Console Output",
    "text": "VerbosePrinter - Rich Console Output\nBeautiful, human-readable terminal output for debugging.\nSimplified from rlmpaper’s version - focuses on core visibility without heavy styling.\n\n\nVerbosePrinter\n\ndef VerbosePrinter(\n    enabled:bool=True\n):\n\nConsole printer for RLM verbose output using Rich.\nProvides real-time visibility into RLM execution with beautiful formatting. Falls back to simple print if Rich is not installed.\nExample: verbose = VerbosePrinter(enabled=True) verbose.print_header(query=‘What is X?’, max_iters=5) verbose.print_iteration(iteration, 1)\nTest VerbosePrinter:\n\n# Test with Rich if available\nverbose = VerbosePrinter(enabled=True)\n\nverbose.print_header(\n    query=\"What is the Activity class in PROV?\",\n    context=\"PROV ontology loaded\",\n    max_iters=3\n)\n\n# Mock iteration\nresult = REPLResult(\n    stdout=\"Activity: http://www.w3.org/ns/prov#Activity\",\n    stderr=\"\",\n    locals={},\n    execution_time=0.05\n)\nblock = CodeBlock(code=\"search_by_label(prov_meta, 'Activity')\", result=result)\niteration = RLMIteration(\n    prompt=\"Find Activity class\",\n    response=\"Let me search for the Activity class\",\n    code_blocks=[block]\n)\n\nverbose.print_iteration(iteration, 1)\nverbose.print_final_answer(\"The Activity class represents processes in PROV.\")\nverbose.print_summary(total_iterations=1, total_time=1.5)\n\nprint(\"\\n✓ VerbosePrinter works\")",
    "crumbs": [
      "logger"
    ]
  },
  {
    "objectID": "rlm_paper.html",
    "href": "rlm_paper.html",
    "title": "Recursive Language Models",
    "section": "",
    "text": "# url2note(\"https://arxiv.org/html/2512.24601v1\")\n\nSo I’m interested in looking at understanding this RML paper and potentially thinking through how this sort of approach might be implemented in Solveit using some of the Solveit tools that are already available. So could you have a think through this RML paper and in particular the ability for a model to manage its own content text? For reference, the source code for this publication is in the rmlpaper directory that you may look at.",
    "crumbs": [
      "Recursive Language Models"
    ]
  },
  {
    "objectID": "reasoning_bank.html",
    "href": "reasoning_bank.html",
    "title": "reasoning_bank",
    "section": "",
    "text": "The ReasoningBank provides ontology-specific procedural recipes that guide the LLM in domain-specific ontology exploration tasks.\n\n\nIMPORTANT ARCHITECTURAL DECISION (2026-01-19 Refactor):\nThis module was refactored to align with the ReasoningBank paper’s memory-based learning model:\n\nGeneral strategies (universal patterns): → procedural_memory.bootstrap_general_strategies() (LEARNED)\nOntology-specific patterns (PROV, SIO, etc.): → reasoning_bank.ONTOLOGY_RECIPES (AUTHORED)\n\nRationale: - General strategies are LEARNED from experience and stored as MemoryItems (BM25-retrieved) - Ontology-specific patterns are AUTHORED by domain experts and stored as Recipes (always injected) - This separation enables: learning new strategies over time while maintaining domain-specific guidance\n\n\n\n\nLayer 0: Sense Card - Compact ontology metadata (always injected)\nLayer 1: Retrieved Memories - General strategies from procedural_memory (BM25-retrieved)\nLayer 2: Ontology Recipes - Domain-specific patterns (this module)\nLayer 3: Base Context - GraphMeta summary\n\n\n\n\n\nExplicit procedures: Step-by-step tool usage patterns\nGrounded in tools: Only reference available functions\nExpected iterations: Set clear convergence expectations\nDomain-specific: Recipes capture ontology-specific conventions (e.g., PROV Activity-Entity patterns)",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#overview",
    "href": "reasoning_bank.html#overview",
    "title": "reasoning_bank",
    "section": "",
    "text": "The ReasoningBank provides ontology-specific procedural recipes that guide the LLM in domain-specific ontology exploration tasks.\n\n\nIMPORTANT ARCHITECTURAL DECISION (2026-01-19 Refactor):\nThis module was refactored to align with the ReasoningBank paper’s memory-based learning model:\n\nGeneral strategies (universal patterns): → procedural_memory.bootstrap_general_strategies() (LEARNED)\nOntology-specific patterns (PROV, SIO, etc.): → reasoning_bank.ONTOLOGY_RECIPES (AUTHORED)\n\nRationale: - General strategies are LEARNED from experience and stored as MemoryItems (BM25-retrieved) - Ontology-specific patterns are AUTHORED by domain experts and stored as Recipes (always injected) - This separation enables: learning new strategies over time while maintaining domain-specific guidance\n\n\n\n\nLayer 0: Sense Card - Compact ontology metadata (always injected)\nLayer 1: Retrieved Memories - General strategies from procedural_memory (BM25-retrieved)\nLayer 2: Ontology Recipes - Domain-specific patterns (this module)\nLayer 3: Base Context - GraphMeta summary\n\n\n\n\n\nExplicit procedures: Step-by-step tool usage patterns\nGrounded in tools: Only reference available functions\nExpected iterations: Set clear convergence expectations\nDomain-specific: Recipes capture ontology-specific conventions (e.g., PROV Activity-Entity patterns)",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#imports",
    "href": "reasoning_bank.html#imports",
    "title": "reasoning_bank",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#recipe-schema",
    "href": "reasoning_bank.html#recipe-schema",
    "title": "reasoning_bank",
    "section": "Recipe Schema",
    "text": "Recipe Schema\n\n\nRecipe\n\ndef Recipe(\n    id:str, title:str, when_to_use:str, procedure:str, expected_iterations:int, layer:int, task_types:list,\n    ontology:Optional=None\n)-&gt;None:\n\nA procedural recipe for ontology exploration.\nRecipes provide explicit step-by-step guidance on HOW to use ontology exploration tools to accomplish specific tasks.",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#ontology-specific-recipes-layer-2",
    "href": "reasoning_bank.html#ontology-specific-recipes-layer-2",
    "title": "reasoning_bank",
    "section": "Ontology-Specific Recipes (Layer 2)",
    "text": "Ontology-Specific Recipes (Layer 2)\nAfter 2026-01-19 Refactor: This section now contains ONLY ontology-specific recipes.\nWhat moved: Universal patterns (describe entity, find subclasses, etc.) moved to procedural_memory.bootstrap_general_strategies().\nWhat stays: Ontology-specific patterns like: - PROV: Activity-Entity relationship patterns - SIO: Measurement and process patterns\n- Domain-specific conventions and idioms\nCurrent status: Placeholder (ONTOLOGY_RECIPES = []) - to be populated with domain-specific recipes as needed.\n\n# Test Recipe creation\ntest_recipe = CORE_RECIPES[1]  # recipe-1-describe-entity\nprint(f\"Recipe: {test_recipe.title}\")\nprint(f\"Layer: {test_recipe.layer}\")\nprint(f\"Expected iterations: {test_recipe.expected_iterations}\")\nprint(f\"Task types: {test_recipe.task_types}\")\nprint(f\"\\nFormatted:\")\nprint(test_recipe.format_for_injection())",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#recipe-retrieval-functions",
    "href": "reasoning_bank.html#recipe-retrieval-functions",
    "title": "reasoning_bank",
    "section": "Recipe Retrieval Functions",
    "text": "Recipe Retrieval Functions\n\n\nformat_recipes_for_injection\n\ndef format_recipes_for_injection(\n    recipes:list\n)-&gt;str:\n\nFormat recipes as markdown for context injection.\nArgs: recipes: List of Recipe objects\nReturns: Formatted markdown string\n\n\n\nretrieve_task_recipes\n\ndef retrieve_task_recipes(\n    task_type:str, k:int=2\n)-&gt;list:\n\nRetrieve task-type specific recipes.\nArgs: task_type: Task type from classify_task_type() k: Number of recipes to retrieve\nReturns: List of relevant Recipe objects\n\n\n\nget_core_recipes\n\ndef get_core_recipes(\n    limit:int=2\n)-&gt;list:\n\nGet always-injected core recipes.\nArgs: limit: Maximum number of core recipes to return\nReturns: List of core Recipe objects\n\n\n\nclassify_task_type\n\ndef classify_task_type(\n    query:str\n)-&gt;str:\n\nClassify query into task type for recipe selection.\nArgs: query: User query string\nReturns: Task type: ‘entity_discovery’, ‘entity_description’, ‘hierarchy’, ‘property_discovery’, ‘pattern_search’, ‘relationship_discovery’\n\n# Test recipe retrieval\nprint(\"Test 1: Classify task types\")\nqueries = [\n    \"What is Activity?\",\n    \"Find all subclasses of Activity\",\n    \"What properties have Entity as domain?\",\n    \"How are Activity and Entity related?\"\n]\n\nfor q in queries:\n    task_type = classify_task_type(q)\n    print(f\"  '{q}' → {task_type}\")\n\nprint(\"\\nTest 2: Get core recipes\")\ncore = get_core_recipes(limit=2)\nprint(f\"  Retrieved {len(core)} core recipes:\")\nfor r in core:\n    print(f\"    - {r.title}\")\n\nprint(\"\\nTest 3: Retrieve task-specific recipes\")\ntask_recipes = retrieve_task_recipes('hierarchy', k=2)\nprint(f\"  Retrieved {len(task_recipes)} recipes for 'hierarchy':\")\nfor r in task_recipes:\n    print(f\"    - {r.title}\")\n\nprint(\"\\nTest 4: Format recipes\")\nformatted = format_recipes_for_injection(core[:1])\nprint(f\"  Formatted length: {len(formatted)} chars\")\nprint(f\"\\nFormatted output (first 300 chars):\")\nprint(formatted[:300] + \"...\")",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#context-injection",
    "href": "reasoning_bank.html#context-injection",
    "title": "reasoning_bank",
    "section": "Context Injection",
    "text": "Context Injection\nFour-layer context injection strategy: 1. Layer 0: Sense card (from structured sense) 2. Layer 1: Core recipes (always injected) 3. Layer 2: Task-type recipes (query-dependent) 4. Layer 3: Ontology-specific knowledge (future)\n\n\ninject_context\n\ndef inject_context(\n    query:str, base_context:str, sense:dict=None, memory_store:NoneType=None, ontology:str=None, max_memories:int=3,\n    max_task_recipes:int=2\n)-&gt;str:\n\nBuild complete context with sense + memories + recipes.\nInjection order: 1. Sense card (Layer 0) - if provided 2. Retrieved memories (Layer 1) - general strategies from memory_store 3. Task-type recipes (Layer 2) - ontology-specific patterns 4. Base context - original graph summary\nArgs: query: User query base_context: Base context string (e.g., GraphMeta.summary()) sense: Structured sense document (from build_sense_structured) memory_store: MemoryStore with general strategies ontology: Optional ontology name for ontology-specific recipes max_memories: Maximum memories to retrieve max_task_recipes: Maximum task-type recipes to inject\nReturns: Enhanced context string",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#enhanced-rlm-runner",
    "href": "reasoning_bank.html#enhanced-rlm-runner",
    "title": "reasoning_bank",
    "section": "Enhanced RLM Runner",
    "text": "Enhanced RLM Runner\n\n\nrlm_run_enhanced\n\ndef rlm_run_enhanced(\n    query:str, context:str, ns:dict=None, sense:dict=None, memory_store:NoneType=None, ontology:str=None,\n    kwargs:VAR_KEYWORD\n)-&gt;tuple:\n\nRLM run with sense and memory injection.\nThis is a drop-in replacement for rlm_run() that automatically enhances context with: - Layer 0: Structured sense card - Layer 1: Retrieved procedural memories (general strategies) - Layer 2: Ontology-specific recipes\nArgs: query: User query context: Base context (e.g., GraphMeta.summary()) ns: Namespace dict sense: Structured sense document (from build_sense_structured) memory_store: MemoryStore with general strategies ontology: Optional ontology name **kwargs: Additional arguments passed to rlm_run()\nReturns: (answer, iterations, final_ns) - same as rlm_run()",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#tests",
    "href": "reasoning_bank.html#tests",
    "title": "reasoning_bank",
    "section": "Tests",
    "text": "Tests\n\n# Test inject_context with memory_store\nfrom rlm.procedural_memory import MemoryStore, bootstrap_general_strategies\n\nprint(\"Test: inject_context() with memory_store\")\nprint(\"=\" * 70)\n\n# Create memory store with general strategies\nmemory_store = MemoryStore()\nstrategies = bootstrap_general_strategies()\nfor s in strategies:\n    memory_store.add(s)\n\nprint(f\"Memory store: {len(memory_store.memories)} strategies\")\n\n# Simulate a sense document\ntest_sense = {\n    'sense_card': {\n        'ontology_id': 'test',\n        'domain_scope': 'Test ontology',\n        'triple_count': 100,\n        'class_count': 10,\n        'property_count': 5,\n        'key_classes': [],\n        'key_properties': [],\n        'label_predicates': ['rdfs:label'],\n        'description_predicates': ['rdfs:comment'],\n        'available_indexes': {},\n        'quick_hints': ['Test hint'],\n        'uri_pattern': 'http://test.org/'\n    },\n    'sense_brief': {}\n}\n\nquery = \"What is Activity?\"\nbase_context = \"Test base context\"\n\nenhanced = inject_context(\n    query=query,\n    base_context=base_context,\n    sense=test_sense,\n    memory_store=memory_store,\n    max_memories=2\n)\n\nprint(f\"\\nQuery: '{query}'\")\nprint(f\"\\nEnhanced context length: {len(enhanced)} chars\")\nprint(f\"\\nContext structure:\")\nif '# Ontology:' in enhanced:\n    print(\"  ✓ Layer 0: Sense card\")\nif 'Relevant Prior Experience' in enhanced or 'Describe Entity' in enhanced:\n    print(\"  ✓ Layer 1: Retrieved memories (general strategies)\")\nif 'Base Context' in enhanced:\n    print(\"  ✓ Base context included\")\n\nprint(f\"\\nFirst 400 chars of enhanced context:\")\nprint(\"-\" * 70)\nprint(enhanced[:400] + \"...\")\n\nTest: inject_context() with memory_store\n======================================================================\nMemory store: 7 strategies\n\nQuery: 'What is Activity?'\n\nEnhanced context length: 1003 chars\n\nContext structure:\n  ✓ Layer 0: Sense card\n  ✓ Layer 1: Retrieved memories (general strategies)\n  ✓ Base context included\n\nFirst 400 chars of enhanced context:\n----------------------------------------------------------------------\n# Ontology: test\n\n**Domain**: Test ontology\n\n**Stats**: 100 triples, 10 classes, 5 properties\n\n**Key Classes**:\n\n**Key Properties**:\n\n**Labels via**: rdfs:label\n\n**Quick Hints**:\n- Test hint\n\n## Relevant Prior Experience\n\nBefore taking action, briefly assess which of these strategies apply to your current task and which do not.\n\n### 1. Describe Entity by Label\nUniversal pattern for finding and des...",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "reasoning_bank.html#validation",
    "href": "reasoning_bank.html#validation",
    "title": "reasoning_bank",
    "section": "Validation",
    "text": "Validation\nValidate memory-recipe separation and ensure correct architecture.\n\n\nvalidate_memory_recipe_separation\n\ndef validate_memory_recipe_separation(\n    memory_store\n)-&gt;dict:\n\nEnsure general strategies aren’t duplicated in ONTOLOGY_RECIPES.\nValidates: - No title overlap between universal memories and ontology recipes - All ONTOLOGY_RECIPES have ontology field set (domain-specific)\nArgs: memory_store: MemoryStore (typically with bootstrap strategies)\nReturns: Dictionary with validation results\n\n# Test memory-recipe separation validation\nfrom rlm.procedural_memory import MemoryStore, bootstrap_general_strategies\n\nprint(\"Test: validate_memory_recipe_separation()\")\nprint(\"=\" * 60)\n\nmemory_store = MemoryStore()\nfor s in bootstrap_general_strategies():\n    memory_store.add(s)\n\nresult = validate_memory_recipe_separation(memory_store)\nprint(f\"Valid: {result['valid']}\")\nprint(f\"Overlap count: {result['overlap_count']}\")\nprint(f\"Ontology recipes count: {result['ontology_recipes_count']}\")\nprint(f\"All recipes have ontology field: {result['all_recipes_have_ontology']}\")\n\nif result['valid']:\n    print(\"\\n✓ Memory-recipe separation validated successfully\")\nelse:\n    print(f\"\\n✗ Validation failed: {result['overlapping_titles']}\")\n\nTest: validate_memory_recipe_separation()\n============================================================\nValid: True\nOverlap count: 0\nOntology recipes count: 0\nAll recipes have ontology field: True\n\n✓ Memory-recipe separation validated successfully",
    "crumbs": [
      "reasoning_bank"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "This module implements the RLM (Recursive Language Model) protocol from rlmpaper using claudette as the LLM backend.\n\n\n\nNamespace-explicit: All functions take ns: dict parameter (no frame walking)\nProtocol-faithful: Uses prompts and types from _rlmpaper_compat.py\nSolveit-independent: Core works anywhere; Solveit integration is separate\nReturn everything useful: Functions return (answer, iterations, ns) for inspection",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#overview",
    "href": "core.html#overview",
    "title": "core",
    "section": "",
    "text": "This module implements the RLM (Recursive Language Model) protocol from rlmpaper using claudette as the LLM backend.\n\n\n\nNamespace-explicit: All functions take ns: dict parameter (no frame walking)\nProtocol-faithful: Uses prompts and types from _rlmpaper_compat.py\nSolveit-independent: Core works anywhere; Solveit integration is separate\nReturn everything useful: Functions return (answer, iterations, ns) for inspection",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#imports",
    "href": "core.html#imports",
    "title": "core",
    "section": "Imports",
    "text": "Imports",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#environment-detection",
    "href": "core.html#environment-detection",
    "title": "core",
    "section": "Environment Detection",
    "text": "Environment Detection\n\n\nin_solveit\n\ndef in_solveit(\n    \n)-&gt;bool:\n\nCheck if running in Solveit environment.\nSolveit injects __msg_id into the call stack. This function tests for that.\n\n# Test environment detection\nis_solveit = in_solveit()\nprint(f\"Running in Solveit: {is_solveit}\")\nassert isinstance(is_solveit, bool)\n\nRunning in Solveit: False",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#core-llm-functions",
    "href": "core.html#core-llm-functions",
    "title": "core",
    "section": "Core LLM Functions",
    "text": "Core LLM Functions\nThese functions provide the REPL environment’s llm_query and llm_query_batched capabilities. They require an explicit namespace dict and store results there.\n\n\nllm_query\n\ndef llm_query(\n    prompt:str, ns:dict, name:str='llm_res', model:str='claude-sonnet-4-5'\n)-&gt;str:\n\nQuery a sub-LLM and store the result in namespace.\nArgs: prompt: The prompt to send to the LLM ns: Namespace dict where result will be stored name: Variable name for storing the result model: Claude model to use\nReturns: The LLM’s response (also stored in ns[name])\n\n# Test llm_query with explicit namespace\ntest_ns = {}\n# Note: Commented out to avoid API calls during CI\n# result = llm_query(\"Say 'hello' and nothing else\", test_ns, name='greeting')\n# assert 'greeting' in test_ns\n# assert len(test_ns['greeting']) &gt; 0\nprint(\"✓ llm_query signature test passed\")\n\n✓ llm_query signature test passed\n\n\n\n\n\nllm_query_batched\n\ndef llm_query_batched(\n    prompts:list, ns:dict, name:str='batch_res', model:str='claude-sonnet-4-5'\n)-&gt;list:\n\nQuery LLM with multiple prompts concurrently.\nNOTE: Currently executes sequentially due to claudette’s synchronous API. True concurrency pending claudette async API support. The async scaffolding is in place for when that becomes available.\nArgs: prompts: List of prompt strings ns: Namespace dict where results will be stored name: Variable name for storing the list of results model: Claude model to use\nReturns: List of LLM responses (also stored in ns[name])\n\n# Test llm_query_batched signature\ntest_ns = {}\n# Note: Commented out to avoid API calls during CI\n# prompts = [\"Say 'one'\", \"Say 'two'\"]\n# result = llm_query_batched(prompts, test_ns, name='batch')\n# assert 'batch' in test_ns\n# assert len(test_ns['batch']) == 2\nprint(\"✓ llm_query_batched signature test passed\")\n\n✓ llm_query_batched signature test passed",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#repl-execution",
    "href": "core.html#repl-execution",
    "title": "core",
    "section": "REPL Execution",
    "text": "REPL Execution\nExecute Python code in a namespace and capture stdout/stderr.\n\n\nexec_code\n\ndef exec_code(\n    code:str, ns:dict\n)-&gt;REPLResult:\n\nExecute code in namespace and return result.\nCaptures stdout, stderr, and any exceptions. The namespace is mutated with any variables created during execution.\nArgs: code: Python code to execute ns: Namespace dict for execution\nReturns: REPLResult with stdout, stderr, locals snapshot, execution_time\n\n# Test exec_code with explicit namespace\ntest_ns = {}\nresult = exec_code(\"x = 2 + 2\\nprint(x)\", test_ns)\nassert test_ns['x'] == 4\nassert '4' in result.stdout\nassert result.execution_time &gt; 0\nprint(\"✓ exec_code works\")\n\n# Test error handling\ntest_ns = {}\nresult = exec_code(\"raise ValueError('test error')\", test_ns)\nassert 'ValueError: test error' in result.stderr\nprint(\"✓ exec_code error handling works\")\n\n✓ exec_code works\n✓ exec_code error handling works",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#rlm-loop",
    "href": "core.html#rlm-loop",
    "title": "core",
    "section": "RLM Loop",
    "text": "RLM Loop\nThe main RLM iteration loop. Follows the rlmpaper protocol: 1. Build system prompt with metadata 2. Add first-iteration safeguard 3. Execute REPL code blocks 4. Check for FINAL/FINAL_VAR 5. Repeat until answer or max iterations\n\n\nrlm_run\n\ndef rlm_run(\n    query:str, context, ns:dict=None, model:str='claude-sonnet-4-5', max_iters:int=10, logger:NoneType=None,\n    verbose:bool=False\n)-&gt;tuple:\n\nRun RLM loop until FINAL or max iterations.\nThis implements the RLM protocol: the root LLM emits repl code blocks which are executed in a namespace with context, llm_query, llm_query_batched, and FINAL_VAR available. The loop continues until the model returns FINAL(…) or FINAL_VAR(…).\nArgs: query: User’s question to answer context: Context data (str, list of str, or dict) ns: Namespace dict (if None, creates fresh namespace) model: Claude model to use max_iters: Maximum iterations before giving up logger: RLMLogger instance for JSON-lines logging (optional) verbose: Enable Rich console output (default: False)\nReturns: (answer, iterations, namespace) tuple where: - answer: Final answer string (or fallback if max_iters reached) - iterations: List of RLMIteration objects - namespace: The dict containing all REPL variables",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#tests",
    "href": "core.html#tests",
    "title": "core",
    "section": "Tests",
    "text": "Tests\n\n# Test QueryMetadata (from _rlmpaper_compat)\nmeta = QueryMetadata([\"chunk1\", \"chunk2\", \"chunk3\"])\nassert meta.context_type == \"list\"\nassert len(meta.context_lengths) == 3\nassert meta.context_total_length == sum(len(c) for c in [\"chunk1\", \"chunk2\", \"chunk3\"])\nprint(\"✓ QueryMetadata works\")\n\n✓ QueryMetadata works\n\n\n\n# Test find_code_blocks (from _rlmpaper_compat)\ntext = \"\"\"Here's some code:\n```repl\nx = 1 + 1\nprint(x)\n```\nAnd more text.\"\"\"\nblocks = find_code_blocks(text)\nassert len(blocks) == 1\nassert 'x = 1 + 1' in blocks[0]\nprint(\"✓ find_code_blocks works\")\n\n✓ find_code_blocks works\n\n\n\n# Test find_final_answer (from _rlmpaper_compat)\nassert find_final_answer(\"FINAL(42)\") == \"42\"\nassert find_final_answer(\"FINAL(The answer is 42)\") == \"The answer is 42\"\n\n# Test FINAL_VAR\ntest_ns = {'result': 'hello world'}\nassert find_final_answer(\"FINAL_VAR(result)\", ns=test_ns) == \"hello world\"\n\n# Test no final\nassert find_final_answer(\"Just some text\") is None\nprint(\"✓ find_final_answer works\")\n\n✓ find_final_answer works\n\n\n\n# Test rlm_run with simple mock scenario\n# Note: This doesn't call LLM APIs, just tests the structure\ncontext = [\"The capital of France is Paris.\"]\ntest_ns = {}\n\n# We can't easily test without API calls, but we can verify the function signature\n# and that it sets up the namespace correctly\nmeta = QueryMetadata(context)\ntest_ns['context'] = context\ntest_ns['llm_query'] = partial(llm_query, ns=test_ns, model='claude-sonnet-4-5')\ntest_ns['llm_query_batched'] = partial(llm_query_batched, ns=test_ns, model='claude-sonnet-4-5')\n\n# Test that FINAL_VAR is now set up as a callable function\ndef _test_final_var(variable_name: str) -&gt; str:\n    variable_name = variable_name.strip().strip('\"').strip(\"'\")\n    if variable_name in test_ns:\n        return str(test_ns[variable_name])\n    return f\"Error: Variable '{variable_name}' not found in namespace\"\n\ntest_ns['FINAL_VAR'] = _test_final_var\n\nassert 'context' in test_ns\nassert 'llm_query' in test_ns\nassert 'llm_query_batched' in test_ns\nassert 'FINAL_VAR' in test_ns\nassert callable(test_ns['llm_query'])\nassert callable(test_ns['llm_query_batched'])\nassert callable(test_ns['FINAL_VAR'])\n\n# Test FINAL_VAR function behavior\ntest_ns['my_var'] = 'test value'\nassert test_ns['FINAL_VAR']('my_var') == 'test value'\nassert 'Error' in test_ns['FINAL_VAR']('nonexistent')\nprint(\"✓ rlm_run namespace setup works\")\nprint(\"✓ FINAL_VAR executable function works\")\n\n✓ rlm_run namespace setup works\n✓ FINAL_VAR executable function works",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#usage-examples",
    "href": "core.html#usage-examples",
    "title": "core",
    "section": "Usage Examples",
    "text": "Usage Examples\n\n# Simple usage (requires API key)\ncontext = [\"The speed of light is 299,792,458 m/s.\"]\nanswer, iterations, ns = rlm_run(\"What is the speed of light?\", context)\nprint(f\"Answer: {answer}\")\nprint(f\"Iterations: {len(iterations)}\")\nprint(f\"Variables in namespace: {[k for k in ns.keys() if not k.startswith('_')]}\")\n\n\n# Persistent namespace across runs\nns = {}\n\n# Define example contexts\ncontext1 = {'prompt': 'Query ontology for X', 'tools': [...]}\ncontext2 = {'prompt': 'Query ontology for Y', 'tools': [...]}\n\n# Run multiple queries, reusing namespace\nanswer1, iters1, ns = rlm_run(\"What is X?\", context1, ns=ns)\nanswer2, iters2, ns = rlm_run(\"What about Y?\", context2, ns=ns)\n# ns now contains variables from both runs",
    "crumbs": [
      "core"
    ]
  }
]