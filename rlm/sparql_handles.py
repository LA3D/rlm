"""SPARQL query execution with first-class result handles"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_sparql_handles.ipynb.

# %% auto 0
__all__ = ['SPARQLResultHandle', 'sparql_query', 'sparql_local', 'res_sample', 'setup_sparql_context']

# %% ../nbs/03_sparql_handles.ipynb 4
from sparqlx import SPARQLWrapper
from rdflib import Graph, URIRef, Literal
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from functools import partial
import random
from typing import Optional

# %% ../nbs/03_sparql_handles.ipynb 6
@dataclass
class SPARQLResultHandle:
    """Wrapper for SPARQL results with metadata and bounded view operations."""

    # Result data (never dumped wholesale to LLM)
    rows: list | Graph          # SELECT rows or CONSTRUCT/DESCRIBE graph
    result_type: str            # 'select' | 'ask' | 'construct' | 'describe'

    # Metadata
    query: str                  # Original SPARQL query
    endpoint: str               # Where executed (URL or 'local')
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')

    # For SELECT results
    columns: list = None        # Column names
    total_rows: int = 0         # Total before limit

    # For Graph results
    triple_count: int = 0       # Number of triples

    def summary(self) -> str:
        """Bounded summary for LLM."""
        if self.result_type == 'select':
            return f"SELECT: {len(self.rows)} rows, columns={self.columns}"
        elif self.result_type == 'ask':
            return f"ASK: {self.rows}"
        else:
            return f"{self.result_type.upper()}: {self.triple_count} triples"

    def __len__(self):
        if isinstance(self.rows, bool):
            return 1  # ASK result
        return len(self.rows) if hasattr(self.rows, '__len__') else 0

    def __iter__(self):
        if isinstance(self.rows, bool):
            return iter([self.rows])
        return iter(self.rows)
    
    def __repr__(self):
        return f"SPARQLResultHandle({self.summary()})"

# %% ../nbs/03_sparql_handles.ipynb 10
def sparql_query(
    query: str,
    endpoint: str = "https://query.wikidata.org/sparql",
    max_results: int = 100,
    name: str = 'res',
    ns: dict = None,
    timeout: float = 30.0,
    # Dataset integration
    ds_meta = None,
    store_in_work: bool = False,
    work_task_id: str = None
) -> str:
    """Execute SPARQL query, store SPARQLResultHandle in namespace.

    For SELECT: Stores SPARQLResultHandle with rows as list of dicts
    For CONSTRUCT/DESCRIBE: Stores SPARQLResultHandle with rdflib.Graph
    For ASK: Stores SPARQLResultHandle with boolean result

    If ds_meta provided and store_in_work=True:
    - CONSTRUCT results stored in work/<task_id> graph
    - Query logged to prov graph
    
    Args:
        query: SPARQL query string
        endpoint: SPARQL endpoint URL
        max_results: Maximum results to return (for SELECT/CONSTRUCT)
        name: Variable name to store result handle
        ns: Namespace dict (defaults to globals())
        timeout: Query timeout in seconds
        ds_meta: Optional DatasetMeta for dataset integration
        store_in_work: If True and ds_meta provided, store CONSTRUCT results in work graph
        work_task_id: Task ID for work graph (auto-generated if None)
        
    Returns:
        Summary string describing the result
    """
    if ns is None:
        ns = globals()
    
    # Configure wrapper with timeout and headers
    headers = {"User-Agent": "RLM/1.0 (https://github.com/LA3D/rlm)"}
    wrapper = SPARQLWrapper(
        sparql_endpoint=endpoint,
        client_config=dict(timeout=timeout, headers=headers)
    )
    
    # Execute query with rdflib conversion
    result = wrapper.query(query, convert=True)
    
    # Determine result type and create handle
    if isinstance(result, bool):
        # ASK query
        handle = SPARQLResultHandle(
            rows=result,
            result_type='ask',
            query=query,
            endpoint=endpoint
        )
        ns[name] = handle
        return f"ASK result: {result}, stored in '{name}'"
    
    elif hasattr(result, 'serialize'):
        # CONSTRUCT or DESCRIBE query - result is rdflib.Graph
        triples = list(result)[:max_results]
        g = Graph()
        for t in triples:
            g.add(t)
        
        # Determine if CONSTRUCT or DESCRIBE
        query_upper = query.upper()
        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'
        
        handle = SPARQLResultHandle(
            rows=g,
            result_type=result_type,
            query=query,
            endpoint=endpoint,
            triple_count=len(g)
        )
        ns[name] = handle
        
        # Dataset integration for CONSTRUCT results
        if ds_meta is not None and store_in_work:
            from rlm.dataset import work_create
            import uuid
            
            task_id = work_task_id if work_task_id else f"sparql_{uuid.uuid4().hex[:8]}"
            graph_uri, work_graph = work_create(ds_meta, task_id)
            
            # Copy triples to work graph
            for s, p, o in g:
                work_graph.add((s, p, o))
            
            # Log query to prov
            from rdflib import Namespace, RDF, XSD
            RLM_PROV = Namespace('urn:rlm:prov:')
            event_uri = URIRef(f'urn:rlm:prov:sparql_{uuid.uuid4().hex[:8]}')
            ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.SPARQLQuery))
            ds_meta.prov.add((event_uri, RLM_PROV.query, Literal(query)))
            ds_meta.prov.add((event_uri, RLM_PROV.endpoint, Literal(endpoint)))
            ds_meta.prov.add((event_uri, RLM_PROV.resultGraph, URIRef(graph_uri)))
            ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(handle.timestamp, datatype=XSD.dateTime)))
            ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))
            
            return f"Graph with {len(g)} triples stored in '{name}' and work/{task_id}" + \
                   (f" (truncated from {len(result)})" if len(result) > max_results else "")
        
        return f"Graph with {len(g)} triples stored in '{name}'" + \
               (f" (truncated from {len(result)})" if len(result) > max_results else "")
    
    else:
        # SELECT query - result is list of dicts
        result = result[:max_results]
        cols = list(result[0].keys()) if result else []
        
        handle = SPARQLResultHandle(
            rows=result,
            result_type='select',
            query=query,
            endpoint=endpoint,
            columns=cols,
            total_rows=len(result)
        )
        ns[name] = handle
        
        return f"SELECT result with {len(result)} rows, columns: {cols}, stored in '{name}'"

# %% ../nbs/03_sparql_handles.ipynb 14
def sparql_local(
    query: str,
    graph: Graph | str,
    max_results: int = 100,
    name: str = 'res',
    ns: dict = None
) -> str:
    """Execute SPARQL query on local rdflib Graph.

    Useful for querying mounted ontologies or work graphs.
    Returns SPARQLResultHandle same as sparql_query().
    
    Args:
        query: SPARQL query string
        graph: rdflib.Graph object or name of graph in namespace
        max_results: Maximum results to return
        name: Variable name to store result handle
        ns: Namespace dict (defaults to globals())
        
    Returns:
        Summary string describing the result
    """
    if ns is None:
        ns = globals()
    
    # Resolve graph if string name provided
    if isinstance(graph, str):
        if graph not in ns:
            return f"Error: Graph '{graph}' not found in namespace"
        graph_obj = ns[graph]
        # Handle GraphMeta wrapper
        if hasattr(graph_obj, 'graph'):
            graph_obj = graph_obj.graph
    else:
        graph_obj = graph
    
    if not isinstance(graph_obj, Graph):
        return f"Error: Expected rdflib.Graph, got {type(graph_obj)}"
    
    # Execute query on local graph
    result = graph_obj.query(query)
    
    # Determine result type
    query_upper = query.upper()
    
    if 'ASK' in query_upper:
        # ASK query
        ask_result = bool(result)
        handle = SPARQLResultHandle(
            rows=ask_result,
            result_type='ask',
            query=query,
            endpoint='local'
        )
        ns[name] = handle
        return f"ASK result: {ask_result}, stored in '{name}'"
    
    elif 'CONSTRUCT' in query_upper or 'DESCRIBE' in query_upper:
        # CONSTRUCT or DESCRIBE query
        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'
        g = Graph()
        for triple in list(result)[:max_results]:
            g.add(triple)
        
        handle = SPARQLResultHandle(
            rows=g,
            result_type=result_type,
            query=query,
            endpoint='local',
            triple_count=len(g)
        )
        ns[name] = handle
        return f"Graph with {len(g)} triples stored in '{name}'"
    
    else:
        # SELECT query
        rows = []
        for row in list(result)[:max_results]:
            row_dict = {}
            for var in result.vars:
                row_dict[str(var)] = row[var] if row[var] else None
            rows.append(row_dict)
        
        cols = [str(v) for v in result.vars] if result.vars else []
        
        handle = SPARQLResultHandle(
            rows=rows,
            result_type='select',
            query=query,
            endpoint='local',
            columns=cols,
            total_rows=len(rows)
        )
        ns[name] = handle
        
        return f"SELECT result with {len(rows)} rows, columns: {cols}, stored in '{name}'"

# %% ../nbs/03_sparql_handles.ipynb 18
def res_sample(result, n: int = 10, seed: int = None) -> list:
    """Get random sample of N rows from result.

    Args:
        result: SPARQLResultHandle, ResultTable, or list
        n: Number of rows to sample
        seed: Optional random seed for reproducibility

    Returns:
        List of sampled rows
    """
    if seed is not None:
        random.seed(seed)

    # Extract rows from different result types
    if isinstance(result, SPARQLResultHandle):
        if result.result_type in ['construct', 'describe']:
            # For graphs, sample triples
            rows = list(result.rows)
        elif result.result_type == 'ask':
            # ASK has no rows to sample
            return [result.rows]
        else:
            rows = result.rows
    elif hasattr(result, 'rows'):
        # ResultTable or similar
        rows = result.rows
    else:
        # Plain list
        rows = result

    if len(rows) <= n:
        return list(rows)
    return random.sample(list(rows), n)

# %% ../nbs/03_sparql_handles.ipynb 22
def setup_sparql_context(
    ns: dict,
    default_endpoint: str = "https://query.wikidata.org/sparql",
    ds_meta = None
) -> str:
    """Initialize SPARQL tools in namespace.

    Binds:
    - sparql_query() with default endpoint
    - sparql_local() if ds_meta provided
    - res_head(), res_where(), res_group(), res_distinct(), res_sample()

    Args:
        ns: Namespace dict where functions will be bound
        default_endpoint: Default SPARQL endpoint URL
        ds_meta: Optional DatasetMeta for dataset integration
        
    Returns:
        Status message
    """
    # Import view functions from dataset module
    try:
        from rlm.dataset import res_head, res_where, res_group, res_distinct
    except ImportError:
        # Fallback if dataset module not available
        res_head = res_where = res_group = res_distinct = None
    
    # Bind sparql_query with default endpoint and dataset integration
    if ds_meta is not None:
        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns, ds_meta=ds_meta)
    else:
        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns)
    
    # Bind sparql_local
    ns['sparql_local'] = partial(sparql_local, ns=ns)
    
    # Bind view operations
    if res_head is not None:
        ns['res_head'] = res_head
        ns['res_where'] = res_where
        ns['res_group'] = res_group
        ns['res_distinct'] = res_distinct
    ns['res_sample'] = res_sample
    
    bound_funcs = ['sparql_query', 'sparql_local', 'res_sample']
    if res_head is not None:
        bound_funcs.extend(['res_head', 'res_where', 'res_group', 'res_distinct'])
    
    msg = f"SPARQL context initialized with endpoint: {default_endpoint}"
    if ds_meta is not None:
        msg += f"\nDataset integration enabled (session: {ds_meta.session_id})"
    msg += f"\nBound functions: {', '.join(bound_funcs)}"
    
    return msg
