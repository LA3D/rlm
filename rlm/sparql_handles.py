"""SPARQL query execution with first-class result handles"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_sparql_handles.ipynb.

# %% auto 0
__all__ = ['SPARQLResultHandle', 'sparql_query', 'sparql_local', 'res_sample', 'setup_sparql_context']

# %% ../nbs/03_sparql_handles.ipynb 4
from sparqlx import SPARQLWrapper
from rdflib import Graph, URIRef, Literal
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from functools import partial
from itertools import islice
import random
from typing import Optional

# %% ../nbs/03_sparql_handles.ipynb 6
@dataclass
class SPARQLResultHandle:
    """Wrapper for SPARQL results with metadata and bounded view operations."""

    # Result data (never dumped wholesale to LLM)
    rows: list | Graph          # SELECT rows or CONSTRUCT/DESCRIBE graph
    result_type: str            # 'select' | 'ask' | 'construct' | 'describe'

    # Metadata
    query: str                  # Original SPARQL query
    endpoint: str               # Where executed (URL or 'local')
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat() + 'Z')

    # For SELECT results
    columns: list = None        # Column names
    total_rows: int = 0         # Total before limit (may be same as len(rows) if not truncated)

    # For Graph results (CONSTRUCT/DESCRIBE)
    triple_count: int = 0       # Number of triples actually stored in self.rows
    total_triples: int = 0      # Total before limit (may be same as triple_count if not truncated)

    def summary(self) -> str:
        """Bounded summary for LLM - reports actual stored count."""
        if self.result_type == 'select':
            stored = len(self.rows)
            if self.total_rows > stored:
                return f"SELECT: {stored} rows (of {self.total_rows} total), columns={self.columns}"
            return f"SELECT: {stored} rows, columns={self.columns}"
        elif self.result_type == 'ask':
            return f"ASK: {self.rows}"
        else:
            # For CONSTRUCT/DESCRIBE, report actual stored count
            if self.total_triples > self.triple_count:
                return f"{self.result_type.upper()}: {self.triple_count} triples (of {self.total_triples} total)"
            return f"{self.result_type.upper()}: {self.triple_count} triples"

    def __len__(self):
        if isinstance(self.rows, bool):
            return 1  # ASK result
        return len(self.rows) if hasattr(self.rows, '__len__') else 0

    def __iter__(self):
        if isinstance(self.rows, bool):
            return iter([self.rows])
        return iter(self.rows)
    
    def __repr__(self):
        return f"SPARQLResultHandle({self.summary()})"

# %% ../nbs/03_sparql_handles.ipynb 11
import re

def _inject_limit(query: str, limit: int) -> tuple[str, bool]:
    """Inject LIMIT clause into SELECT query if not already present.
    
    Args:
        query: SPARQL query string
        limit: LIMIT value to inject
        
    Returns:
        (modified_query, was_modified) tuple
    """
    query_upper = query.upper()
    
    # Only inject for SELECT queries
    if 'SELECT' not in query_upper:
        return query, False
    
    # Check if LIMIT already present
    if re.search(r'\bLIMIT\s+\d+', query_upper):
        return query, False
    
    # Find where to inject LIMIT (before ORDER BY if present, otherwise at end)
    order_match = re.search(r'\bORDER\s+BY\b', query_upper)
    if order_match:
        # Insert LIMIT before ORDER BY
        pos = order_match.start()
        modified = query[:pos] + f' LIMIT {limit} ' + query[pos:]
        return modified, True
    else:
        # Append LIMIT at end (before any trailing whitespace/comments)
        modified = query.rstrip() + f' LIMIT {limit}'
        return modified, True

# %% ../nbs/03_sparql_handles.ipynb 13
def sparql_query(
    query: str,
    endpoint: str = "https://query.wikidata.org/sparql",
    max_results: int = 100,
    name: str = 'res',
    ns: dict = None,
    timeout: float = 30.0,
    # Dataset integration
    ds_meta = None,
    store_in_work: bool = False,
    work_task_id: str = None
) -> str:
    """Execute SPARQL query, store SPARQLResultHandle in namespace.

    For SELECT: Stores SPARQLResultHandle with rows as list of dicts
    For CONSTRUCT/DESCRIBE: Stores SPARQLResultHandle with rdflib.Graph
    For ASK: Stores SPARQLResultHandle with boolean result

    IMPORTANT - Work Bounds:
    - For SELECT: Automatically injects LIMIT clause to bound server-side work
    - For CONSTRUCT/DESCRIBE: max_results only truncates locally; full results 
      still fetched from endpoint (SPARQL 1.1 has no standard LIMIT for graphs)

    If ds_meta provided and store_in_work=True:
    - CONSTRUCT results stored in work/<task_id> graph
    - Query logged to prov graph
    
    Args:
        query: SPARQL query string
        endpoint: SPARQL endpoint URL
        max_results: Maximum results to return (for SELECT/CONSTRUCT)
        name: Variable name to store result handle
        ns: Namespace dict (defaults to globals())
        timeout: Query timeout in seconds
        ds_meta: Optional DatasetMeta for dataset integration
        store_in_work: If True and ds_meta provided, store CONSTRUCT results in work graph
        work_task_id: Task ID for work graph (auto-generated if None)
        
    Returns:
        Summary string describing the result
    """
    if ns is None:
        ns = globals()
    
    # Try to inject LIMIT for SELECT queries to bound server-side work
    modified_query, limit_injected = _inject_limit(query, max_results)
    
    # Configure wrapper with timeout and headers
    headers = {"User-Agent": "RLM/1.0 (https://github.com/LA3D/rlm)"}
    wrapper = SPARQLWrapper(
        sparql_endpoint=endpoint,
        client_config=dict(timeout=timeout, headers=headers)
    )
    
    # Execute query with rdflib conversion (use modified_query if LIMIT was injected)
    result = wrapper.query(modified_query, convert=True)
    
    # Determine result type and create handle
    if isinstance(result, bool):
        # ASK query
        handle = SPARQLResultHandle(
            rows=result,
            result_type='ask',
            query=query,  # Store original query
            endpoint=endpoint
        )
        ns[name] = handle
        return f"ASK result: {result}, stored in '{name}'"
    
    elif hasattr(result, 'serialize'):
        # CONSTRUCT or DESCRIBE query - result is rdflib.Graph
        # NOTE: SPARQL 1.1 has no standard LIMIT for graph patterns, so we still
        # fetch full results from endpoint and truncate locally. This is a known
        # limitation - max_results is output truncation, not work bound.
        
        # Capture original count before truncation
        original_count = len(result)
        
        # Use islice to limit triples
        g = Graph()
        for triple in islice(result, max_results):
            g.add(triple)
        
        # Determine if CONSTRUCT or DESCRIBE
        query_upper = query.upper()
        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'
        
        handle = SPARQLResultHandle(
            rows=g,
            result_type=result_type,
            query=query,
            endpoint=endpoint,
            triple_count=len(g),        # Actual stored count
            total_triples=original_count  # Pre-truncation count
        )
        ns[name] = handle
        
        # Dataset integration for CONSTRUCT results
        if ds_meta is not None and store_in_work:
            from rlm.dataset import work_create
            import uuid
            
            task_id = work_task_id if work_task_id else f"sparql_{uuid.uuid4().hex[:8]}"
            graph_uri, work_graph = work_create(ds_meta, task_id)
            
            # Copy triples to work graph
            for s, p, o in g:
                work_graph.add((s, p, o))
            
            # Log query to prov
            from rdflib import Namespace, RDF, XSD
            RLM_PROV = Namespace('urn:rlm:prov:')
            event_uri = URIRef(f'urn:rlm:prov:sparql_{uuid.uuid4().hex[:8]}')
            ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.SPARQLQuery))
            ds_meta.prov.add((event_uri, RLM_PROV.query, Literal(query)))
            ds_meta.prov.add((event_uri, RLM_PROV.endpoint, Literal(endpoint)))
            ds_meta.prov.add((event_uri, RLM_PROV.resultGraph, URIRef(graph_uri)))
            ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(handle.timestamp, datatype=XSD.dateTime)))
            ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))
            
            return f"Graph with {len(g)} triples stored in '{name}' and work/{task_id}" + \
                   (f" (of {original_count} total)" if original_count > len(g) else "")
        
        return f"Graph with {len(g)} triples stored in '{name}'" + \
               (f" (of {original_count} total)" if original_count > len(g) else "")
    
    else:
        # SELECT query - result is list of dicts
        # If we injected LIMIT, the result is already bounded
        result_list = list(result)
        original_count = len(result_list)
        
        # Apply additional client-side truncation if needed
        # (this should rarely happen if LIMIT injection worked)
        rows = list(islice(result_list, max_results))
        cols = list(rows[0].keys()) if rows else []
        
        handle = SPARQLResultHandle(
            rows=rows,
            result_type='select',
            query=query,  # Store original query
            endpoint=endpoint,
            columns=cols,
            total_rows=original_count  # This may equal len(rows) if LIMIT injection worked
        )
        ns[name] = handle
        
        msg = f"SELECT result with {len(rows)} rows, columns: {cols}, stored in '{name}'"
        if limit_injected:
            msg += f" (LIMIT {max_results} injected)"
        return msg

# %% ../nbs/03_sparql_handles.ipynb 17
def sparql_local(
    query: str,
    graph: Graph | str,
    max_results: int = 100,
    name: str = 'res',
    ns: dict = None
) -> str:
    """Execute SPARQL query on local rdflib Graph.

    Useful for querying mounted ontologies or work graphs.
    Returns SPARQLResultHandle same as sparql_query().
    
    IMPORTANT - Work Bounds:
    - max_results is output truncation only; full result set is materialized
    - For large local graphs, consider filtering in the SPARQL query itself
    
    Args:
        query: SPARQL query string
        graph: rdflib.Graph object or name of graph in namespace
        max_results: Maximum results to return
        name: Variable name to store result handle
        ns: Namespace dict (defaults to globals())
        
    Returns:
        Summary string describing the result
    """
    if ns is None:
        ns = globals()
    
    # Resolve graph if string name provided
    if isinstance(graph, str):
        if graph not in ns:
            return f"Error: Graph '{graph}' not found in namespace"
        graph_obj = ns[graph]
        # Handle GraphMeta wrapper
        if hasattr(graph_obj, 'graph'):
            graph_obj = graph_obj.graph
    else:
        graph_obj = graph
    
    if not isinstance(graph_obj, Graph):
        return f"Error: Expected rdflib.Graph, got {type(graph_obj)}"
    
    # Execute query on local graph
    result = graph_obj.query(query)
    
    # Determine result type
    query_upper = query.upper()
    
    if 'ASK' in query_upper:
        # ASK query
        ask_result = bool(result)
        handle = SPARQLResultHandle(
            rows=ask_result,
            result_type='ask',
            query=query,
            endpoint='local'
        )
        ns[name] = handle
        return f"ASK result: {ask_result}, stored in '{name}'"
    
    elif 'CONSTRUCT' in query_upper or 'DESCRIBE' in query_upper:
        # CONSTRUCT or DESCRIBE query
        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'
        
        # Capture original count before truncation
        result_list = list(result)
        original_count = len(result_list)
        
        # Use islice to limit triples
        g = Graph()
        for triple in islice(result_list, max_results):
            g.add(triple)
        
        handle = SPARQLResultHandle(
            rows=g,
            result_type=result_type,
            query=query,
            endpoint='local',
            triple_count=len(g),        # Actual stored count
            total_triples=original_count  # Pre-truncation count
        )
        ns[name] = handle
        
        msg = f"Graph with {len(g)} triples stored in '{name}'"
        if original_count > len(g):
            msg += f" (of {original_count} total)"
        return msg
    
    else:
        # SELECT query
        # Capture original count before truncation
        result_list = list(result)
        original_count = len(result_list)
        
        rows = []
        for row in islice(result_list, max_results):
            row_dict = {}
            for var in result.vars:
                row_dict[str(var)] = row[var] if row[var] else None
            rows.append(row_dict)
        
        cols = [str(v) for v in result.vars] if result.vars else []
        
        handle = SPARQLResultHandle(
            rows=rows,
            result_type='select',
            query=query,
            endpoint='local',
            columns=cols,
            total_rows=original_count  # Pre-truncation count
        )
        ns[name] = handle
        
        msg = f"SELECT result with {len(rows)} rows, columns: {cols}, stored in '{name}'"
        if original_count > len(rows):
            msg += f" (of {original_count} total)"
        return msg

# %% ../nbs/03_sparql_handles.ipynb 21
def res_sample(result, n: int = 10, seed: int = None) -> list:
    """Get random sample of N rows from result.

    Args:
        result: SPARQLResultHandle, ResultTable, or list
        n: Number of rows to sample
        seed: Optional random seed for reproducibility

    Returns:
        List of sampled rows
    """
    if seed is not None:
        random.seed(seed)

    # Extract rows from different result types
    if isinstance(result, SPARQLResultHandle):
        if result.result_type in ['construct', 'describe']:
            # For graphs, sample triples
            rows = list(result.rows)
        elif result.result_type == 'ask':
            # ASK has no rows to sample
            return [result.rows]
        else:
            rows = result.rows
    elif hasattr(result, 'rows'):
        # ResultTable or similar
        rows = result.rows
    else:
        # Plain list
        rows = result

    if len(rows) <= n:
        return list(rows)
    return random.sample(list(rows), n)

# %% ../nbs/03_sparql_handles.ipynb 25
def setup_sparql_context(
    ns: dict,
    default_endpoint: str = "https://query.wikidata.org/sparql",
    ds_meta = None
) -> str:
    """Initialize SPARQL tools in namespace.

    Binds:
    - sparql_query() with default endpoint
    - sparql_local() if ds_meta provided
    - res_head(), res_where(), res_group(), res_distinct(), res_sample()

    Args:
        ns: Namespace dict where functions will be bound
        default_endpoint: Default SPARQL endpoint URL
        ds_meta: Optional DatasetMeta for dataset integration
        
    Returns:
        Status message
    """
    # Import view functions from dataset module
    try:
        from rlm.dataset import res_head, res_where, res_group, res_distinct
    except ImportError:
        # Fallback if dataset module not available
        res_head = res_where = res_group = res_distinct = None
    
    # Bind sparql_query with default endpoint and dataset integration
    if ds_meta is not None:
        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns, ds_meta=ds_meta)
    else:
        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns)
    
    # Bind sparql_local
    ns['sparql_local'] = partial(sparql_local, ns=ns)
    
    # Bind view operations
    if res_head is not None:
        ns['res_head'] = res_head
        ns['res_where'] = res_where
        ns['res_group'] = res_group
        ns['res_distinct'] = res_distinct
    ns['res_sample'] = res_sample
    
    bound_funcs = ['sparql_query', 'sparql_local', 'res_sample']
    if res_head is not None:
        bound_funcs.extend(['res_head', 'res_where', 'res_group', 'res_distinct'])
    
    msg = f"SPARQL context initialized with endpoint: {default_endpoint}"
    if ds_meta is not None:
        msg += f"\nDataset integration enabled (session: {ds_meta.session_id})"
    msg += f"\nBound functions: {', '.join(bound_funcs)}"
    
    return msg
