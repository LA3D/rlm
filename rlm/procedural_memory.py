"""ReasoningBank-style procedural memory for RLM"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_procedural_memory.ipynb.

# %% auto 0
__all__ = ['MemoryItem', 'MemoryStore', 'extract_trajectory_artifact', 'judge_trajectory', 'extract_memories',
           'retrieve_memories', 'format_memories_for_injection', 'rlm_run_with_memory']

# %% ../nbs/05_procedural_memory.ipynb 4
from dataclasses import dataclass, field, asdict
from typing import Optional
from pathlib import Path
from datetime import datetime
import json
import uuid
from rank_bm25 import BM25Okapi

# %% ../nbs/05_procedural_memory.ipynb 5
from .core import llm_query, rlm_run
from ._rlmpaper_compat import RLMIteration

# %% ../nbs/05_procedural_memory.ipynb 7
@dataclass
class MemoryItem:
    """A reusable procedural memory extracted from an RLM trajectory.
    
    Attributes:
        id: Unique identifier (UUID)
        title: Concise identifier (≤10 words)
        description: One-sentence summary
        content: Procedural steps/checklist/template (Markdown)
        source_type: 'success' or 'failure'
        task_query: Original task that produced this memory
        created_at: ISO timestamp
        access_count: Number of times retrieved (for future consolidation)
        tags: Keywords for BM25 retrieval
        session_id: Optional session ID from DatasetMeta (links to dataset session)
    """
    id: str
    title: str
    description: str
    content: str
    source_type: str  # 'success' or 'failure'
    task_query: str
    created_at: str
    access_count: int = 0
    tags: Optional[list[str]] = None
    session_id: Optional[str] = None  # NEW: Links to DatasetMeta.session_id
    
    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict) -> 'MemoryItem':
        """Create MemoryItem from dictionary."""
        return cls(**data)

# %% ../nbs/05_procedural_memory.ipynb 10
@dataclass
class MemoryStore:
    """Persistent storage for procedural memories.
    
    Attributes:
        memories: List of MemoryItem objects
        path: Path to JSON file
    """
    memories: list[MemoryItem] = field(default_factory=list)
    path: Optional[Path] = None
    
    def add(self, item: MemoryItem) -> str:
        """Add a memory item to the store.
        
        Returns:
            Status message
        """
        self.memories.append(item)
        return f"Added memory '{item.title}' (id={item.id})"
    
    def save(self) -> str:
        """Persist memories to JSON file.
        
        Returns:
            Status message with path and count
        """
        if self.path is None:
            return "No path configured - not saving"
        
        self.path.parent.mkdir(parents=True, exist_ok=True)
        data = [m.to_dict() for m in self.memories]
        
        with open(self.path, 'w') as f:
            json.dump(data, f, indent=2)
        
        return f"Saved {len(self.memories)} memories to {self.path}"
    
    @classmethod
    def load(cls, path: Path) -> 'MemoryStore':
        """Load memories from JSON file.
        
        Returns:
            MemoryStore instance with loaded memories
        """
        if not path.exists():
            return cls(memories=[], path=path)
        
        with open(path, 'r') as f:
            data = json.load(f)
        
        memories = [MemoryItem.from_dict(item) for item in data]
        return cls(memories=memories, path=path)
    
    def get_corpus_for_bm25(self) -> list[list[str]]:
        """Build corpus for BM25 indexing.
        
        Each document is title + description + tags, tokenized.
        
        Returns:
            List of tokenized documents
        """
        corpus = []
        for m in self.memories:
            text = f"{m.title} {m.description}"
            if m.tags:
                text += " " + " ".join(m.tags)
            corpus.append(text.lower().split())
        return corpus

# %% ../nbs/05_procedural_memory.ipynb 13
def extract_trajectory_artifact(
    task: str,
    answer: str,
    iterations: list[RLMIteration],
    ns: dict
) -> dict:
    """Create bounded trajectory artifact for judge/extractor.
    
    Summarizes each iteration's code blocks into 1-2 line "action + outcome",
    limiting to ~10 most informative key steps.
    
    Args:
        task: Original task query
        answer: Final answer from rlm_run
        iterations: List of RLMIteration objects
        ns: Final namespace dict
    
    Returns:
        Dictionary with keys:
        - task: str
        - final_answer: str
        - iteration_count: int
        - converged: bool (whether final_answer was set)
        - key_steps: List of {iteration, action, outcome}
        - variables_created: List of variable names in ns
        - errors_encountered: List of error messages from stderr
    """
    key_steps = []
    errors = []
    
    for i, iteration in enumerate(iterations, 1):
        # Summarize code blocks in this iteration
        for block in iteration.code_blocks:
            # Extract action from code (first line or summary)
            code_lines = block.code.strip().split('\n')
            action = code_lines[0][:80] if code_lines else "[empty code]"
            
            # Extract outcome from result
            if block.result and block.result.stderr:
                outcome = f"ERROR: {block.result.stderr[:100]}"
                errors.append(block.result.stderr)
            elif block.result and block.result.stdout:
                outcome = block.result.stdout[:100]
            else:
                outcome = "(no output)"
            
            key_steps.append({
                'iteration': i,
                'action': action,
                'outcome': outcome
            })
    
    # Limit to 10 most informative steps (prioritize errors and final steps)
    if len(key_steps) > 10:
        # Keep first 3, last 4, and up to 3 with errors
        error_steps = [s for s in key_steps if 'ERROR' in s['outcome']]
        key_steps = key_steps[:3] + error_steps[:3] + key_steps[-4:]
        # Remove duplicates while preserving order
        seen = set()
        key_steps = [s for s in key_steps if not (tuple(s.items()) in seen or seen.add(tuple(s.items())))]
        key_steps = key_steps[:10]
    
    return {
        'task': task,
        'final_answer': answer,
        'iteration_count': len(iterations),
        'converged': bool(answer and answer != "No answer provided"),
        'key_steps': key_steps,
        'variables_created': list(ns.keys()) if ns else [],
        'errors_encountered': errors
    }

# %% ../nbs/05_procedural_memory.ipynb 16
def judge_trajectory(artifact: dict, ns: dict = None) -> dict:
    """Judge trajectory success using llm_query.
    
    Evidence-sensitive: success requires grounding in retrieved evidence.
    
    Args:
        artifact: Trajectory artifact from extract_trajectory_artifact()
        ns: Optional namespace for additional context
    
    Returns:
        Dictionary with keys:
        - is_success: bool
        - reason: str
        - confidence: str ('high', 'medium', 'low')
        - missing: list[str] (what evidence was lacking if failure)
    """
    # Format key steps for prompt
    steps_text = "\n".join([
        f"  {s['iteration']}. {s['action']} → {s['outcome']}"
        for s in artifact['key_steps']
    ])
    
    prompt = f"""Evaluate this RLM trajectory for task completion quality.

Task: {artifact['task']}
Final Answer: {artifact['final_answer']}
Converged: {artifact['converged']}
Key Steps:
{steps_text}

A trajectory is SUCCESSFUL if:
1. The answer directly addresses the task
2. The answer is grounded in retrieved evidence (not hallucinated)
3. The reasoning steps show systematic exploration

A trajectory FAILED if:
1. No answer was produced (didn't converge)
2. Answer doesn't address the task
3. Answer makes claims without supporting evidence

Return ONLY valid JSON:
{{"is_success": true/false, "reason": "...", "confidence": "high/medium/low", "missing": ["..."]}}"""
    
    # Use llm_query to get judgment (create temp namespace)
    temp_ns = ns if ns is not None else {}
    response = llm_query(prompt, temp_ns, name='judgment_response')
    
    # Parse JSON response
    try:
        # Try to extract JSON from response
        response_text = response.strip()
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()
        
        judgment = json.loads(response_text)
        
        # Ensure required fields
        if 'missing' not in judgment:
            judgment['missing'] = []
        
        return judgment
    except (json.JSONDecodeError, IndexError) as e:
        # Fallback for parsing errors
        return {
            'is_success': artifact['converged'],
            'reason': f"Parse error: {e}. Raw response: {response[:200]}",
            'confidence': 'low',
            'missing': ['Unable to parse judgment']
        }

# %% ../nbs/05_procedural_memory.ipynb 19
def extract_memories(
    artifact: dict,
    judgment: dict,
    ns: dict = None
) -> list[MemoryItem]:
    """Extract up to 3 reusable memory items from trajectory.
    
    Args:
        artifact: Trajectory artifact from extract_trajectory_artifact()
        judgment: Judgment dict from judge_trajectory()
        ns: Optional namespace for additional context
    
    Returns:
        List of MemoryItem objects (0-3 items)
    """
    source_type = 'success' if judgment['is_success'] else 'failure'
    
    # Capture session_id if available in namespace
    session_id = None
    if ns is not None:
        # Try to get session_id from DatasetMeta
        if 'ds_meta' in ns and hasattr(ns['ds_meta'], 'session_id'):
            session_id = ns['ds_meta'].session_id
    
    # Format key steps for prompt
    steps_text = "\n".join([
        f"  {s['iteration']}. {s['action']} → {s['outcome']}"
        for s in artifact['key_steps']
    ])
    
    prompt = f"""Extract reusable procedural memories from this {source_type} trajectory.

Task: {artifact['task']}
Outcome: {judgment['reason']}
Key Steps:
{steps_text}

Extract UP TO 3 distinct, reusable insights. Each should be:
- Procedural (steps/checklist/template), NOT a retelling of this run
- Applicable to similar future tasks
- Concise but actionable

For ontology/SPARQL work, prefer:
- Query templates with placeholders
- Debugging strategies ("if X fails, try Y")
- Exploration patterns ("start with search, then describe, then probe")

Return ONLY valid JSON array (may have 1-3 items, or empty if no lessons):
[{{
  "title": "≤10 word identifier",
  "description": "One sentence summary",
  "content": "Markdown with steps/checklist/template",
  "tags": ["keyword1", "keyword2"]
}}]"""
    
    # Use llm_query to extract memories (create temp namespace)
    temp_ns = ns if ns is not None else {}
    response = llm_query(prompt, temp_ns, name='extractor_response')
    
    # Parse JSON response
    try:
        response_text = response.strip()
        if '```json' in response_text:
            response_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()
        
        extracted = json.loads(response_text)
        
        # Convert to MemoryItem objects
        memories = []
        for item in extracted[:3]:  # Limit to 3
            memory = MemoryItem(
                id=str(uuid.uuid4()),
                title=item['title'],
                description=item['description'],
                content=item['content'],
                source_type=source_type,
                task_query=artifact['task'],
                created_at=datetime.utcnow().isoformat(),
                tags=item.get('tags', []),
                session_id=session_id  # NEW: Capture session_id from namespace
            )
            memories.append(memory)
        
        return memories
    except (json.JSONDecodeError, KeyError, IndexError) as e:
        # Return empty list on parsing errors
        print(f"Warning: Failed to extract memories: {e}")
        return []

# %% ../nbs/05_procedural_memory.ipynb 22
def retrieve_memories(
    store: MemoryStore,
    task: str,
    k: int = 3
) -> list[MemoryItem]:
    """Retrieve top-k relevant memories using BM25.
    
    Tokenizes task and searches over title + description + tags.
    
    Args:
        store: MemoryStore instance
        task: Task query string
        k: Number of memories to retrieve
    
    Returns:
        List of top-k MemoryItem objects (may be fewer if scores ≤ 0)
    """
    if not store.memories:
        return []
    
    # Build BM25 index
    corpus = store.get_corpus_for_bm25()
    bm25 = BM25Okapi(corpus)
    
    # Query
    query_tokens = task.lower().split()
    scores = bm25.get_scores(query_tokens)
    
    # Get top-k by score (BM25 can return negative scores for small corpora)
    scored = [(i, s) for i, s in enumerate(scores)]
    scored.sort(key=lambda x: x[1], reverse=True)
    
    # Increment access_count for retrieved memories
    results = []
    for i, _ in scored[:k]:
        store.memories[i].access_count += 1
        results.append(store.memories[i])
    
    return results

# %% ../nbs/05_procedural_memory.ipynb 25
def format_memories_for_injection(
    memories: list[MemoryItem],
    max_bullets: int = 3
) -> str:
    """Format memories for bounded prompt injection.
    
    Returns string with:
    - Assessment instruction
    - Title + description + key bullets from content (up to max_bullets)
    
    Args:
        memories: List of MemoryItem objects to format
        max_bullets: Maximum bullets to extract from content
    
    Returns:
        Formatted string for prompt injection
    """
    if not memories:
        return ""
    
    lines = [
        "## Relevant Prior Experience",
        "",
        "Before taking action, briefly assess which of these strategies apply to your current task and which do not.",
        ""
    ]
    
    for i, mem in enumerate(memories, 1):
        lines.append(f"### {i}. {mem.title}")
        lines.append(mem.description)
        lines.append("Key points:")
        
        # Extract bullets from content (look for lines starting with - or numbers)
        content_lines = mem.content.split('\n')
        bullets = []
        for line in content_lines:
            stripped = line.strip()
            if stripped.startswith('-') or stripped.startswith('*'):
                bullets.append(stripped)
            elif len(stripped) > 0 and stripped[0].isdigit() and '.' in stripped:
                bullets.append(stripped)
        
        # Use first max_bullets bullets, or first max_bullets lines if no bullets found
        if bullets:
            for bullet in bullets[:max_bullets]:
                lines.append(f"- {bullet.lstrip('- *')}")
        else:
            # Fall back to first few lines
            for line in content_lines[:max_bullets]:
                if line.strip():
                    lines.append(f"- {line.strip()}")
        
        lines.append("")  # Blank line between memories
    
    return "\n".join(lines)

# %% ../nbs/05_procedural_memory.ipynb 28
def rlm_run_with_memory(
    query: str,
    context: str,
    memory_store: MemoryStore,
    ns: dict = None,
    enable_memory_extraction: bool = True,
    # NEW: Dataset persistence
    persist_dataset: bool = False,
    dataset_path: Path = None,
    **kwargs
) -> tuple[str, list, dict, list[MemoryItem]]:
    """RLM run with procedural memory loop.
    
    Closed-loop cycle:
    1. RETRIEVE: Get relevant memories via BM25
    2. INJECT: Add to context/prompt
    3. INTERACT: Run rlm_run()
    4. EXTRACT: Judge + extract new memories
    5. STORE: Persist new memories
    
    NEW: Dataset persistence:
    - If persist_dataset=True and dataset_path provided, loads snapshot before run
    - After run, if dataset was modified, saves snapshot
    - Stores snapshot path in extracted MemoryItem for lineage
    
    Args:
        query: Task query string
        context: Context string (e.g., ontology summary)
        memory_store: MemoryStore instance for retrieval/storage
        ns: Optional namespace dict
        enable_memory_extraction: Whether to extract and store new memories (default True)
        persist_dataset: Whether to persist dataset snapshots (default False)
        dataset_path: Optional path for dataset snapshot
        **kwargs: Additional arguments for rlm_run()
    
    Returns:
        Tuple of (answer, iterations, ns, new_memories)
    """
    # NEW: Load dataset snapshot if it exists
    if persist_dataset and dataset_path is not None and dataset_path.exists():
        try:
            from rlm.dataset import load_snapshot
            load_snapshot(str(dataset_path), ns)
        except Exception as e:
            print(f"Warning: Failed to load dataset snapshot: {e}")
    
    # 1. RETRIEVE relevant memories
    relevant = retrieve_memories(memory_store, query, k=3)
    
    # 2. INJECT into context
    if relevant:
        memory_text = format_memories_for_injection(relevant)
        enhanced_context = f"{memory_text}\n\n---\n\n{context}"
    else:
        enhanced_context = context
    
    # 3. INTERACT - run RLM
    answer, iterations, ns = rlm_run(query, enhanced_context, ns=ns, **kwargs)
    
    # NEW: Save dataset snapshot if dataset was modified
    if persist_dataset and dataset_path is not None:
        try:
            from rlm.dataset import snapshot_dataset
            if 'ds_meta' in ns:
                result = snapshot_dataset(ns['ds_meta'], path=str(dataset_path))
                print(f"Dataset snapshot: {result}")
        except Exception as e:
            print(f"Warning: Failed to save dataset snapshot: {e}")
    
    new_memories = []
    if enable_memory_extraction:
        # 4. EXTRACT - judge and extract memories
        artifact = extract_trajectory_artifact(query, answer, iterations, ns)
        judgment = judge_trajectory(artifact, ns)
        new_memories = extract_memories(artifact, judgment, ns)
        
        # 5. STORE - persist new memories
        for mem in new_memories:
            memory_store.add(mem)
        if memory_store.path:
            memory_store.save()
    
    return answer, iterations, ns, new_memories
