"""
Protocol artifacts from rlmpaper for RLM implementation.

This module contains prompts, types, and parsing logic from the rlmpaper reference
implementation (https://github.com/alexzhang13/rlm). We use claudette for LLM calls
but follow rlmpaper's protocol for:

- System and user prompts (including first-iteration safeguard)
- Type definitions (QueryMetadata, REPLResult, CodeBlock, RLMIteration)
- Parsing patterns (FINAL/FINAL_VAR, code blocks)
- Iteration formatting

This is a manually-maintained file, not generated by nbdev.
"""

import re
import textwrap
from dataclasses import dataclass, field
from typing import Any, Literal

__all__ = [
    # Prompts
    'RLM_SYSTEM_PROMPT',
    'USER_PROMPT',
    'USER_PROMPT_WITH_ROOT',
    'build_rlm_system_prompt',
    'build_user_prompt',
    # Types
    'QueryMetadata',
    'REPLResult',
    'CodeBlock',
    'RLMIteration',
    'RLMChatCompletion',
    'UsageSummary',
    # Parsing
    'find_code_blocks',
    'find_final_answer',
    'format_iteration',
    'format_execution_result',
]

# =============================================================================
# System Prompt (from rlmpaper/rlm/utils/prompts.py)
# =============================================================================

RLM_SYSTEM_PROMPT = textwrap.dedent(
    """You are tasked with answering a query with associated context. You can access, transform, and analyze this context interactively in a REPL environment that can recursively query sub-LLMs, which you are strongly encouraged to use as much as possible. You will be queried iteratively until you provide a final answer.

The REPL environment is initialized with:
1. A `context` variable that contains extremely important information about your query. You should check the content of the `context` variable to understand what you are working with. Make sure you look through it sufficiently as you answer your query.
2. A `llm_query` function that allows you to query an LLM (that can handle around 500K chars) inside your REPL environment.
3. A `llm_query_batched` function that allows you to query multiple prompts concurrently: `llm_query_batched(prompts: List[str]) -> List[str]`. This is much faster than sequential `llm_query` calls when you have multiple independent queries. Results are returned in the same order as the input prompts.
4. The ability to use `print()` statements to view the output of your REPL code and continue your reasoning.

You will only be able to see truncated outputs from the REPL environment, so you should use the query LLM function on variables you want to analyze. You will find this function especially useful when you have to analyze the semantics of the context. Use these variables as buffers to build up your final answer.
Make sure to explicitly look through the entire context in REPL before answering your query. An example strategy is to first look at the context and figure out a chunking strategy, then break up the context into smart chunks, and query an LLM per chunk with a particular question and save the answers to a buffer, then query an LLM with all the buffers to produce your final answer.

You can use the REPL environment to help you understand your context, especially if it is huge. Remember that your sub LLMs are powerful -- they can fit around 500K characters in their context window, so don't be afraid to put a lot of context into them. For example, a viable strategy is to feed 10 documents per sub-LLM query. Analyze your input data and see if it is sufficient to just fit it in a few sub-LLM calls!

When you want to execute Python code in the REPL environment, wrap it in triple backticks with 'repl' language identifier. For example, say we want our recursive model to search for the magic number in the context (assuming the context is a string), and the context is very long, so we want to chunk it:
```repl
chunk = context[:10000]
answer = llm_query(f"What is the magic number in the context? Here is the chunk: {chunk}")
print(answer)
```

As an example, suppose you're trying to answer a question about a book. You can iteratively chunk the context section by section, query an LLM on that chunk, and track relevant information in a buffer.
```repl
query = "In Harry Potter and the Sorcerer's Stone, did Gryffindor win the House Cup because they led?"
for i, section in enumerate(context):
    if i == len(context) - 1:
        buffer = llm_query(f"You are on the last section of the book. So far you know that: {buffers}. Gather from this last section to answer {query}. Here is the section: {section}")
        print(f"Based on reading iteratively through the book, the answer is: {buffer}")
    else:
        buffer = llm_query(f"You are iteratively looking through a book, and are on section {i} of {len(context)}. Gather information to help answer {query}. Here is the section: {section}")
        print(f"After section {i} of {len(context)}, you have tracked: {buffer}")
```

As another example, when the context isn't that long (e.g. >100M characters), a simple but viable strategy is, based on the context chunk lengths, to combine them and recursively query an LLM over chunks. For example, if the context is a List[str], we ask the same query over each chunk using `llm_query_batched` for concurrent processing:
```repl
query = "A man became famous for his book "The Great Gatsby". How many jobs did he have?"
# Suppose our context is ~1M chars, and we want each sub-LLM query to be ~0.1M chars so we split it into 10 chunks
chunk_size = len(context) // 10
chunks = []
for i in range(10):
    if i < 9:
        chunk_str = "\\n".join(context[i*chunk_size:(i+1)*chunk_size])
    else:
        chunk_str = "\\n".join(context[i*chunk_size:])
    chunks.append(chunk_str)

# Use batched query for concurrent processing - much faster than sequential calls!
prompts = [f"Try to answer the following query: {query}. Here are the documents:\\n{chunk}. Only answer if you are confident in your answer based on the evidence." for chunk in chunks]
answers = llm_query_batched(prompts)
for i, answer in enumerate(answers):
    print(f"I got the answer from chunk {i}: {answer}")
final_answer = llm_query(f"Aggregating all the answers per chunk, answer the original query about total number of jobs: {query}\\n\\nAnswers:\\n" + "\\n".join(answers))
```

As a final example, after analyzing the context and realizing its separated by Markdown headers, we can maintain state through buffers by chunking the context by headers, and iteratively querying an LLM over it:
```repl
# After finding out the context is separated by Markdown headers, we can chunk, summarize, and answer
import re
sections = re.split(r'### (.+)', context["content"])
buffers = []
for i in range(1, len(sections), 2):
    header = sections[i]
    info = sections[i+1]
    summary = llm_query(f"Summarize this {header} section: {info}")
    buffers.append(f"{header}: {summary}")
final_answer = llm_query(f"Based on these summaries, answer the original query: {query}\\n\\nSummaries:\\n" + "\\n".join(buffers))
```
In the next step, we can return FINAL_VAR(final_answer).

IMPORTANT: When you are done with the iterative process, you MUST provide a final answer inside a FINAL function when you have completed your task, NOT in code. Do not use these tags unless you have completed your task. You have two options:
1. Use FINAL(your final answer here) to provide the answer directly
2. Use FINAL_VAR(variable_name) to return a variable you have created in the REPL environment as your final output

Think step by step carefully, plan, and execute this plan immediately in your response -- do not just say "I will do this" or "I will do that". Output to the REPL environment and recursive LLMs as much as possible. Remember to explicitly answer the original query in your final answer.
"""
)

USER_PROMPT = """Think step-by-step on what to do using the REPL environment (which contains the context) to answer the prompt.\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:"""

USER_PROMPT_WITH_ROOT = """Think step-by-step on what to do using the REPL environment (which contains the context) to answer the original prompt: \"{root_prompt}\".\n\nContinue using the REPL environment, which has the `context` variable, and querying sub-LLMs by writing to ```repl``` tags, and determine your answer. Your next action:"""


# =============================================================================
# Types (from rlmpaper/rlm/core/types.py)
# =============================================================================

@dataclass
class UsageSummary:
    """Token usage summary for LLM calls."""
    model_usage: dict = field(default_factory=dict)

    def to_dict(self) -> dict:
        return {"model_usage": self.model_usage}


@dataclass
class RLMChatCompletion:
    """Record of a complete RLM completion call."""
    root_model: str
    prompt: str | dict[str, Any]
    response: str
    usage_summary: UsageSummary | None = None
    execution_time: float = 0.0

    def to_dict(self) -> dict:
        return {
            "root_model": self.root_model,
            "prompt": self.prompt,
            "response": self.response,
            "usage_summary": self.usage_summary.to_dict() if self.usage_summary else None,
            "execution_time": self.execution_time,
        }


@dataclass
class REPLResult:
    """Result from executing code in REPL environment.

    Aligned with rlmpaper/rlm/core/types.py REPLResult.
    """
    stdout: str = ''
    stderr: str = ''
    locals: dict = field(default_factory=dict)
    execution_time: float = 0.0
    llm_calls: list = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "stdout": self.stdout,
            "stderr": self.stderr,
            "locals": {k: repr(v)[:200] for k, v in self.locals.items()
                      if not k.startswith('_')},
            "execution_time": self.execution_time,
            "llm_calls": [c.to_dict() if hasattr(c, 'to_dict') else str(c)
                         for c in self.llm_calls],
        }


@dataclass
class CodeBlock:
    """Parsed code block with execution result.

    Aligned with rlmpaper/rlm/core/types.py CodeBlock.
    """
    code: str
    result: REPLResult | None = None

    def to_dict(self) -> dict:
        return {
            "code": self.code,
            "result": self.result.to_dict() if self.result else None,
        }


@dataclass
class RLMIteration:
    """Record of a single RLM iteration.

    Aligned with rlmpaper/rlm/core/types.py RLMIteration.
    """
    prompt: str | list[dict]
    response: str
    code_blocks: list[CodeBlock] = field(default_factory=list)
    final_answer: str | None = None
    iteration_time: float | None = None

    def to_dict(self) -> dict:
        return {
            "prompt": self.prompt,
            "response": self.response,
            "code_blocks": [cb.to_dict() for cb in self.code_blocks],
            "final_answer": self.final_answer,
            "iteration_time": self.iteration_time,
        }


@dataclass
class QueryMetadata:
    """Metadata about context for RLM prompts.

    Aligned with rlmpaper/rlm/core/types.py QueryMetadata.
    Auto-computes lengths and type from the prompt.
    """
    context_lengths: list[int] = field(default_factory=list)
    context_total_length: int = 0
    context_type: str = 'str'

    def __init__(self, prompt: str | list | dict):
        """Initialize metadata by auto-computing from prompt structure."""
        if isinstance(prompt, str):
            self.context_lengths = [len(prompt)]
            self.context_type = "str"
        elif isinstance(prompt, dict):
            self.context_type = "dict"
            self.context_lengths = []
            for chunk in prompt.values():
                if isinstance(chunk, str):
                    self.context_lengths.append(len(chunk))
                else:
                    try:
                        import json
                        self.context_lengths.append(len(json.dumps(chunk, default=str)))
                    except Exception:
                        self.context_lengths.append(len(repr(chunk)))
        elif isinstance(prompt, list):
            self.context_type = "list"
            if len(prompt) == 0:
                self.context_lengths = [0]
            elif isinstance(prompt[0], dict):
                if "content" in prompt[0]:
                    self.context_lengths = [len(str(chunk.get("content", ""))) for chunk in prompt]
                else:
                    self.context_lengths = []
                    for chunk in prompt:
                        try:
                            import json
                            self.context_lengths.append(len(json.dumps(chunk, default=str)))
                        except Exception:
                            self.context_lengths.append(len(repr(chunk)))
            else:
                self.context_lengths = [len(str(chunk)) for chunk in prompt]
        else:
            raise ValueError(f"Invalid prompt type: {type(prompt)}")

        self.context_total_length = sum(self.context_lengths)


# =============================================================================
# Prompt Builders (from rlmpaper/rlm/utils/prompts.py)
# =============================================================================

def build_rlm_system_prompt(
    system_prompt: str = None,
    query_metadata: QueryMetadata = None,
) -> list[dict[str, str]]:
    """Build the initial system prompt for the REPL environment.

    Args:
        system_prompt: Custom system prompt or use RLM_SYSTEM_PROMPT
        query_metadata: QueryMetadata object containing context metadata

    Returns:
        List of message dictionaries for chat initialization
    """
    if system_prompt is None:
        system_prompt = RLM_SYSTEM_PROMPT

    if query_metadata is None:
        return [{"role": "system", "content": system_prompt}]

    context_lengths = query_metadata.context_lengths
    context_total_length = query_metadata.context_total_length
    context_type = query_metadata.context_type

    # Truncate display if too many chunks
    if len(context_lengths) > 100:
        others = len(context_lengths) - 100
        lengths_display = str(context_lengths[:100]) + f"... [{others} others]"
    else:
        lengths_display = str(context_lengths)

    metadata_prompt = f"Your context is a {context_type} with {context_total_length} total characters, and is broken up into chunks of char lengths: {lengths_display}."

    return [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": metadata_prompt},
    ]


def build_user_prompt(
    root_prompt: str | None = None,
    iteration: int = 0,
    context_count: int = 1,
    history_count: int = 0,
) -> dict[str, str]:
    """Build iteration-aware user prompt with first-iteration safeguard.

    Args:
        root_prompt: The user's original query (shown in prompt if provided)
        iteration: Current iteration number (0-indexed)
        context_count: Number of contexts available
        history_count: Number of prior conversation histories

    Returns:
        User message dict with appropriate safeguards
    """
    if iteration == 0:
        # First-iteration safeguard: prevent premature answering
        safeguard = (
            "You have not interacted with the REPL environment or seen your "
            "prompt / context yet. Your next action should be to look through "
            "and figure out how to answer the prompt, so don't just provide a "
            "final answer yet.\n\n"
        )
        prompt = safeguard + (
            USER_PROMPT_WITH_ROOT.format(root_prompt=root_prompt)
            if root_prompt else USER_PROMPT
        )
    else:
        prompt = "The history before is your previous interactions with the REPL environment. " + (
            USER_PROMPT_WITH_ROOT.format(root_prompt=root_prompt)
            if root_prompt else USER_PROMPT
        )

    # Inform model about multiple contexts if present
    if context_count > 1:
        prompt += f"\n\nNote: You have {context_count} contexts available (context_0 through context_{context_count - 1})."

    # Inform model about prior conversation histories if present
    if history_count > 0:
        if history_count == 1:
            prompt += "\n\nNote: You have 1 prior conversation history available in the `history` variable."
        else:
            prompt += f"\n\nNote: You have {history_count} prior conversation histories available (history_0 through history_{history_count - 1})."

    return {"role": "user", "content": prompt}


# =============================================================================
# Parsing (from rlmpaper/rlm/utils/parsing.py)
# =============================================================================

def find_code_blocks(text: str) -> list[str]:
    """Find REPL code blocks in text wrapped in triple backticks.

    Args:
        text: The text to search for code blocks

    Returns:
        List of code block content strings (without the backticks)
    """
    pattern = r"```repl\s*\n(.*?)\n```"
    results = []

    for match in re.finditer(pattern, text, re.DOTALL):
        code_content = match.group(1).strip()
        results.append(code_content)

    return results


def find_final_answer(text: str, ns: dict = None) -> str | None:
    """Find FINAL(...) or FINAL_VAR(...) statement in response.

    Args:
        text: The response text to parse
        ns: Namespace dict for FINAL_VAR variable lookup

    Returns:
        The final answer string, or None if no final answer pattern is found
    """
    # Check for FINAL_VAR pattern first - must be at start of line
    final_var_pattern = r"^\s*FINAL_VAR\((.*?)\)"
    match = re.search(final_var_pattern, text, re.MULTILINE | re.DOTALL)
    if match:
        variable_name = match.group(1).strip().strip('"').strip("'")
        if ns is not None and variable_name in ns:
            return str(ns[variable_name])
        return None

    # Check for FINAL pattern - must be at start of line
    final_pattern = r"^\s*FINAL\((.*?)\)"
    match = re.search(final_pattern, text, re.MULTILINE | re.DOTALL)
    if match:
        return match.group(1).strip()

    return None


def format_execution_result(result: REPLResult) -> str:
    """Format execution result as a string for display.

    Args:
        result: The REPLResult object to format

    Returns:
        Formatted string representation
    """
    result_parts = []

    if result.stdout:
        result_parts.append(f"\n{result.stdout}")

    if result.stderr:
        result_parts.append(f"\n{result.stderr}")

    # Show variable names (excluding internal ones)
    important_vars = [
        key for key in result.locals.keys()
        if not key.startswith("_") and key not in ["__builtins__", "__name__", "__doc__"]
    ]

    if important_vars:
        result_parts.append(f"REPL variables: {important_vars}")

    return "\n\n".join(result_parts).rstrip() if result_parts else "No output"


def format_iteration(
    iteration: RLMIteration,
    max_character_length: int = 20000
) -> list[dict[str, str]]:
    """Format an RLM iteration for the next prompt's message history.

    Truncates execution results that exceed max_character_length.

    Args:
        iteration: The iteration to format
        max_character_length: Maximum characters for execution results

    Returns:
        List of message dicts to append to chat history
    """
    messages = [{"role": "assistant", "content": iteration.response}]

    for code_block in iteration.code_blocks:
        code = code_block.code
        result = format_execution_result(code_block.result) if code_block.result else "No output"

        if len(result) > max_character_length:
            result = (
                result[:max_character_length]
                + f"... + [{len(result) - max_character_length} chars...]"
            )

        execution_message = {
            "role": "user",
            "content": f"Code executed:\n```python\n{code}\n```\n\nREPL output:\n{result}".rstrip(),
        }
        messages.append(execution_message)

    return messages
