"""Core functionality for RLM using dialoghelper inspecttools"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['symget', 'bind_ns', 'llm_query', 'run_coro', 'llm_query_batched', 'QueryMetadata', 'find_final', 'rlm_system_prompt',
           'get_chunk', 'rlm_cont', 'named_partial', 'make_chunk_tool', 'make_llm_tool', 'REPLResult', 'CodeBlock',
           'RLMIteration', 'find_code_blocks', 'exec_code', 'format_result', 'format_iteration', 'rlm_run', 'rlm_step',
           'rlm_inspect', 'rlm_history', 'rlm_context_summary', 'rlm_tool_info']

# %% ../nbs/00_core.ipynb 4
from dialoghelper.inspecttools import *
from dialoghelper.core import add_msg
from fastcore import *
from claudette import Chat, contents
from functools import partial, update_wrapper
import asyncio
from dataclasses import dataclass
from mistletoe import Document
from mistletoe.block_token import CodeFence
import sys
from io import StringIO
import time
import re
from functools import partial

# %% ../nbs/00_core.ipynb 6
def symget(sym: str, key: str):
    "Get a key from a dict symbol and set `_last`"
    global _last
    from dialoghelper.inspecttools import resolve
    obj = resolve(sym)
    _last = obj[key]
    return _last


# %% ../nbs/00_core.ipynb 9
def bind_ns(func, ns):
    "Bind function to use specific namespace for REPL storage"
    return update_wrapper(partial(func, ns=ns), func)

# %% ../nbs/00_core.ipynb 11
def llm_query(prompt: str, model: str = 'claude-sonnet-4-5', name: str = 'llm_res', ns: dict = None) -> str:
    "Query an LLM with a prompt, return response text"
    if ns is None: ns = globals()
    result = contents(Chat(model)(prompt))
    ns[name] = result
    return f"Stored response into '{name}' ({len(result)} chars)"

# %% ../nbs/00_core.ipynb 13
async def _query_one(prompt: str, model: str, **kwargs) -> str: return contents(Chat(model, **kwargs)(prompt))

# %% ../nbs/00_core.ipynb 14
def run_coro(coro):
    "Run coroutine in both sync and async contexts"
    try: loop = asyncio.get_running_loop()
    except RuntimeError: return asyncio.run(coro)
    import nest_asyncio; nest_asyncio.apply()
    return loop.run_until_complete(coro)

# %% ../nbs/00_core.ipynb 15
def llm_query_batched(prompts: list, model: str = 'claude-sonnet-4-5', name: str = 'batch_res', ns: dict = None) -> str:
    "Query LLM with multiple prompts concurrently, store results in REPL"
    if ns is None: ns = globals()
    results = run_coro(asyncio.gather(*[_query_one(p, model) for p in prompts]))
    ns[name] = list(results)
    return f"Stored {len(results)} responses into '{name}'"

# %% ../nbs/00_core.ipynb 20
@dataclass
class QueryMetadata:
    "Metadata about context for RLM prompts"
    context: str|list|dict; context_type: str = 'document'
    
    @property
    def chunks(self) -> list:
        "Return context as list of chunks"
        if isinstance(self.context, dict): return [f"{k}: {v}" for k, v in self.context.items()]
        if isinstance(self.context, list):
            if self.context and isinstance(self.context[0], dict): return [str(d) for d in self.context]
            return self.context
        return [self.context]
    
    @property
    def keys(self) -> list|None:
        "Return keys if dict context"
        if isinstance(self.context, dict): return list(self.context.keys())
        if isinstance(self.context, list) and self.context and isinstance(self.context[0], dict): return list(self.context[0].keys())
        return None
    
    @property
    def total_chars(self) -> int: return sum(len(c) for c in self.chunks)
    
    @property
    def chunk_lengths(self) -> list[int]: return [len(c) for c in self.chunks]
    
    def summary(self) -> str:
        "Format metadata string for LLM prompt"
        base = f"Your context is a {self.context_type} with {self.total_chars} total characters, broken into {len(self.chunks)} chunks of char lengths: {self.chunk_lengths}."
        if self.keys: base += f" Keys/fields: {self.keys}"
        return base


# %% ../nbs/00_core.ipynb 26
def find_final(text: str, ns: dict = None) -> tuple:
    "Extract FINAL(answer) or FINAL_VAR(var_name) from text. Returns (value, is_final)"
    if ns is None: ns = globals()
    if m := re.search(r'FINAL_VAR\((\w+)\)', text):
        var_name = m.group(1)
        if var_name in ns: return ns[var_name], True
        return None, False
    if m := re.search(r'FINAL\((.+?)\)', text, re.DOTALL):
        val = m.group(1).strip()
        if val in ns: return ns[val], True
        return val, True
    return None, False

# %% ../nbs/00_core.ipynb 31
def rlm_system_prompt(meta: QueryMetadata, tools: list = None) -> str:
    "Build RLM system prompt with context metadata and available tools"
    tool_names = ', '.join(t.__name__ for t in tools) if tools else ''
    return f"""You are answering a query using context in a REPL environment. You will be queried iteratively until you provide a final answer.

{meta.summary()}

The REPL environment has:
1. `context` â€” your chunks as a list
2. `llm_query(prompt)` â€” query a sub-LLM (can handle ~500K chars)
3. `llm_query_batched(prompts)` â€” concurrent queries, much faster for multiple independent calls

Available tools: {tool_names}

IMPORTANT: Your answer MUST be grounded in the actual context. Do NOT use prior knowledge. Always verify by reading relevant chunks.

Chunking strategies â€” analyze context structure first, then choose:

Size-based (when context is a string):
```repl
chunk = context[:10000]
answer = llm_query(f"Find X in: {{chunk}}")
```

Pre-chunked (when context is List[str]):
```repl
prompts = [f"Find X in: {{c}}" for c in context]
answers = llm_query_batched(prompts)
```

Structure-based (split by headers/sections):
```repl
import re
sections = re.split(r'### (.+)', context)
```

Buffer accumulation:
```repl
buffer = []
for i, chunk in enumerate(context):
    result = llm_query(f"Extract relevant info from: {{chunk}}")
    buffer.append(result)
final = llm_query(f"Synthesize findings: {{buffer}}")
```

When done:
- Use `FINAL("your answer here")` with the actual answer text
- Use `FINAL_VAR(varname)` to return the value of a variable
- Put FINAL/FINAL_VAR on its own line, NOT inside a code block

NEVER guess. If the answer isn't in the context, say so."""

# %% ../nbs/00_core.ipynb 34
def get_chunk(i: int, meta: QueryMetadata) -> str:
    "Get chunk at index i from context"
    return meta.chunks[i]

# %% ../nbs/00_core.ipynb 37
def rlm_cont(*responses, ns=None):
    "Continue function for toolloop â€” returns False when FINAL found"
    _, is_final = find_final(contents(responses[-1]), ns)
    return not is_final

# %% ../nbs/00_core.ipynb 38
def named_partial(func, **kwargs):
    "Partial that preserves __name__ for tool introspection"
    return update_wrapper(partial(func, **kwargs), func)

# %% ../nbs/00_core.ipynb 40
def make_chunk_tool(meta: QueryMetadata):
    "Create a get_chunk tool bound to specific metadata"
    def get_chunk(i: int) -> str:
        "Get chunk at index i from context"
        return meta.chunks[i]
    return get_chunk

# %% ../nbs/00_core.ipynb 42
def make_llm_tool(ns: dict = None, model: str = 'claude-sonnet-4-5'):
    "Create an llm_query tool bound to namespace"
    if ns is None: ns = globals()
    def llm_query(prompt: str) -> str:
        "Query a sub-LLM with a prompt"
        result = contents(Chat(model)(prompt))
        ns['llm_res'] = result
        return result
    return llm_query

# %% ../nbs/00_core.ipynb 50
@dataclass
class REPLResult:
    "Result from executing code in REPL"
    stdout: str = ''; stderr: str = ''; return_val: str = ''; error: str = ''
    execution_time: float = 0.0
    
    def to_dict(self): return {k: v for k, v in self.__dict__.items() if v}

@dataclass  
class CodeBlock:
    "Parsed code block with execution result"
    code: str; lang: str = 'python'; result: REPLResult = None
    
    def to_dict(self): return {'code': self.code, 'lang': self.lang, 'result': self.result.to_dict() if self.result else None}

@dataclass
class RLMIteration:
    "Record of a single RLM iteration"
    prompt: str; response: str; code_blocks: list[CodeBlock] = None
    final_answer: str = None; iteration_time: float = None
    
    def to_dict(self):
        return {'prompt': self.prompt, 'response': self.response,
                'code_blocks': [cb.to_dict() for cb in (self.code_blocks or [])],
                'final_answer': self.final_answer, 'iteration_time': self.iteration_time}

# %% ../nbs/00_core.ipynb 54
def find_code_blocks(text: str, lang: str = 'repl') -> list[CodeBlock]:
    "Extract code blocks with specified language from markdown text"
    doc = Document(text)
    return [CodeBlock(code=t.content.strip(), lang=t.language) 
            for t in doc.children if isinstance(t, CodeFence) and t.language == lang]

# %% ../nbs/00_core.ipynb 57
def exec_code(block: CodeBlock, ns: dict = None) -> CodeBlock:
    "Execute code block in namespace, return block with result attached"
    if ns is None: ns = globals()
    stdout, stderr = StringIO(), StringIO()
    old_stdout, old_stderr = sys.stdout, sys.stderr
    start = time.time()
    try:
        sys.stdout, sys.stderr = stdout, stderr
        exec(compile(block.code, '<repl>', 'exec'), ns)
        result = REPLResult(stdout=stdout.getvalue(), stderr=stderr.getvalue(), 
                           execution_time=time.time()-start)
    except Exception as e:
        result = REPLResult(stdout=stdout.getvalue(), stderr=stderr.getvalue(),
                           error=f"{type(e).__name__}: {e}", execution_time=time.time()-start)
    finally:
        sys.stdout, sys.stderr = old_stdout, old_stderr
    return CodeBlock(code=block.code, lang=block.lang, result=result)

# %% ../nbs/00_core.ipynb 59
def format_result(r: REPLResult, max_len: int = 4000) -> str:
    "Format REPLResult as string, truncating if needed"
    parts = []
    if r.stdout: parts.append(r.stdout.rstrip())
    if r.stderr: parts.append(f"stderr: {r.stderr.rstrip()}")
    if r.error: parts.append(f"Error: {r.error.rstrip()}")
    result = '\n'.join(parts) if parts else '(no output)'
    if len(result) > max_len: result = result[:max_len] + f"... + [{len(result)-max_len} chars...]"
    return result


# %% ../nbs/00_core.ipynb 60
def format_iteration(iteration: RLMIteration, max_len: int = 4000) -> list[dict]:
    "Format iteration as chat messages for next prompt"
    msgs = [{"role": "assistant", "content": iteration.response.rstrip()}]
    for cb in (iteration.code_blocks or []):
        content = f"Code executed:\n```python\n{cb.code.rstrip()}\n```\n\nREPL output:\n{format_result(cb.result, max_len)}"
        msgs.append({"role": "user", "content": content.rstrip()})
    return msgs

# %% ../nbs/00_core.ipynb 63
def rlm_run(query: str, meta: QueryMetadata, model: str = 'claude-sonnet-4-5', max_iters: int = 10, ns: dict = None) -> tuple[str, list[RLMIteration]]:
    "Run RLM loop until FINAL or max iterations"
    if ns is None: ns = {}
    ns['context'] = meta.chunks
    chunk_tool, llm_tool = make_chunk_tool(meta), make_llm_tool(ns, model)
    tools = [chunk_tool, llm_tool]
    chat = Chat(model=model, sp=rlm_system_prompt(meta, tools))
    iterations = []
    prompt = query
    for i in range(max_iters):
        start = time.time()
        response = contents(chat(prompt))
        blocks = [exec_code(b, ns) for b in find_code_blocks(response)]
        answer, is_final = find_final(response, ns)
        iteration = RLMIteration(prompt=prompt, response=response, code_blocks=blocks, 
                                 final_answer=answer if is_final else None, iteration_time=time.time()-start)
        iterations.append(iteration)
        if is_final: return answer, iterations
        for msg in format_iteration(iteration): chat.h.append(msg)
        prompt = "Continue."
    return None, iterations


# %% ../nbs/00_core.ipynb 68
def rlm_step(query: str, chat: Chat = None, meta: QueryMetadata = None, model: str = 'claude-sonnet-4-5', ns: dict = None) -> tuple:
    "Execute single RLM iteration. Returns (answer, is_final, chat, iteration)"
    if ns is None: ns = globals()
    if chat is None:
        ns['context'] = meta.chunks
        tools = [make_chunk_tool(meta), make_llm_tool(ns, model)]
        chat = Chat(model=model, sp=rlm_system_prompt(meta, tools))
    response = contents(chat(query))
    answer, is_final = find_final(response, ns)
    blocks = [exec_code(b, ns) for b in find_code_blocks(response)]
    iteration = RLMIteration(prompt=query, response=response, code_blocks=blocks, final_answer=answer if is_final else None)
    if not is_final:
        for msg in format_iteration(iteration): chat.h.append(msg)
    return answer, is_final, chat, iteration

# %% ../nbs/00_core.ipynb 88
def rlm_inspect(sym: str, ns: dict = None) -> str:
    "Inspect a variable in the RLM namespace"
    if ns is None: ns = globals()
    if sym not in ns: return f"'{sym}' not found in namespace"
    val = ns[sym]
    return f"{sym}: {type(val).__name__} = {repr(val)[:500]}"

# %% ../nbs/00_core.ipynb 89
def rlm_history(iterations: list[RLMIteration]) -> str:
    "Format iteration history for display"
    if not iterations: return "No iterations yet."
    lines = [f"**RLM History: {len(iterations)} iterations**\n"]
    for i, it in enumerate(iterations):
        status = "âœ… FINAL" if it.final_answer else "ðŸ”„ Continue"
        code_count = len(it.code_blocks) if it.code_blocks else 0
        lines.append(f"**{i+1}.** {status} | {code_count} code blocks | {it.iteration_time:.2f}s" if it.iteration_time else f"**{i+1}.** {status} | {code_count} code blocks")
        lines.append(f"   Response: {it.response[:100]}...")
    return '\n'.join(lines)

# %% ../nbs/00_core.ipynb 90
def rlm_context_summary(meta: QueryMetadata) -> str:
    "Display context metadata summary"
    lines = [f"**Context Summary**", meta.summary(), f"Chunks: {len(meta.chunks)}"]
    for i, c in enumerate(meta.chunks): lines.append(f"  [{i}] ({len(c)} chars): {c[:60]}...")
    return '\n'.join(lines)

# %% ../nbs/00_core.ipynb 97
def rlm_tool_info():
    add_msg('RLM tools available: &`[rlm_run,rlm_inspect,rlm_history,rlm_context_summary,llm_query,llm_query_batched,find_final,find_code_blocks,exec_code]`')

