"""RLM core implementation using claudette with namespace-explicit design"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['in_solveit', 'llm_query', 'llm_query_batched', 'exec_code', 'rlm_run']

# %% ../nbs/00_core.ipynb 4
from claudette import Chat, contents
from functools import partial
import asyncio
import sys
import time
from io import StringIO

# Protocol artifacts from rlmpaper
from rlm._rlmpaper_compat import (
    RLM_SYSTEM_PROMPT,
    QueryMetadata,
    REPLResult, CodeBlock, RLMIteration,
    build_rlm_system_prompt, build_user_prompt,
    find_code_blocks, find_final_answer, format_iteration,
)

# %% ../nbs/00_core.ipynb 6
def in_solveit() -> bool:
    """Check if running in Solveit environment.
    
    Solveit injects `__msg_id` into the call stack. This function tests for that.
    """
    try:
        from dialoghelper.inspecttools import _find_frame_dict
        _find_frame_dict('__msg_id')
        return True
    except (ValueError, ImportError):
        return False

# %% ../nbs/00_core.ipynb 9
def llm_query(prompt: str, ns: dict, name: str = 'llm_res', 
              model: str = 'claude-sonnet-4-5') -> str:
    """Query a sub-LLM and store the result in namespace.
    
    Args:
        prompt: The prompt to send to the LLM
        ns: Namespace dict where result will be stored
        name: Variable name for storing the result
        model: Claude model to use
        
    Returns:
        The LLM's response (also stored in ns[name])
    """
    result = contents(Chat(model)(prompt))
    ns[name] = result
    return result  # Return actual result, not summary

# %% ../nbs/00_core.ipynb 11
async def _query_one(prompt: str, model: str) -> str:
    """Execute a single LLM query asynchronously."""
    return contents(Chat(model)(prompt))

# %% ../nbs/00_core.ipynb 12
def llm_query_batched(prompts: list, ns: dict, name: str = 'batch_res',
                      model: str = 'claude-sonnet-4-5') -> list:
    """Query LLM with multiple prompts concurrently.
    
    NOTE: Currently executes sequentially due to claudette's synchronous API.
    True concurrency pending claudette async API support. The async scaffolding
    is in place for when that becomes available.
    
    Args:
        prompts: List of prompt strings
        ns: Namespace dict where results will be stored
        name: Variable name for storing the list of results
        model: Claude model to use
        
    Returns:
        List of LLM responses (also stored in ns[name])
    """
    try:
        loop = asyncio.get_running_loop()
        try:
            import nest_asyncio
            nest_asyncio.apply()
        except ImportError:
            pass  # Optional dependency for nested event loops
        results = loop.run_until_complete(
            asyncio.gather(*[_query_one(p, model) for p in prompts])
        )
    except RuntimeError:
        results = asyncio.run(
            asyncio.gather(*[_query_one(p, model) for p in prompts])
        )
    
    results = list(results)
    ns[name] = results
    return results  # Return actual results, not summary

# %% ../nbs/00_core.ipynb 15
def exec_code(code: str, ns: dict) -> REPLResult:
    """Execute code in namespace and return result.
    
    Captures stdout, stderr, and any exceptions. The namespace is mutated
    with any variables created during execution.
    
    Args:
        code: Python code to execute
        ns: Namespace dict for execution
        
    Returns:
        REPLResult with stdout, stderr, locals snapshot, execution_time
    """
    stdout_capture = StringIO()
    stderr_capture = StringIO()
    old_stdout, old_stderr = sys.stdout, sys.stderr
    start = time.time()
    
    try:
        sys.stdout, sys.stderr = stdout_capture, stderr_capture
        exec(compile(code, '<repl>', 'exec'), ns)
        stderr_out = stderr_capture.getvalue()
    except Exception as e:
        stderr_out = stderr_capture.getvalue() + f"\n{type(e).__name__}: {e}"
    finally:
        sys.stdout, sys.stderr = old_stdout, old_stderr
    
    # FIX: Create a snapshot copy of namespace instead of storing reference
    # This prevents later modifications from affecting inspection of old iterations
    return REPLResult(
        stdout=stdout_capture.getvalue(),
        stderr=stderr_out,
        locals=dict(ns),  # Snapshot, not reference
        execution_time=time.time() - start
    )

# %% ../nbs/00_core.ipynb 18
def _synthesize_fallback_answer(iterations: list, ns: dict) -> str:
    """Synthesize best-effort answer when max_iters reached.
    
    Tries to extract useful information from:
    1. Common result variable names
    2. Last REPL output
    
    Args:
        iterations: List of RLMIteration objects
        ns: Namespace dict with variables
        
    Returns:
        Fallback answer string with [Max iterations] prefix
    """
    # Check for common result variable names
    for var in ['result', 'answer', 'final', 'output', 'solution']:
        if var in ns and ns[var] is not None:
            value_str = str(ns[var])
            # Truncate if too long
            if len(value_str) > 500:
                value_str = value_str[:500] + "..."
            return f"[Max iterations] Partial result: {value_str}"
    
    # Get last REPL output with actual content
    for iteration in reversed(iterations):
        for cb in iteration.code_blocks:
            if cb.result and cb.result.stdout.strip():
                stdout = cb.result.stdout.strip()
                # Truncate if too long
                if len(stdout) > 500:
                    stdout = stdout[:500] + "..."
                return f"[Max iterations] Last output: {stdout}"
    
    return "[Max iterations] No answer produced"


def rlm_run(query: str, context, ns: dict = None,
            model: str = 'claude-sonnet-4-5',
            max_iters: int = 10,
            logger=None,
            verbose: bool = False) -> tuple:
    """Run RLM loop until FINAL or max iterations.

    This implements the RLM protocol: the root LLM emits ```repl``` code blocks
    which are executed in a namespace with `context`, `llm_query`,
    `llm_query_batched`, and `FINAL_VAR` available. The loop continues until
    the model returns FINAL(...) or FINAL_VAR(...).

    Args:
        query: User's question to answer
        context: Context data (str, list of str, or dict)
        ns: Namespace dict (if None, creates fresh namespace)
        model: Claude model to use
        max_iters: Maximum iterations before giving up
        logger: RLMLogger instance for JSON-lines logging (optional)
        verbose: Enable Rich console output (default: False)

    Returns:
        (answer, iterations, namespace) tuple where:
        - answer: Final answer string (or fallback if max_iters reached)
        - iterations: List of RLMIteration objects
        - namespace: The dict containing all REPL variables
    """
    if ns is None:
        ns = {}

    # Initialize verbose printer
    from rlm.logger import VerbosePrinter
    verbose_printer = VerbosePrinter(enabled=verbose)

    # Define FINAL_VAR helper (following rlmpaper design)
    def _final_var(variable_name: str) -> str:
        """Return the value of a variable as a final answer.

        This is an executable function that the model can call inside code blocks
        to test if a variable exists and preview its value before committing to
        using FINAL_VAR outside code blocks.

        Args:
            variable_name: Name of the variable to return

        Returns:
            The variable's value as a string, or an error message if not found
        """
        variable_name = variable_name.strip().strip('"').strip("'")
        if variable_name in ns:
            return str(ns[variable_name])
        return f"Error: Variable '{variable_name}' not found in namespace"

    # Setup namespace with REPL environment
    meta = QueryMetadata(context)
    ns['context'] = context

    # Bind llm_query functions to this namespace and model
    ns['llm_query'] = partial(llm_query, ns=ns, model=model)
    ns['llm_query_batched'] = partial(llm_query_batched, ns=ns, model=model)

    # Add FINAL_VAR as executable function (following rlmpaper design)
    ns['FINAL_VAR'] = _final_var

    # Log metadata
    if logger:
        logger.log_metadata({
            'query': query,
            'context_type': meta.context_type,
            'context_length': meta.context_total_length,
            'model': model,
            'max_iters': max_iters
        })

    # Print header
    verbose_printer.print_header(query=query, context=str(context)[:100], max_iters=max_iters)

    # Build initial messages with rlmpaper system prompt
    messages = build_rlm_system_prompt(query_metadata=meta)
    chat = Chat(model, sp=messages[0]['content'])

    # Add metadata message if present
    if len(messages) > 1:
        chat.h.append(messages[1])

    iterations = []
    start_time = time.time()

    for i in range(max_iters):
        iter_start = time.time()

        # Build user prompt (includes first-iteration safeguard)
        user_msg = build_user_prompt(root_prompt=query, iteration=i)

        # Get response from root LLM
        response = contents(chat(user_msg['content']))

        # Extract and execute code blocks
        code_strs = find_code_blocks(response)
        code_blocks = []
        for code in code_strs:
            result = exec_code(code, ns)
            code_blocks.append(CodeBlock(code=code, result=result))

        # Check for final answer
        answer = find_final_answer(response, ns=ns)

        # Record iteration
        iteration = RLMIteration(
            prompt=user_msg['content'],
            response=response,
            code_blocks=code_blocks,
            final_answer=answer,
            iteration_time=time.time() - iter_start
        )
        iterations.append(iteration)

        # Log iteration
        if logger:
            logger.log(iteration, i + 1)
        verbose_printer.print_iteration(iteration, i + 1)

        # If we have an answer, we're done
        if answer is not None:
            verbose_printer.print_final_answer(answer)
            verbose_printer.print_summary(
                total_iterations=len(iterations),
                total_time=time.time() - start_time
            )
            return answer, iterations, ns

        # Add iteration to chat history for next round
        for msg in format_iteration(iteration):
            chat.h.append(msg)

    # Reached max iterations - synthesize fallback answer
    fallback = _synthesize_fallback_answer(iterations, ns)
    verbose_printer.print_summary(
        total_iterations=len(iterations),
        total_time=time.time() - start_time
    )
    return fallback, iterations, ns
