{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> RLM core implementation using claudette with namespace-explicit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements the RLM (Recursive Language Model) protocol from [rlmpaper](https://github.com/alexzhang13/rlm) using [claudette](https://claudette.answer.ai) as the LLM backend.\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Namespace-explicit**: All functions take `ns: dict` parameter (no frame walking)\n",
    "- **Protocol-faithful**: Uses prompts and types from `_rlmpaper_compat.py`\n",
    "- **Solveit-independent**: Core works anywhere; Solveit integration is separate\n",
    "- **Return everything useful**: Functions return `(answer, iterations, ns)` for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from claudette import Chat, contents\n",
    "from functools import partial\n",
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "# Protocol artifacts from rlmpaper\n",
    "from rlm._rlmpaper_compat import (\n",
    "    RLM_SYSTEM_PROMPT,\n",
    "    QueryMetadata,\n",
    "    REPLResult, CodeBlock, RLMIteration,\n",
    "    build_rlm_system_prompt, build_user_prompt,\n",
    "    find_code_blocks, find_final_answer, format_iteration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def in_solveit() -> bool:\n",
    "    \"\"\"Check if running in Solveit environment.\n",
    "    \n",
    "    Solveit injects `__msg_id` into the call stack. This function tests for that.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from dialoghelper.inspecttools import _find_frame_dict\n",
    "        _find_frame_dict('__msg_id')\n",
    "        return True\n",
    "    except (ValueError, ImportError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment detection\n",
    "is_solveit = in_solveit()\n",
    "print(f\"Running in Solveit: {is_solveit}\")\n",
    "assert isinstance(is_solveit, bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core LLM Functions\n",
    "\n",
    "These functions provide the REPL environment's `llm_query` and `llm_query_batched` capabilities.\n",
    "They require an explicit namespace dict and store results there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llm_query(prompt: str, ns: dict, name: str = 'llm_res', \n",
    "              model: str = 'claude-sonnet-4-5') -> str:\n",
    "    \"\"\"Query a sub-LLM and store the result in namespace.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        ns: Namespace dict where result will be stored\n",
    "        name: Variable name for storing the result\n",
    "        model: Claude model to use\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing what was stored\n",
    "    \"\"\"\n",
    "    result = contents(Chat(model)(prompt))\n",
    "    ns[name] = result\n",
    "    return f\"Stored response in '{name}' ({len(result)} chars)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test llm_query with explicit namespace\n",
    "test_ns = {}\n",
    "# Note: Commented out to avoid API calls during CI\n",
    "# result = llm_query(\"Say 'hello' and nothing else\", test_ns, name='greeting')\n",
    "# assert 'greeting' in test_ns\n",
    "# assert len(test_ns['greeting']) > 0\n",
    "print(\"✓ llm_query signature test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _query_one(prompt: str, model: str) -> str:\n",
    "    \"\"\"Execute a single LLM query asynchronously.\"\"\"\n",
    "    return contents(Chat(model)(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llm_query_batched(prompts: list, ns: dict, name: str = 'batch_res',\n",
    "                      model: str = 'claude-sonnet-4-5') -> str:\n",
    "    \"\"\"Query LLM with multiple prompts concurrently.\n",
    "    \n",
    "    Much faster than sequential `llm_query` calls when you have multiple\n",
    "    independent queries. Results are returned in the same order as prompts.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt strings\n",
    "        ns: Namespace dict where results will be stored\n",
    "        name: Variable name for storing the list of results\n",
    "        model: Claude model to use\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing what was stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        results = loop.run_until_complete(\n",
    "            asyncio.gather(*[_query_one(p, model) for p in prompts])\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        results = asyncio.run(\n",
    "            asyncio.gather(*[_query_one(p, model) for p in prompts])\n",
    "        )\n",
    "    \n",
    "    ns[name] = list(results)\n",
    "    return f\"Stored {len(results)} responses in '{name}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test llm_query_batched signature\n",
    "test_ns = {}\n",
    "# Note: Commented out to avoid API calls during CI\n",
    "# prompts = [\"Say 'one'\", \"Say 'two'\"]\n",
    "# result = llm_query_batched(prompts, test_ns, name='batch')\n",
    "# assert 'batch' in test_ns\n",
    "# assert len(test_ns['batch']) == 2\n",
    "print(\"✓ llm_query_batched signature test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPL Execution\n",
    "\n",
    "Execute Python code in a namespace and capture stdout/stderr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exec_code(code: str, ns: dict) -> REPLResult:\n",
    "    \"\"\"Execute code in namespace and return result.\n",
    "    \n",
    "    Captures stdout, stderr, and any exceptions. The namespace is mutated\n",
    "    with any variables created during execution.\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to execute\n",
    "        ns: Namespace dict for execution\n",
    "        \n",
    "    Returns:\n",
    "        REPLResult with stdout, stderr, locals, execution_time\n",
    "    \"\"\"\n",
    "    stdout_capture = StringIO()\n",
    "    stderr_capture = StringIO()\n",
    "    old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        sys.stdout, sys.stderr = stdout_capture, stderr_capture\n",
    "        exec(compile(code, '<repl>', 'exec'), ns)\n",
    "        stderr_out = stderr_capture.getvalue()\n",
    "    except Exception as e:\n",
    "        stderr_out = stderr_capture.getvalue() + f\"\\n{type(e).__name__}: {e}\"\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "    \n",
    "    return REPLResult(\n",
    "        stdout=stdout_capture.getvalue(),\n",
    "        stderr=stderr_out,\n",
    "        locals=ns,\n",
    "        execution_time=time.time() - start\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test exec_code with explicit namespace\n",
    "test_ns = {}\n",
    "result = exec_code(\"x = 2 + 2\\nprint(x)\", test_ns)\n",
    "assert test_ns['x'] == 4\n",
    "assert '4' in result.stdout\n",
    "assert result.execution_time > 0\n",
    "print(\"✓ exec_code works\")\n",
    "\n",
    "# Test error handling\n",
    "test_ns = {}\n",
    "result = exec_code(\"raise ValueError('test error')\", test_ns)\n",
    "assert 'ValueError: test error' in result.stderr\n",
    "print(\"✓ exec_code error handling works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLM Loop\n",
    "\n",
    "The main RLM iteration loop. Follows the rlmpaper protocol:\n",
    "1. Build system prompt with metadata\n",
    "2. Add first-iteration safeguard\n",
    "3. Execute REPL code blocks\n",
    "4. Check for FINAL/FINAL_VAR\n",
    "5. Repeat until answer or max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rlm_run(query: str, context, ns: dict = None, \n",
    "            model: str = 'claude-sonnet-4-5', \n",
    "            max_iters: int = 10) -> tuple:\n",
    "    \"\"\"Run RLM loop until FINAL or max iterations.\n",
    "    \n",
    "    This implements the RLM protocol: the root LLM emits ```repl``` code blocks\n",
    "    which are executed in a namespace with `context`, `llm_query`, and \n",
    "    `llm_query_batched` available. The loop continues until the model returns\n",
    "    FINAL(...) or FINAL_VAR(...).\n",
    "    \n",
    "    Args:\n",
    "        query: User's question to answer\n",
    "        context: Context data (str, list of str, or dict)\n",
    "        ns: Namespace dict (if None, creates fresh namespace)\n",
    "        model: Claude model to use\n",
    "        max_iters: Maximum iterations before giving up\n",
    "        \n",
    "    Returns:\n",
    "        (answer, iterations, namespace) tuple where:\n",
    "        - answer: Final answer string or None if didn't converge\n",
    "        - iterations: List of RLMIteration objects\n",
    "        - namespace: The dict containing all REPL variables\n",
    "    \"\"\"\n",
    "    if ns is None:\n",
    "        ns = {}\n",
    "    \n",
    "    # Setup namespace with REPL environment\n",
    "    meta = QueryMetadata(context)\n",
    "    ns['context'] = context\n",
    "    \n",
    "    # Bind llm_query functions to this namespace and model\n",
    "    ns['llm_query'] = partial(llm_query, ns=ns, model=model)\n",
    "    ns['llm_query_batched'] = partial(llm_query_batched, ns=ns, model=model)\n",
    "    \n",
    "    # Build initial messages with rlmpaper system prompt\n",
    "    messages = build_rlm_system_prompt(query_metadata=meta)\n",
    "    chat = Chat(model, sp=messages[0]['content'])\n",
    "    \n",
    "    # Add metadata message if present\n",
    "    if len(messages) > 1:\n",
    "        chat.h.append(messages[1])\n",
    "    \n",
    "    iterations = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        start = time.time()\n",
    "        \n",
    "        # Build user prompt (includes first-iteration safeguard)\n",
    "        user_msg = build_user_prompt(root_prompt=query, iteration=i)\n",
    "        \n",
    "        # Get response from root LLM\n",
    "        response = contents(chat(user_msg['content']))\n",
    "        \n",
    "        # Extract and execute code blocks\n",
    "        code_strs = find_code_blocks(response)\n",
    "        code_blocks = []\n",
    "        for code in code_strs:\n",
    "            result = exec_code(code, ns)\n",
    "            code_blocks.append(CodeBlock(code=code, result=result))\n",
    "        \n",
    "        # Check for final answer\n",
    "        answer = find_final_answer(response, ns=ns)\n",
    "        \n",
    "        # Record iteration\n",
    "        iteration = RLMIteration(\n",
    "            prompt=user_msg['content'],\n",
    "            response=response,\n",
    "            code_blocks=code_blocks,\n",
    "            final_answer=answer,\n",
    "            iteration_time=time.time() - start\n",
    "        )\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "        # If we have an answer, we're done\n",
    "        if answer is not None:\n",
    "            return answer, iterations, ns\n",
    "        \n",
    "        # Add iteration to chat history for next round\n",
    "        for msg in format_iteration(iteration):\n",
    "            chat.h.append(msg)\n",
    "    \n",
    "    # Reached max iterations without final answer\n",
    "    return None, iterations, ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test QueryMetadata (from _rlmpaper_compat)\n",
    "meta = QueryMetadata([\"chunk1\", \"chunk2\", \"chunk3\"])\n",
    "assert meta.context_type == \"list\"\n",
    "assert len(meta.context_lengths) == 3\n",
    "assert meta.context_total_length == sum(len(c) for c in [\"chunk1\", \"chunk2\", \"chunk3\"])\n",
    "print(\"✓ QueryMetadata works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test find_code_blocks (from _rlmpaper_compat)\n",
    "text = \"\"\"Here's some code:\n",
    "```repl\n",
    "x = 1 + 1\n",
    "print(x)\n",
    "```\n",
    "And more text.\"\"\"\n",
    "blocks = find_code_blocks(text)\n",
    "assert len(blocks) == 1\n",
    "assert 'x = 1 + 1' in blocks[0]\n",
    "print(\"✓ find_code_blocks works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test find_final_answer (from _rlmpaper_compat)\n",
    "assert find_final_answer(\"FINAL(42)\") == \"42\"\n",
    "assert find_final_answer(\"FINAL(The answer is 42)\") == \"The answer is 42\"\n",
    "\n",
    "# Test FINAL_VAR\n",
    "test_ns = {'result': 'hello world'}\n",
    "assert find_final_answer(\"FINAL_VAR(result)\", ns=test_ns) == \"hello world\"\n",
    "\n",
    "# Test no final\n",
    "assert find_final_answer(\"Just some text\") is None\n",
    "print(\"✓ find_final_answer works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rlm_run with simple mock scenario\n",
    "# Note: This doesn't call LLM APIs, just tests the structure\n",
    "context = [\"The capital of France is Paris.\"]\n",
    "test_ns = {}\n",
    "\n",
    "# We can't easily test without API calls, but we can verify the function signature\n",
    "# and that it sets up the namespace correctly\n",
    "meta = QueryMetadata(context)\n",
    "test_ns['context'] = context\n",
    "test_ns['llm_query'] = partial(llm_query, ns=test_ns, model='claude-sonnet-4-5')\n",
    "test_ns['llm_query_batched'] = partial(llm_query_batched, ns=test_ns, model='claude-sonnet-4-5')\n",
    "\n",
    "assert 'context' in test_ns\n",
    "assert 'llm_query' in test_ns\n",
    "assert 'llm_query_batched' in test_ns\n",
    "assert callable(test_ns['llm_query'])\n",
    "assert callable(test_ns['llm_query_batched'])\n",
    "print(\"✓ rlm_run namespace setup works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Simple usage (requires API key)\n",
    "context = [\"The speed of light is 299,792,458 m/s.\"]\n",
    "answer, iterations, ns = rlm_run(\"What is the speed of light?\", context)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Iterations: {len(iterations)}\")\n",
    "print(f\"Variables in namespace: {[k for k in ns.keys() if not k.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Persistent namespace across runs\n",
    "ns = {}\n",
    "answer1, iters1, ns = rlm_run(\"What is X?\", context1, ns=ns)\n",
    "answer2, iters2, ns = rlm_run(\"What about Y?\", context2, ns=ns)\n",
    "# ns now contains variables from both runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
