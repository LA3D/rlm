{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353bda72",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> RLM core implementation using claudette with namespace-explicit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d091534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e816d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements the RLM (Recursive Language Model) protocol from [rlmpaper](https://github.com/alexzhang13/rlm) using [claudette](https://claudette.answer.ai) as the LLM backend.\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Namespace-explicit**: All functions take `ns: dict` parameter (no frame walking)\n",
    "- **Protocol-faithful**: Uses prompts and types from `_rlmpaper_compat.py`\n",
    "- **Solveit-independent**: Core works anywhere; Solveit integration is separate\n",
    "- **Return everything useful**: Functions return `(answer, iterations, ns)` for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9159c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from claudette import Chat, contents\n",
    "from functools import partial\n",
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "# Protocol artifacts from rlmpaper\n",
    "from rlm._rlmpaper_compat import (\n",
    "    RLM_SYSTEM_PROMPT,\n",
    "    QueryMetadata,\n",
    "    REPLResult, CodeBlock, RLMIteration,\n",
    "    build_rlm_system_prompt, build_user_prompt,\n",
    "    find_code_blocks, find_final_answer, format_iteration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92993531",
   "metadata": {},
   "source": [
    "## Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def in_solveit() -> bool:\n",
    "    \"\"\"Check if running in Solveit environment.\n",
    "    \n",
    "    Solveit injects `__msg_id` into the call stack. This function tests for that.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from dialoghelper.inspecttools import _find_frame_dict\n",
    "        _find_frame_dict('__msg_id')\n",
    "        return True\n",
    "    except (ValueError, ImportError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Solveit: False\n"
     ]
    }
   ],
   "source": [
    "# Test environment detection\n",
    "is_solveit = in_solveit()\n",
    "print(f\"Running in Solveit: {is_solveit}\")\n",
    "assert isinstance(is_solveit, bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372d5ff",
   "metadata": {},
   "source": [
    "## Core LLM Functions\n",
    "\n",
    "These functions provide the REPL environment's `llm_query` and `llm_query_batched` capabilities.\n",
    "They require an explicit namespace dict and store results there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llm_query(prompt: str, ns: dict, name: str = 'llm_res', \n",
    "              model: str = 'claude-sonnet-4-5') -> str:\n",
    "    \"\"\"Query a sub-LLM and store the result in namespace.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        ns: Namespace dict where result will be stored\n",
    "        name: Variable name for storing the result\n",
    "        model: Claude model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM's response (also stored in ns[name])\n",
    "    \"\"\"\n",
    "    result = contents(Chat(model)(prompt))\n",
    "    ns[name] = result\n",
    "    return result  # Return actual result, not summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c87e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ llm_query signature test passed\n"
     ]
    }
   ],
   "source": [
    "# Test llm_query with explicit namespace\n",
    "test_ns = {}\n",
    "# Note: Commented out to avoid API calls during CI\n",
    "# result = llm_query(\"Say 'hello' and nothing else\", test_ns, name='greeting')\n",
    "# assert 'greeting' in test_ns\n",
    "# assert len(test_ns['greeting']) > 0\n",
    "print(\"✓ llm_query signature test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb60457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _query_one(prompt: str, model: str) -> str:\n",
    "    \"\"\"Execute a single LLM query asynchronously.\"\"\"\n",
    "    return contents(Chat(model)(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f976c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llm_query_batched(prompts: list, ns: dict, name: str = 'batch_res',\n",
    "                      model: str = 'claude-sonnet-4-5') -> list:\n",
    "    \"\"\"Query LLM with multiple prompts concurrently.\n",
    "    \n",
    "    NOTE: Currently executes sequentially due to claudette's synchronous API.\n",
    "    True concurrency pending claudette async API support. The async scaffolding\n",
    "    is in place for when that becomes available.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt strings\n",
    "        ns: Namespace dict where results will be stored\n",
    "        name: Variable name for storing the list of results\n",
    "        model: Claude model to use\n",
    "        \n",
    "    Returns:\n",
    "        List of LLM responses (also stored in ns[name])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        try:\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "        except ImportError:\n",
    "            pass  # Optional dependency for nested event loops\n",
    "        results = loop.run_until_complete(\n",
    "            asyncio.gather(*[_query_one(p, model) for p in prompts])\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        results = asyncio.run(\n",
    "            asyncio.gather(*[_query_one(p, model) for p in prompts])\n",
    "        )\n",
    "    \n",
    "    results = list(results)\n",
    "    ns[name] = results\n",
    "    return results  # Return actual results, not summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ llm_query_batched signature test passed\n"
     ]
    }
   ],
   "source": [
    "# Test llm_query_batched signature\n",
    "test_ns = {}\n",
    "# Note: Commented out to avoid API calls during CI\n",
    "# prompts = [\"Say 'one'\", \"Say 'two'\"]\n",
    "# result = llm_query_batched(prompts, test_ns, name='batch')\n",
    "# assert 'batch' in test_ns\n",
    "# assert len(test_ns['batch']) == 2\n",
    "print(\"✓ llm_query_batched signature test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52f17d",
   "metadata": {},
   "source": [
    "## REPL Execution\n",
    "\n",
    "Execute Python code in a namespace and capture stdout/stderr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ce8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exec_code(code: str, ns: dict) -> REPLResult:\n",
    "    \"\"\"Execute code in namespace and return result.\n",
    "    \n",
    "    Captures stdout, stderr, and any exceptions. The namespace is mutated\n",
    "    with any variables created during execution.\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to execute\n",
    "        ns: Namespace dict for execution\n",
    "        \n",
    "    Returns:\n",
    "        REPLResult with stdout, stderr, locals snapshot, execution_time\n",
    "    \"\"\"\n",
    "    stdout_capture = StringIO()\n",
    "    stderr_capture = StringIO()\n",
    "    old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        sys.stdout, sys.stderr = stdout_capture, stderr_capture\n",
    "        exec(compile(code, '<repl>', 'exec'), ns)\n",
    "        stderr_out = stderr_capture.getvalue()\n",
    "    except Exception as e:\n",
    "        stderr_out = stderr_capture.getvalue() + f\"\\n{type(e).__name__}: {e}\"\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "    \n",
    "    # FIX: Create a snapshot copy of namespace instead of storing reference\n",
    "    # This prevents later modifications from affecting inspection of old iterations\n",
    "    return REPLResult(\n",
    "        stdout=stdout_capture.getvalue(),\n",
    "        stderr=stderr_out,\n",
    "        locals=dict(ns),  # Snapshot, not reference\n",
    "        execution_time=time.time() - start\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ exec_code works\n",
      "✓ exec_code error handling works\n"
     ]
    }
   ],
   "source": [
    "# Test exec_code with explicit namespace\n",
    "test_ns = {}\n",
    "result = exec_code(\"x = 2 + 2\\nprint(x)\", test_ns)\n",
    "assert test_ns['x'] == 4\n",
    "assert '4' in result.stdout\n",
    "assert result.execution_time > 0\n",
    "print(\"✓ exec_code works\")\n",
    "\n",
    "# Test error handling\n",
    "test_ns = {}\n",
    "result = exec_code(\"raise ValueError('test error')\", test_ns)\n",
    "assert 'ValueError: test error' in result.stderr\n",
    "print(\"✓ exec_code error handling works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4222973",
   "metadata": {},
   "source": [
    "## RLM Loop\n",
    "\n",
    "The main RLM iteration loop. Follows the rlmpaper protocol:\n",
    "1. Build system prompt with metadata\n",
    "2. Add first-iteration safeguard\n",
    "3. Execute REPL code blocks\n",
    "4. Check for FINAL/FINAL_VAR\n",
    "5. Repeat until answer or max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _synthesize_fallback_answer(iterations: list, ns: dict) -> str:\n",
    "    \"\"\"Synthesize best-effort answer when max_iters reached.\n",
    "    \n",
    "    Tries to extract useful information from:\n",
    "    1. Common result variable names\n",
    "    2. Last REPL output\n",
    "    \n",
    "    Args:\n",
    "        iterations: List of RLMIteration objects\n",
    "        ns: Namespace dict with variables\n",
    "        \n",
    "    Returns:\n",
    "        Fallback answer string with [Max iterations] prefix\n",
    "    \"\"\"\n",
    "    # Check for common result variable names\n",
    "    for var in ['result', 'answer', 'final', 'output', 'solution']:\n",
    "        if var in ns and ns[var] is not None:\n",
    "            value_str = str(ns[var])\n",
    "            # Truncate if too long\n",
    "            if len(value_str) > 500:\n",
    "                value_str = value_str[:500] + \"...\"\n",
    "            return f\"[Max iterations] Partial result: {value_str}\"\n",
    "    \n",
    "    # Get last REPL output with actual content\n",
    "    for iteration in reversed(iterations):\n",
    "        for cb in iteration.code_blocks:\n",
    "            if cb.result and cb.result.stdout.strip():\n",
    "                stdout = cb.result.stdout.strip()\n",
    "                # Truncate if too long\n",
    "                if len(stdout) > 500:\n",
    "                    stdout = stdout[:500] + \"...\"\n",
    "                return f\"[Max iterations] Last output: {stdout}\"\n",
    "    \n",
    "    return \"[Max iterations] No answer produced\"\n",
    "\n",
    "\n",
    "def rlm_run(query: str, context, ns: dict = None,\n",
    "            model: str = 'claude-sonnet-4-5',\n",
    "            max_iters: int = 10,\n",
    "            logger=None,\n",
    "            verbose: bool = False) -> tuple:\n",
    "    \"\"\"Run RLM loop until FINAL or max iterations.\n",
    "\n",
    "    This implements the RLM protocol: the root LLM emits ```repl``` code blocks\n",
    "    which are executed in a namespace with `context`, `llm_query`,\n",
    "    `llm_query_batched`, and `FINAL_VAR` available. The loop continues until\n",
    "    the model returns FINAL(...) or FINAL_VAR(...).\n",
    "\n",
    "    Args:\n",
    "        query: User's question to answer\n",
    "        context: Context data (str, list of str, or dict)\n",
    "        ns: Namespace dict (if None, creates fresh namespace)\n",
    "        model: Claude model to use\n",
    "        max_iters: Maximum iterations before giving up\n",
    "        logger: RLMLogger instance for JSON-lines logging (optional)\n",
    "        verbose: Enable Rich console output (default: False)\n",
    "\n",
    "    Returns:\n",
    "        (answer, iterations, namespace) tuple where:\n",
    "        - answer: Final answer string (or fallback if max_iters reached)\n",
    "        - iterations: List of RLMIteration objects\n",
    "        - namespace: The dict containing all REPL variables\n",
    "    \"\"\"\n",
    "    if ns is None:\n",
    "        ns = {}\n",
    "\n",
    "    # Initialize verbose printer\n",
    "    from rlm.logger import VerbosePrinter\n",
    "    verbose_printer = VerbosePrinter(enabled=verbose)\n",
    "\n",
    "    # Define FINAL_VAR helper (following rlmpaper design)\n",
    "    def _final_var(variable_name: str) -> str:\n",
    "        \"\"\"Return the value of a variable as a final answer.\n",
    "\n",
    "        This is an executable function that the model can call inside code blocks\n",
    "        to test if a variable exists and preview its value before committing to\n",
    "        using FINAL_VAR outside code blocks.\n",
    "\n",
    "        Args:\n",
    "            variable_name: Name of the variable to return\n",
    "\n",
    "        Returns:\n",
    "            The variable's value as a string, or an error message if not found\n",
    "        \"\"\"\n",
    "        variable_name = variable_name.strip().strip('\"').strip(\"'\")\n",
    "        if variable_name in ns:\n",
    "            return str(ns[variable_name])\n",
    "        return f\"Error: Variable '{variable_name}' not found in namespace\"\n",
    "\n",
    "    # Setup namespace with REPL environment\n",
    "    meta = QueryMetadata(context)\n",
    "    ns['context'] = context\n",
    "\n",
    "    # Bind llm_query functions to this namespace and model\n",
    "    ns['llm_query'] = partial(llm_query, ns=ns, model=model)\n",
    "    ns['llm_query_batched'] = partial(llm_query_batched, ns=ns, model=model)\n",
    "\n",
    "    # Add FINAL_VAR as executable function (following rlmpaper design)\n",
    "    ns['FINAL_VAR'] = _final_var\n",
    "\n",
    "    # Log metadata\n",
    "    if logger:\n",
    "        logger.log_metadata({\n",
    "            'query': query,\n",
    "            'context_type': meta.context_type,\n",
    "            'context_length': meta.context_total_length,\n",
    "            'model': model,\n",
    "            'max_iters': max_iters\n",
    "        })\n",
    "\n",
    "    # Print header\n",
    "    verbose_printer.print_header(query=query, context=str(context)[:100], max_iters=max_iters)\n",
    "\n",
    "    # Build initial messages with rlmpaper system prompt\n",
    "    messages = build_rlm_system_prompt(query_metadata=meta)\n",
    "    chat = Chat(model, sp=messages[0]['content'])\n",
    "\n",
    "    # Add metadata message if present\n",
    "    if len(messages) > 1:\n",
    "        chat.h.append(messages[1])\n",
    "\n",
    "    iterations = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        iter_start = time.time()\n",
    "\n",
    "        # Build user prompt (includes first-iteration safeguard)\n",
    "        user_msg = build_user_prompt(root_prompt=query, iteration=i)\n",
    "\n",
    "        # Get response from root LLM\n",
    "        response = contents(chat(user_msg['content']))\n",
    "\n",
    "        # Extract and execute code blocks\n",
    "        code_strs = find_code_blocks(response)\n",
    "        code_blocks = []\n",
    "        for code in code_strs:\n",
    "            result = exec_code(code, ns)\n",
    "            code_blocks.append(CodeBlock(code=code, result=result))\n",
    "\n",
    "        # Check for final answer\n",
    "        answer = find_final_answer(response, ns=ns)\n",
    "\n",
    "        # Record iteration\n",
    "        iteration = RLMIteration(\n",
    "            prompt=user_msg['content'],\n",
    "            response=response,\n",
    "            code_blocks=code_blocks,\n",
    "            final_answer=answer,\n",
    "            iteration_time=time.time() - iter_start\n",
    "        )\n",
    "        iterations.append(iteration)\n",
    "\n",
    "        # Log iteration\n",
    "        if logger:\n",
    "            logger.log(iteration, i + 1)\n",
    "        verbose_printer.print_iteration(iteration, i + 1)\n",
    "\n",
    "        # If we have an answer, we're done\n",
    "        if answer is not None:\n",
    "            verbose_printer.print_final_answer(answer)\n",
    "            verbose_printer.print_summary(\n",
    "                total_iterations=len(iterations),\n",
    "                total_time=time.time() - start_time\n",
    "            )\n",
    "            return answer, iterations, ns\n",
    "\n",
    "        # Add iteration to chat history for next round\n",
    "        for msg in format_iteration(iteration):\n",
    "            chat.h.append(msg)\n",
    "\n",
    "    # Reached max iterations - synthesize fallback answer\n",
    "    fallback = _synthesize_fallback_answer(iterations, ns)\n",
    "    verbose_printer.print_summary(\n",
    "        total_iterations=len(iterations),\n",
    "        total_time=time.time() - start_time\n",
    "    )\n",
    "    return fallback, iterations, ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d8e05",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cabd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ QueryMetadata works\n"
     ]
    }
   ],
   "source": [
    "# Test QueryMetadata (from _rlmpaper_compat)\n",
    "meta = QueryMetadata([\"chunk1\", \"chunk2\", \"chunk3\"])\n",
    "assert meta.context_type == \"list\"\n",
    "assert len(meta.context_lengths) == 3\n",
    "assert meta.context_total_length == sum(len(c) for c in [\"chunk1\", \"chunk2\", \"chunk3\"])\n",
    "print(\"✓ QueryMetadata works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99616afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ find_code_blocks works\n"
     ]
    }
   ],
   "source": [
    "# Test find_code_blocks (from _rlmpaper_compat)\n",
    "text = \"\"\"Here's some code:\n",
    "```repl\n",
    "x = 1 + 1\n",
    "print(x)\n",
    "```\n",
    "And more text.\"\"\"\n",
    "blocks = find_code_blocks(text)\n",
    "assert len(blocks) == 1\n",
    "assert 'x = 1 + 1' in blocks[0]\n",
    "print(\"✓ find_code_blocks works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b3b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ find_final_answer works\n"
     ]
    }
   ],
   "source": [
    "# Test find_final_answer (from _rlmpaper_compat)\n",
    "assert find_final_answer(\"FINAL(42)\") == \"42\"\n",
    "assert find_final_answer(\"FINAL(The answer is 42)\") == \"The answer is 42\"\n",
    "\n",
    "# Test FINAL_VAR\n",
    "test_ns = {'result': 'hello world'}\n",
    "assert find_final_answer(\"FINAL_VAR(result)\", ns=test_ns) == \"hello world\"\n",
    "\n",
    "# Test no final\n",
    "assert find_final_answer(\"Just some text\") is None\n",
    "print(\"✓ find_final_answer works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d038ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rlm_run namespace setup works\n",
      "✓ FINAL_VAR executable function works\n"
     ]
    }
   ],
   "source": [
    "# Test rlm_run with simple mock scenario\n",
    "# Note: This doesn't call LLM APIs, just tests the structure\n",
    "context = [\"The capital of France is Paris.\"]\n",
    "test_ns = {}\n",
    "\n",
    "# We can't easily test without API calls, but we can verify the function signature\n",
    "# and that it sets up the namespace correctly\n",
    "meta = QueryMetadata(context)\n",
    "test_ns['context'] = context\n",
    "test_ns['llm_query'] = partial(llm_query, ns=test_ns, model='claude-sonnet-4-5')\n",
    "test_ns['llm_query_batched'] = partial(llm_query_batched, ns=test_ns, model='claude-sonnet-4-5')\n",
    "\n",
    "# Test that FINAL_VAR is now set up as a callable function\n",
    "def _test_final_var(variable_name: str) -> str:\n",
    "    variable_name = variable_name.strip().strip('\"').strip(\"'\")\n",
    "    if variable_name in test_ns:\n",
    "        return str(test_ns[variable_name])\n",
    "    return f\"Error: Variable '{variable_name}' not found in namespace\"\n",
    "\n",
    "test_ns['FINAL_VAR'] = _test_final_var\n",
    "\n",
    "assert 'context' in test_ns\n",
    "assert 'llm_query' in test_ns\n",
    "assert 'llm_query_batched' in test_ns\n",
    "assert 'FINAL_VAR' in test_ns\n",
    "assert callable(test_ns['llm_query'])\n",
    "assert callable(test_ns['llm_query_batched'])\n",
    "assert callable(test_ns['FINAL_VAR'])\n",
    "\n",
    "# Test FINAL_VAR function behavior\n",
    "test_ns['my_var'] = 'test value'\n",
    "assert test_ns['FINAL_VAR']('my_var') == 'test value'\n",
    "assert 'Error' in test_ns['FINAL_VAR']('nonexistent')\n",
    "print(\"✓ rlm_run namespace setup works\")\n",
    "print(\"✓ FINAL_VAR executable function works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417ea98",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Simple usage (requires API key)\n",
    "context = [\"The speed of light is 299,792,458 m/s.\"]\n",
    "answer, iterations, ns = rlm_run(\"What is the speed of light?\", context)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Iterations: {len(iterations)}\")\n",
    "print(f\"Variables in namespace: {[k for k in ns.keys() if not k.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a87416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Persistent namespace across runs\n",
    "ns = {}\n",
    "\n",
    "# Define example contexts\n",
    "context1 = {'prompt': 'Query ontology for X', 'tools': [...]}\n",
    "context2 = {'prompt': 'Query ontology for Y', 'tools': [...]}\n",
    "\n",
    "# Run multiple queries, reusing namespace\n",
    "answer1, iters1, ns = rlm_run(\"What is X?\", context1, ns=ns)\n",
    "answer2, iters2, ns = rlm_run(\"What about Y?\", context2, ns=ns)\n",
    "# ns now contains variables from both runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
