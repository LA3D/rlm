{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d399ad97",
   "metadata": {},
   "source": [
    "# procedural_memory\n",
    "\n",
    "> ReasoningBank-style procedural memory for RLM\n",
    ">\n",
    "> This module implements a closed-loop memory system that learns reusable strategies from RLM trajectories.\n",
    "> After each run, the system judges success/failure, extracts procedural memories, and retrieves relevant\n",
    "> memories for future tasks using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07726e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp procedural_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc6233",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements **Stage 2.5: Procedural Memory Loop** inspired by the ReasoningBank paper. The goal is to enable an RLM agent to improve over time by accumulating procedural knowledge (strategies, templates, debugging moves) without replacing evidence-based retrieval.\n",
    "\n",
    "### Closed-Loop Cycle\n",
    "\n",
    "```\n",
    "┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "│ RETRIEVE │───▶│ INTERACT │───▶│ EXTRACT  │\n",
    "│ (BM25)   │    │ (rlm_run)│    │ (Judge + │\n",
    "└────▲─────┘    └──────────┘    │ Extractor)│\n",
    "     │                          └─────┬─────┘\n",
    "     │                                │\n",
    "     │          ┌──────────┐          │\n",
    "     └──────────│  STORE   │◀─────────┘\n",
    "                │ (JSON)   │\n",
    "                └──────────┘\n",
    "```\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Procedural, not episodic**: Memories are strategies/checklists, not retellings\n",
    "2. **Bounded injection**: Only title + description + 3 key bullets in prompts\n",
    "3. **Evidence-sensitive judgment**: Success requires grounding in retrieved evidence\n",
    "4. **Keyword retrieval**: BM25 over title/description/tags (deterministic, offline)\n",
    "5. **Append-only storage**: Simple JSON file for experimentation\n",
    "\n",
    "### Reference\n",
    "\n",
    "- [ReasoningBank Paper](https://arxiv.org/html/2509.25140v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056bf41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rlm.core import llm_query, rlm_run\n",
    "from rlm._rlmpaper_compat import RLMIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c351f5",
   "metadata": {},
   "source": [
    "## Memory Schema\n",
    "\n",
    "A `MemoryItem` represents a reusable procedural insight extracted from an RLM trajectory.\n",
    "\n",
    "**Constraints**:\n",
    "- Items must be small enough to inject into prompts\n",
    "- `content` should be procedural (steps/checklist), not a retelling\n",
    "- Up to 3 items extracted per trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc33a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MemoryItem:\n",
    "    \"\"\"A reusable procedural memory extracted from an RLM trajectory.\n",
    "    \n",
    "    Attributes:\n",
    "        id: Unique identifier (UUID)\n",
    "        title: Concise identifier (≤10 words)\n",
    "        description: One-sentence summary\n",
    "        content: Procedural steps/checklist/template (Markdown)\n",
    "        source_type: 'success' or 'failure'\n",
    "        task_query: Original task that produced this memory\n",
    "        created_at: ISO timestamp\n",
    "        access_count: Number of times retrieved (for future consolidation)\n",
    "        tags: Keywords for BM25 retrieval\n",
    "        session_id: Optional session ID from DatasetMeta (links to dataset session)\n",
    "    \"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    content: str\n",
    "    source_type: str  # 'success' or 'failure'\n",
    "    task_query: str\n",
    "    created_at: str\n",
    "    access_count: int = 0\n",
    "    tags: Optional[list[str]] = None\n",
    "    session_id: Optional[str] = None  # NEW: Links to DatasetMeta.session_id\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict) -> 'MemoryItem':\n",
    "        \"\"\"Create MemoryItem from dictionary.\"\"\"\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d8fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MemoryItem serialization works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ac327ed6d57d>:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "# Test MemoryItem creation and serialization\n",
    "test_item = MemoryItem(\n",
    "    id='test-uuid',\n",
    "    title='SPARQL Query Pattern',\n",
    "    description='Template for searching entities by label.',\n",
    "    content='- Use `rdfs:label` for human-readable names\\n- Add FILTER for case-insensitive search',\n",
    "    source_type='success',\n",
    "    task_query='Find entities named \"Activity\"',\n",
    "    created_at=datetime.utcnow().isoformat(),\n",
    "    tags=['sparql', 'search', 'rdfs']\n",
    ")\n",
    "\n",
    "# Test roundtrip\n",
    "data = test_item.to_dict()\n",
    "restored = MemoryItem.from_dict(data)\n",
    "assert restored.title == test_item.title\n",
    "assert restored.tags == test_item.tags\n",
    "print(\"✓ MemoryItem serialization works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8beab",
   "metadata": {},
   "source": [
    "## Memory Store\n",
    "\n",
    "Persistent storage for procedural memories using a simple JSON file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b28490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MemoryStore:\n",
    "    \"\"\"Persistent storage for procedural memories.\n",
    "    \n",
    "    Attributes:\n",
    "        memories: List of MemoryItem objects\n",
    "        path: Path to JSON file\n",
    "    \"\"\"\n",
    "    memories: list[MemoryItem] = field(default_factory=list)\n",
    "    path: Optional[Path] = None\n",
    "    \n",
    "    def add(self, item: MemoryItem) -> str:\n",
    "        \"\"\"Add a memory item to the store.\n",
    "        \n",
    "        Returns:\n",
    "            Status message\n",
    "        \"\"\"\n",
    "        self.memories.append(item)\n",
    "        return f\"Added memory '{item.title}' (id={item.id})\"\n",
    "    \n",
    "    def save(self) -> str:\n",
    "        \"\"\"Persist memories to JSON file.\n",
    "        \n",
    "        Returns:\n",
    "            Status message with path and count\n",
    "        \"\"\"\n",
    "        if self.path is None:\n",
    "            return \"No path configured - not saving\"\n",
    "        \n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        data = [m.to_dict() for m in self.memories]\n",
    "        \n",
    "        with open(self.path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        return f\"Saved {len(self.memories)} memories to {self.path}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'MemoryStore':\n",
    "        \"\"\"Load memories from JSON file.\n",
    "        \n",
    "        Returns:\n",
    "            MemoryStore instance with loaded memories\n",
    "        \"\"\"\n",
    "        if not path.exists():\n",
    "            return cls(memories=[], path=path)\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        memories = [MemoryItem.from_dict(item) for item in data]\n",
    "        return cls(memories=memories, path=path)\n",
    "    \n",
    "    def get_corpus_for_bm25(self) -> list[list[str]]:\n",
    "        \"\"\"Build corpus for BM25 indexing.\n",
    "        \n",
    "        Each document is title + description + tags, tokenized.\n",
    "        \n",
    "        Returns:\n",
    "            List of tokenized documents\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for m in self.memories:\n",
    "            text = f\"{m.title} {m.description}\"\n",
    "            if m.tags:\n",
    "                text += \" \" + \" \".join(m.tags)\n",
    "            corpus.append(text.lower().split())\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MemoryStore save/load/corpus works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d4d6b5faae2e>:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n",
      "<ipython-input-1-d4d6b5faae2e>:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "# Test MemoryStore save/load roundtrip\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    test_path = Path(tmpdir) / 'test_memories.json'\n",
    "    \n",
    "    # Create store and add items\n",
    "    store = MemoryStore(path=test_path)\n",
    "    item1 = MemoryItem(\n",
    "        id=str(uuid.uuid4()),\n",
    "        title='Test Memory 1',\n",
    "        description='First test memory',\n",
    "        content='- Step 1\\n- Step 2',\n",
    "        source_type='success',\n",
    "        task_query='test task 1',\n",
    "        created_at=datetime.utcnow().isoformat(),\n",
    "        tags=['test', 'example']\n",
    "    )\n",
    "    item2 = MemoryItem(\n",
    "        id=str(uuid.uuid4()),\n",
    "        title='Test Memory 2',\n",
    "        description='Second test memory',\n",
    "        content='- Action A\\n- Action B',\n",
    "        source_type='failure',\n",
    "        task_query='test task 2',\n",
    "        created_at=datetime.utcnow().isoformat(),\n",
    "        tags=['test']\n",
    "    )\n",
    "    \n",
    "    store.add(item1)\n",
    "    store.add(item2)\n",
    "    store.save()\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded = MemoryStore.load(test_path)\n",
    "    assert len(loaded.memories) == 2\n",
    "    assert loaded.memories[0].title == 'Test Memory 1'\n",
    "    assert loaded.memories[1].source_type == 'failure'\n",
    "    assert loaded.memories[0].tags == ['test', 'example']\n",
    "    \n",
    "    # Test corpus generation\n",
    "    corpus = loaded.get_corpus_for_bm25()\n",
    "    assert len(corpus) == 2\n",
    "    assert 'test' in corpus[0]  # From title and tags\n",
    "    \n",
    "    print(\"✓ MemoryStore save/load/corpus works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2754ec",
   "metadata": {},
   "source": [
    "## Trajectory Artifact\n",
    "\n",
    "Extract a bounded representation of an RLM run for the judge and extractor.\n",
    "\n",
    "**Purpose**: Summarize iterations into key steps (~10 max) with actions and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_trajectory_artifact(\n",
    "    task: str,\n",
    "    answer: str,\n",
    "    iterations: list[RLMIteration],\n",
    "    ns: dict\n",
    ") -> dict:\n",
    "    \"\"\"Create bounded trajectory artifact for judge/extractor.\n",
    "    \n",
    "    Summarizes each iteration's code blocks into 1-2 line \"action + outcome\",\n",
    "    limiting to ~10 most informative key steps.\n",
    "    \n",
    "    Args:\n",
    "        task: Original task query\n",
    "        answer: Final answer from rlm_run\n",
    "        iterations: List of RLMIteration objects\n",
    "        ns: Final namespace dict\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "        - task: str\n",
    "        - final_answer: str\n",
    "        - iteration_count: int\n",
    "        - converged: bool (whether final_answer was set)\n",
    "        - key_steps: List of {iteration, action, outcome}\n",
    "        - variables_created: List of variable names in ns\n",
    "        - errors_encountered: List of error messages from stderr\n",
    "    \"\"\"\n",
    "    key_steps = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, iteration in enumerate(iterations, 1):\n",
    "        # Summarize code blocks in this iteration\n",
    "        for block in iteration.code_blocks:\n",
    "            # Extract action from code (first line or summary)\n",
    "            code_lines = block.code.strip().split('\\n')\n",
    "            action = code_lines[0][:80] if code_lines else \"[empty code]\"\n",
    "            \n",
    "            # Extract outcome from result\n",
    "            if block.result and block.result.stderr:\n",
    "                outcome = f\"ERROR: {block.result.stderr[:100]}\"\n",
    "                errors.append(block.result.stderr)\n",
    "            elif block.result and block.result.stdout:\n",
    "                outcome = block.result.stdout[:100]\n",
    "            else:\n",
    "                outcome = \"(no output)\"\n",
    "            \n",
    "            key_steps.append({\n",
    "                'iteration': i,\n",
    "                'action': action,\n",
    "                'outcome': outcome\n",
    "            })\n",
    "    \n",
    "    # Limit to 10 most informative steps (prioritize errors and final steps)\n",
    "    if len(key_steps) > 10:\n",
    "        # Keep first 3, last 4, and up to 3 with errors\n",
    "        error_steps = [s for s in key_steps if 'ERROR' in s['outcome']]\n",
    "        key_steps = key_steps[:3] + error_steps[:3] + key_steps[-4:]\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        key_steps = [s for s in key_steps if not (tuple(s.items()) in seen or seen.add(tuple(s.items())))]\n",
    "        key_steps = key_steps[:10]\n",
    "    \n",
    "    return {\n",
    "        'task': task,\n",
    "        'final_answer': answer,\n",
    "        'iteration_count': len(iterations),\n",
    "        'converged': bool(answer and answer != \"No answer provided\"),\n",
    "        'key_steps': key_steps,\n",
    "        'variables_created': list(ns.keys()) if ns else [],\n",
    "        'errors_encountered': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trajectory artifact extraction works\n"
     ]
    }
   ],
   "source": [
    "# Test with mock iterations\n",
    "from rlm._rlmpaper_compat import CodeBlock, REPLResult\n",
    "\n",
    "mock_block1 = CodeBlock(\n",
    "    code=\"search('Activity')\",\n",
    "    result=REPLResult(stdout=\"Found 3 entities\", stderr=None, locals={})\n",
    ")\n",
    "mock_block2 = CodeBlock(\n",
    "    code=\"describe_entity('prov:Activity')\",\n",
    "    result=REPLResult(stdout=\"prov:Activity is a class\", stderr=None, locals={})\n",
    ")\n",
    "mock_iteration = RLMIteration(\n",
    "    prompt=\"test prompt\",\n",
    "    response=\"test response\",\n",
    "    code_blocks=[mock_block1, mock_block2],\n",
    "    final_answer=None,\n",
    "    iteration_time=0.5\n",
    ")\n",
    "\n",
    "artifact = extract_trajectory_artifact(\n",
    "    task=\"What is prov:Activity?\",\n",
    "    answer=\"prov:Activity is a class\",\n",
    "    iterations=[mock_iteration],\n",
    "    ns={'result': 'prov:Activity is a class'}\n",
    ")\n",
    "\n",
    "assert artifact['task'] == \"What is prov:Activity?\"\n",
    "assert artifact['iteration_count'] == 1\n",
    "assert artifact['converged'] == True\n",
    "assert len(artifact['key_steps']) == 2\n",
    "assert 'search' in artifact['key_steps'][0]['action'].lower()\n",
    "assert len(artifact['variables_created']) == 1\n",
    "print(\"✓ Trajectory artifact extraction works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996881f",
   "metadata": {},
   "source": [
    "## Judge\n",
    "\n",
    "Classify trajectory as success or failure with evidence-sensitivity.\n",
    "\n",
    "**Success criteria**:\n",
    "1. Answer directly addresses the task\n",
    "2. Answer is grounded in retrieved evidence (not hallucinated)\n",
    "3. Reasoning shows systematic exploration\n",
    "\n",
    "**Failure indicators**:\n",
    "1. No answer produced (didn't converge)\n",
    "2. Answer doesn't address the task\n",
    "3. Answer makes claims without supporting evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def judge_trajectory(artifact: dict, ns: dict = None) -> dict:\n",
    "    \"\"\"Judge trajectory success using llm_query.\n",
    "    \n",
    "    Evidence-sensitive: success requires grounding in retrieved evidence.\n",
    "    \n",
    "    Args:\n",
    "        artifact: Trajectory artifact from extract_trajectory_artifact()\n",
    "        ns: Optional namespace for additional context\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "        - is_success: bool\n",
    "        - reason: str\n",
    "        - confidence: str ('high', 'medium', 'low')\n",
    "        - missing: list[str] (what evidence was lacking if failure)\n",
    "    \"\"\"\n",
    "    # Format key steps for prompt\n",
    "    steps_text = \"\\n\".join([\n",
    "        f\"  {s['iteration']}. {s['action']} → {s['outcome']}\"\n",
    "        for s in artifact['key_steps']\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate this RLM trajectory for task completion quality.\n",
    "\n",
    "Task: {artifact['task']}\n",
    "Final Answer: {artifact['final_answer']}\n",
    "Converged: {artifact['converged']}\n",
    "Key Steps:\n",
    "{steps_text}\n",
    "\n",
    "A trajectory is SUCCESSFUL if:\n",
    "1. The answer directly addresses the task\n",
    "2. The answer is grounded in retrieved evidence (not hallucinated)\n",
    "3. The reasoning steps show systematic exploration\n",
    "\n",
    "A trajectory FAILED if:\n",
    "1. No answer was produced (didn't converge)\n",
    "2. Answer doesn't address the task\n",
    "3. Answer makes claims without supporting evidence\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"is_success\": true/false, \"reason\": \"...\", \"confidence\": \"high/medium/low\", \"missing\": [\"...\"]}}\"\"\"\n",
    "    \n",
    "    # Use llm_query to get judgment (create temp namespace)\n",
    "    temp_ns = ns if ns is not None else {}\n",
    "    response = llm_query(prompt, temp_ns, name='judgment_response')\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        response_text = response.strip()\n",
    "        if '```json' in response_text:\n",
    "            response_text = response_text.split('```json')[1].split('```')[0].strip()\n",
    "        elif '```' in response_text:\n",
    "            response_text = response_text.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        judgment = json.loads(response_text)\n",
    "        \n",
    "        # Ensure required fields\n",
    "        if 'missing' not in judgment:\n",
    "            judgment['missing'] = []\n",
    "        \n",
    "        return judgment\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        # Fallback for parsing errors\n",
    "        return {\n",
    "            'is_success': artifact['converged'],\n",
    "            'reason': f\"Parse error: {e}. Raw response: {response[:200]}\",\n",
    "            'confidence': 'low',\n",
    "            'missing': ['Unable to parse judgment']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Reason: The trajectory successfully answers the task by identifying prov:Activity as a class in the PROV ontology that represents activities. The answer directly addresses the question 'What is prov:Activity?', is grounded in retrieved evidence from the describe_entity step, and shows systematic exploration through searching and then describing the specific entity. The trajectory converged with a clear, accurate answer.\n",
      "Confidence: high\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test judge with real LLM (requires API key)\n",
    "test_artifact = {\n",
    "    'task': 'What is prov:Activity?',\n",
    "    'final_answer': 'prov:Activity is a class representing activities in PROV ontology',\n",
    "    'iteration_count': 2,\n",
    "    'converged': True,\n",
    "    'key_steps': [\n",
    "        {'iteration': 1, 'action': \"search('Activity')\", 'outcome': 'Found 3 entities'},\n",
    "        {'iteration': 2, 'action': \"describe_entity('prov:Activity')\", 'outcome': 'A class in PROV'}\n",
    "    ],\n",
    "    'variables_created': ['result'],\n",
    "    'errors_encountered': []\n",
    "}\n",
    "\n",
    "judgment = judge_trajectory(test_artifact)\n",
    "print(f\"Success: {judgment['is_success']}\")\n",
    "print(f\"Reason: {judgment['reason']}\")\n",
    "print(f\"Confidence: {judgment['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40cec4",
   "metadata": {},
   "source": [
    "## Extractor\n",
    "\n",
    "Extract 1-3 reusable memory items from a trajectory.\n",
    "\n",
    "**For successes**: Emphasize why the approach worked\n",
    "\n",
    "**For failures**: Emphasize what to avoid and recovery strategies\n",
    "\n",
    "**Output format**: Procedural (steps/checklist/template), NOT a retelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_memories(\n",
    "    artifact: dict,\n",
    "    judgment: dict,\n",
    "    ns: dict = None\n",
    ") -> list[MemoryItem]:\n",
    "    \"\"\"Extract up to 3 reusable memory items from trajectory.\n",
    "    \n",
    "    Args:\n",
    "        artifact: Trajectory artifact from extract_trajectory_artifact()\n",
    "        judgment: Judgment dict from judge_trajectory()\n",
    "        ns: Optional namespace for additional context\n",
    "    \n",
    "    Returns:\n",
    "        List of MemoryItem objects (0-3 items)\n",
    "    \"\"\"\n",
    "    source_type = 'success' if judgment['is_success'] else 'failure'\n",
    "    \n",
    "    # Capture session_id if available in namespace\n",
    "    session_id = None\n",
    "    if ns is not None:\n",
    "        # Try to get session_id from DatasetMeta\n",
    "        if 'ds_meta' in ns and hasattr(ns['ds_meta'], 'session_id'):\n",
    "            session_id = ns['ds_meta'].session_id\n",
    "    \n",
    "    # Format key steps for prompt\n",
    "    steps_text = \"\\n\".join([\n",
    "        f\"  {s['iteration']}. {s['action']} → {s['outcome']}\"\n",
    "        for s in artifact['key_steps']\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Extract reusable procedural memories from this {source_type} trajectory.\n",
    "\n",
    "Task: {artifact['task']}\n",
    "Outcome: {judgment['reason']}\n",
    "Key Steps:\n",
    "{steps_text}\n",
    "\n",
    "Extract UP TO 3 distinct, reusable insights. Each should be:\n",
    "- Procedural (steps/checklist/template), NOT a retelling of this run\n",
    "- Applicable to similar future tasks\n",
    "- Concise but actionable\n",
    "\n",
    "For ontology/SPARQL work, prefer:\n",
    "- Query templates with placeholders\n",
    "- Debugging strategies (\"if X fails, try Y\")\n",
    "- Exploration patterns (\"start with search, then describe, then probe\")\n",
    "\n",
    "Return ONLY valid JSON array (may have 1-3 items, or empty if no lessons):\n",
    "[{{\n",
    "  \"title\": \"≤10 word identifier\",\n",
    "  \"description\": \"One sentence summary\",\n",
    "  \"content\": \"Markdown with steps/checklist/template\",\n",
    "  \"tags\": [\"keyword1\", \"keyword2\"]\n",
    "}}]\"\"\"\n",
    "    \n",
    "    # Use llm_query to extract memories (create temp namespace)\n",
    "    temp_ns = ns if ns is not None else {}\n",
    "    response = llm_query(prompt, temp_ns, name='extractor_response')\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        response_text = response.strip()\n",
    "        if '```json' in response_text:\n",
    "            response_text = response_text.split('```json')[1].split('```')[0].strip()\n",
    "        elif '```' in response_text:\n",
    "            response_text = response_text.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        extracted = json.loads(response_text)\n",
    "        \n",
    "        # Convert to MemoryItem objects\n",
    "        memories = []\n",
    "        for item in extracted[:3]:  # Limit to 3\n",
    "            memory = MemoryItem(\n",
    "                id=str(uuid.uuid4()),\n",
    "                title=item['title'],\n",
    "                description=item['description'],\n",
    "                content=item['content'],\n",
    "                source_type=source_type,\n",
    "                task_query=artifact['task'],\n",
    "                created_at=datetime.utcnow().isoformat(),\n",
    "                tags=item.get('tags', []),\n",
    "                session_id=session_id  # NEW: Capture session_id from namespace\n",
    "            )\n",
    "            memories.append(memory)\n",
    "        \n",
    "        return memories\n",
    "    except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "        # Return empty list on parsing errors\n",
    "        print(f\"Warning: Failed to extract memories: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7346d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to extract memories: Unterminated string starting at: line 17 column 16 (char 1334)\n",
      "Extracted 0 memories:\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test extractor with real LLM\n",
    "test_artifact = {\n",
    "    'task': 'Find properties of prov:Activity',\n",
    "    'final_answer': 'prov:Activity has properties: prov:startedAtTime, prov:endedAtTime',\n",
    "    'iteration_count': 3,\n",
    "    'converged': True,\n",
    "    'key_steps': [\n",
    "        {'iteration': 1, 'action': \"search('Activity')\", 'outcome': 'Found prov:Activity'},\n",
    "        {'iteration': 2, 'action': \"describe_entity('prov:Activity')\", 'outcome': 'A class'},\n",
    "        {'iteration': 3, 'action': \"get_properties('prov:Activity')\", 'outcome': 'Listed properties'}\n",
    "    ],\n",
    "    'variables_created': ['activity_props'],\n",
    "    'errors_encountered': []\n",
    "}\n",
    "\n",
    "test_judgment = {\n",
    "    'is_success': True,\n",
    "    'reason': 'Answer grounded in ontology data',\n",
    "    'confidence': 'high',\n",
    "    'missing': []\n",
    "}\n",
    "\n",
    "memories = extract_memories(test_artifact, test_judgment)\n",
    "print(f\"Extracted {len(memories)} memories:\")\n",
    "for m in memories:\n",
    "    print(f\"  - {m.title}\")\n",
    "    print(f\"    Tags: {m.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19f22f",
   "metadata": {},
   "source": [
    "## BM25 Retrieval\n",
    "\n",
    "Find relevant memories for new tasks using keyword-based BM25 retrieval.\n",
    "\n",
    "**Searches over**: title + description + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def retrieve_memories(\n",
    "    store: MemoryStore,\n",
    "    task: str,\n",
    "    k: int = 3\n",
    ") -> list[MemoryItem]:\n",
    "    \"\"\"Retrieve top-k relevant memories using BM25.\n",
    "    \n",
    "    Tokenizes task and searches over title + description + tags.\n",
    "    \n",
    "    Args:\n",
    "        store: MemoryStore instance\n",
    "        task: Task query string\n",
    "        k: Number of memories to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of top-k MemoryItem objects (may be fewer if scores ≤ 0)\n",
    "    \"\"\"\n",
    "    if not store.memories:\n",
    "        return []\n",
    "    \n",
    "    # Build BM25 index\n",
    "    corpus = store.get_corpus_for_bm25()\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    \n",
    "    # Query\n",
    "    query_tokens = task.lower().split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k by score (BM25 can return negative scores for small corpora)\n",
    "    scored = [(i, s) for i, s in enumerate(scores)]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Increment access_count for retrieved memories\n",
    "    results = []\n",
    "    for i, _ in scored[:k]:\n",
    "        store.memories[i].access_count += 1\n",
    "        results.append(store.memories[i])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3973db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retrieved 2 memories for 'search for entities'\n",
      "✓ Retrieved 2 memories for 'SPARQL query broken'\n",
      "✓ Retrieved 2 memories for 'properties question'\n",
      "✓ Access count tracking works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c9306d916f1d>:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n",
      "<ipython-input-1-c9306d916f1d>:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n",
      "<ipython-input-1-c9306d916f1d>:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "# Test BM25 retrieval\n",
    "test_store = MemoryStore()\n",
    "\n",
    "# Add diverse memories\n",
    "test_store.add(MemoryItem(\n",
    "    id=str(uuid.uuid4()),\n",
    "    title='SPARQL query pattern for entity search',\n",
    "    description='Use rdfs:label with FILTER for case-insensitive search.',\n",
    "    content='- Step 1\\n- Step 2',\n",
    "    source_type='success',\n",
    "    task_query='Find entities by name',\n",
    "    created_at=datetime.utcnow().isoformat(),\n",
    "    tags=['sparql', 'search', 'entity']\n",
    "))\n",
    "\n",
    "test_store.add(MemoryItem(\n",
    "    id=str(uuid.uuid4()),\n",
    "    title='Property exploration strategy',\n",
    "    description='Systematically explore properties using describe then probe.',\n",
    "    content='- Action A\\n- Action B',\n",
    "    source_type='success',\n",
    "    task_query='What properties does X have?',\n",
    "    created_at=datetime.utcnow().isoformat(),\n",
    "    tags=['properties', 'exploration']\n",
    "))\n",
    "\n",
    "test_store.add(MemoryItem(\n",
    "    id=str(uuid.uuid4()),\n",
    "    title='Debugging failed SPARQL queries',\n",
    "    description='Check syntax, namespaces, and endpoint first.',\n",
    "    content='- Check 1\\n- Check 2',\n",
    "    source_type='failure',\n",
    "    task_query='Query failed with error',\n",
    "    created_at=datetime.utcnow().isoformat(),\n",
    "    tags=['sparql', 'debugging', 'error']\n",
    "))\n",
    "\n",
    "# Test retrieval for different queries\n",
    "results1 = retrieve_memories(test_store, 'How do I search for entities?', k=2)\n",
    "assert len(results1) <= 2\n",
    "assert any('search' in r.title.lower() or 'search' in r.tags for r in results1)\n",
    "print(f\"✓ Retrieved {len(results1)} memories for 'search for entities'\")\n",
    "\n",
    "results2 = retrieve_memories(test_store, 'My SPARQL query is broken', k=2)\n",
    "assert len(results2) <= 2\n",
    "assert any('sparql' in r.tags for r in results2)\n",
    "print(f\"✓ Retrieved {len(results2)} memories for 'SPARQL query broken'\")\n",
    "\n",
    "results3 = retrieve_memories(test_store, 'What properties does prov:Activity have?', k=2)\n",
    "print(f\"✓ Retrieved {len(results3)} memories for 'properties question'\")\n",
    "\n",
    "# Test access count increment\n",
    "assert results1[0].access_count > 0\n",
    "print(\"✓ Access count tracking works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac3495",
   "metadata": {},
   "source": [
    "## Injection Formatting\n",
    "\n",
    "Format retrieved memories for bounded prompt injection.\n",
    "\n",
    "**Output includes**:\n",
    "- Assessment instruction\n",
    "- Title + description + up to 3 key bullets from content\n",
    "\n",
    "**Never injects full content** to maintain bounded prompt size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_memories_for_injection(\n",
    "    memories: list[MemoryItem],\n",
    "    max_bullets: int = 3\n",
    ") -> str:\n",
    "    \"\"\"Format memories for bounded prompt injection.\n",
    "    \n",
    "    Returns string with:\n",
    "    - Assessment instruction\n",
    "    - Title + description + key bullets from content (up to max_bullets)\n",
    "    \n",
    "    Args:\n",
    "        memories: List of MemoryItem objects to format\n",
    "        max_bullets: Maximum bullets to extract from content\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string for prompt injection\n",
    "    \"\"\"\n",
    "    if not memories:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = [\n",
    "        \"## Relevant Prior Experience\",\n",
    "        \"\",\n",
    "        \"Before taking action, briefly assess which of these strategies apply to your current task and which do not.\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    for i, mem in enumerate(memories, 1):\n",
    "        lines.append(f\"### {i}. {mem.title}\")\n",
    "        lines.append(mem.description)\n",
    "        lines.append(\"Key points:\")\n",
    "        \n",
    "        # Extract bullets from content (look for lines starting with - or numbers)\n",
    "        content_lines = mem.content.split('\\n')\n",
    "        bullets = []\n",
    "        for line in content_lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped.startswith('-') or stripped.startswith('*'):\n",
    "                bullets.append(stripped)\n",
    "            elif len(stripped) > 0 and stripped[0].isdigit() and '.' in stripped:\n",
    "                bullets.append(stripped)\n",
    "        \n",
    "        # Use first max_bullets bullets, or first max_bullets lines if no bullets found\n",
    "        if bullets:\n",
    "            for bullet in bullets[:max_bullets]:\n",
    "                lines.append(f\"- {bullet.lstrip('- *')}\")\n",
    "        else:\n",
    "            # Fall back to first few lines\n",
    "            for line in content_lines[:max_bullets]:\n",
    "                if line.strip():\n",
    "                    lines.append(f\"- {line.strip()}\")\n",
    "        \n",
    "        lines.append(\"\")  # Blank line between memories\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Injection formatting works\n",
      "\n",
      "Formatted output:\n",
      "## Relevant Prior Experience\n",
      "\n",
      "Before taking action, briefly assess which of these strategies apply to your current task and which do not.\n",
      "\n",
      "### 1. SPARQL Search Pattern\n",
      "Template for searching entities by label.\n",
      "Key points:\n",
      "- Use rdfs:label for human-readable names\n",
      "- Add FILTER for case-insensitive ma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-2dc0c9d48ca1>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n",
      "<ipython-input-1-2dc0c9d48ca1>:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "# Test injection formatting\n",
    "test_memories = [\n",
    "    MemoryItem(\n",
    "        id='test-1',\n",
    "        title='SPARQL Search Pattern',\n",
    "        description='Template for searching entities by label.',\n",
    "        content=\"\"\"- Use rdfs:label for human-readable names\n",
    "- Add FILTER for case-insensitive matching\n",
    "- Include LIMIT to avoid timeout\n",
    "- Check for alternative label properties\"\"\",\n",
    "        source_type='success',\n",
    "        task_query='test',\n",
    "        created_at=datetime.utcnow().isoformat(),\n",
    "        tags=['sparql']\n",
    "    ),\n",
    "    MemoryItem(\n",
    "        id='test-2',\n",
    "        title='Property Discovery',\n",
    "        description='Systematic approach to finding properties.',\n",
    "        content=\"\"\"1. Start with describe_entity() for overview\n",
    "2. Use get_properties() for full list\n",
    "3. Check both domain and range\n",
    "4. Look for inverse properties\"\"\",\n",
    "        source_type='success',\n",
    "        task_query='test',\n",
    "        created_at=datetime.utcnow().isoformat(),\n",
    "        tags=['properties']\n",
    "    )\n",
    "]\n",
    "\n",
    "formatted = format_memories_for_injection(test_memories, max_bullets=3)\n",
    "\n",
    "# Verify format\n",
    "assert '## Relevant Prior Experience' in formatted\n",
    "assert 'assess which of these strategies' in formatted\n",
    "assert '### 1. SPARQL Search Pattern' in formatted\n",
    "assert '### 2. Property Discovery' in formatted\n",
    "assert 'Use rdfs:label' in formatted\n",
    "assert 'Start with describe_entity' in formatted\n",
    "\n",
    "# Verify bullet limiting (should have max 3 bullets per memory)\n",
    "lines = formatted.split('\\n')\n",
    "bullet_count_mem1 = sum(1 for l in lines[lines.index('### 1. SPARQL Search Pattern'):lines.index('### 2. Property Discovery')] if l.strip().startswith('-'))\n",
    "assert bullet_count_mem1 <= 3\n",
    "\n",
    "print(\"✓ Injection formatting works\")\n",
    "print(\"\\nFormatted output:\")\n",
    "print(formatted[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da15705",
   "metadata": {},
   "source": [
    "## Integration\n",
    "\n",
    "Complete closed-loop: RETRIEVE → INJECT → INTERACT → EXTRACT → STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rlm_run_with_memory(\n",
    "    query: str,\n",
    "    context: str,\n",
    "    memory_store: MemoryStore,\n",
    "    ns: dict = None,\n",
    "    enable_memory_extraction: bool = True,\n",
    "    # NEW: Dataset persistence\n",
    "    persist_dataset: bool = False,\n",
    "    dataset_path: Path = None,\n",
    "    **kwargs\n",
    ") -> tuple[str, list, dict, list[MemoryItem]]:\n",
    "    \"\"\"RLM run with procedural memory loop.\n",
    "    \n",
    "    Closed-loop cycle:\n",
    "    1. RETRIEVE: Get relevant memories via BM25\n",
    "    2. INJECT: Add to context/prompt\n",
    "    3. INTERACT: Run rlm_run()\n",
    "    4. EXTRACT: Judge + extract new memories\n",
    "    5. STORE: Persist new memories\n",
    "    \n",
    "    NEW: Dataset persistence:\n",
    "    - If persist_dataset=True and dataset_path provided, loads snapshot before run\n",
    "    - After run, if dataset was modified, saves snapshot\n",
    "    - Stores snapshot path in extracted MemoryItem for lineage\n",
    "    \n",
    "    Args:\n",
    "        query: Task query string\n",
    "        context: Context string (e.g., ontology summary)\n",
    "        memory_store: MemoryStore instance for retrieval/storage\n",
    "        ns: Optional namespace dict\n",
    "        enable_memory_extraction: Whether to extract and store new memories (default True)\n",
    "        persist_dataset: Whether to persist dataset snapshots (default False)\n",
    "        dataset_path: Optional path for dataset snapshot\n",
    "        **kwargs: Additional arguments for rlm_run()\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answer, iterations, ns, new_memories)\n",
    "    \"\"\"\n",
    "    # NEW: Load dataset snapshot if it exists\n",
    "    if persist_dataset and dataset_path is not None and dataset_path.exists():\n",
    "        try:\n",
    "            from rlm.dataset import load_snapshot\n",
    "            load_snapshot(str(dataset_path), ns)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load dataset snapshot: {e}\")\n",
    "    \n",
    "    # 1. RETRIEVE relevant memories\n",
    "    relevant = retrieve_memories(memory_store, query, k=3)\n",
    "    \n",
    "    # 2. INJECT into context\n",
    "    if relevant:\n",
    "        memory_text = format_memories_for_injection(relevant)\n",
    "        enhanced_context = f\"{memory_text}\\n\\n---\\n\\n{context}\"\n",
    "    else:\n",
    "        enhanced_context = context\n",
    "    \n",
    "    # 3. INTERACT - run RLM\n",
    "    answer, iterations, ns = rlm_run(query, enhanced_context, ns=ns, **kwargs)\n",
    "    \n",
    "    # NEW: Save dataset snapshot if dataset was modified\n",
    "    if persist_dataset and dataset_path is not None:\n",
    "        try:\n",
    "            from rlm.dataset import snapshot_dataset\n",
    "            if 'ds_meta' in ns:\n",
    "                result = snapshot_dataset(ns['ds_meta'], path=str(dataset_path))\n",
    "                print(f\"Dataset snapshot: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save dataset snapshot: {e}\")\n",
    "    \n",
    "    new_memories = []\n",
    "    if enable_memory_extraction:\n",
    "        # 4. EXTRACT - judge and extract memories\n",
    "        artifact = extract_trajectory_artifact(query, answer, iterations, ns)\n",
    "        judgment = judge_trajectory(artifact, ns)\n",
    "        new_memories = extract_memories(artifact, judgment, ns)\n",
    "        \n",
    "        # 5. STORE - persist new memories\n",
    "        for mem in new_memories:\n",
    "            memory_store.add(mem)\n",
    "        if memory_store.path:\n",
    "            memory_store.save()\n",
    "    \n",
    "    return answer, iterations, ns, new_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fe4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Integration test (requires full RLM setup)\n",
    "from rlm.ontology import setup_ontology_context\n",
    "import tempfile\n",
    "\n",
    "def test_memory_improves_convergence():\n",
    "    \"\"\"Second attempt should benefit from first attempt's memory.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        store = MemoryStore(path=Path(tmpdir) / 'test_integration.json')\n",
    "        \n",
    "        # First run - no memories\n",
    "        ns = {}\n",
    "        setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "        \n",
    "        answer1, iters1, ns1, mems1 = rlm_run_with_memory(\n",
    "            \"What is prov:Activity and what properties does it have?\",\n",
    "            ns['prov_meta'].summary(),\n",
    "            store,\n",
    "            ns=ns\n",
    "        )\n",
    "        print(f\"\\nFirst run: {len(iters1)} iterations, {len(mems1)} memories extracted\")\n",
    "        for mem in mems1:\n",
    "            print(f\"  - {mem.title}\")\n",
    "        \n",
    "        # Second run - similar task, should retrieve memories\n",
    "        ns2 = {}\n",
    "        setup_ontology_context('ontology/prov.ttl', ns2, name='prov')\n",
    "        \n",
    "        answer2, iters2, ns2, mems2 = rlm_run_with_memory(\n",
    "            \"What is prov:Entity and what properties does it have?\",\n",
    "            ns2['prov_meta'].summary(),\n",
    "            store,\n",
    "            ns=ns2\n",
    "        )\n",
    "        print(f\"\\nSecond run: {len(iters2)} iterations\")\n",
    "        print(f\"Total memories in store: {len(store.memories)}\")\n",
    "        \n",
    "        # Verify memories were retrieved\n",
    "        retrieved_for_second = retrieve_memories(\n",
    "            store,\n",
    "            \"What is prov:Entity and what properties does it have?\",\n",
    "            k=3\n",
    "        )\n",
    "        print(f\"Memories that would be retrieved for second run: {len(retrieved_for_second)}\")\n",
    "        for mem in retrieved_for_second:\n",
    "            print(f\"  - {mem.title} (accessed {mem.access_count} times)\")\n",
    "\n",
    "# Run test\n",
    "# test_memory_improves_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234abf5",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "End-to-end examples with PROV ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Full example: Build up procedural memory over multiple queries\n",
    "from rlm.ontology import setup_ontology_context\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize memory store\n",
    "store = MemoryStore(path=Path('memories/prov_memories.json'))\n",
    "\n",
    "# If store exists, load it\n",
    "if store.path.exists():\n",
    "    store = MemoryStore.load(store.path)\n",
    "    print(f\"Loaded {len(store.memories)} existing memories\")\n",
    "\n",
    "# Setup ontology context\n",
    "ns = {}\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "\n",
    "# Series of queries\n",
    "queries = [\n",
    "    \"What is prov:Activity?\",\n",
    "    \"What properties does prov:Activity have?\",\n",
    "    \"How are prov:Activity and prov:Entity related?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    answer, iterations, ns, new_memories = rlm_run_with_memory(\n",
    "        query,\n",
    "        ns['prov_meta'].summary(),\n",
    "        store,\n",
    "        ns=ns\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"Iterations: {len(iterations)}\")\n",
    "    print(f\"New memories extracted: {len(new_memories)}\")\n",
    "    for mem in new_memories:\n",
    "        print(f\"  - {mem.title}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final memory store: {len(store.memories)} memories\")\n",
    "print('='*60)\n",
    "\n",
    "# Show all memories with access counts\n",
    "for mem in store.memories:\n",
    "    print(f\"\\n{mem.title}\")\n",
    "    print(f\"  Source: {mem.source_type}\")\n",
    "    print(f\"  Accessed: {mem.access_count} times\")\n",
    "    print(f\"  Tags: {mem.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3101d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
