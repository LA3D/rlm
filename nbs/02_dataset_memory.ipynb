{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c289ec5",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> RDF Dataset-based session memory for RLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d335a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements RDF Dataset-based memory for RLM sessions using named graphs:\n",
    "\n",
    "- `onto/<name>` - Read-only ontology graphs\n",
    "- `mem` - Mutable working memory for current session\n",
    "- `prov` - Provenance/audit trail\n",
    "- `work/<task_id>` - Scratch graphs for intermediate results\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Session-scoped**: `mem` is working memory for current RLM run\n",
    "- **Handle-based access**: Model sees bounded views, never raw quads\n",
    "- **Provenance tracking**: All `mem` changes recorded with timestamp/source/reason\n",
    "- **Lazy indexing**: Caches invalidated on mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628ae7a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa851fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rdflib import Dataset, Graph, Namespace, RDF, RDFS, URIRef, Literal, XSD, BNode\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "\n",
    "# Helper function for URI/Literal conversion\n",
    "def _to_rdf_term(value):\n",
    "    \"\"\"Convert value to appropriate RDF term.\n",
    "    \n",
    "    Handles URIs (any scheme), literals, and existing RDF terms.\n",
    "    \"\"\"\n",
    "    # Already an RDF term\n",
    "    if isinstance(value, (URIRef, Literal, BNode)):\n",
    "        return value\n",
    "    \n",
    "    # String conversion\n",
    "    if isinstance(value, str):\n",
    "        # Check if it looks like a URI (has scheme like http:, urn:, https:, etc.)\n",
    "        # Exclude blank nodes (_:) which are handled separately\n",
    "        if ':' in value and not value.startswith('_:'):\n",
    "            # Could be URI - check if it's likely a URI vs a literal with colon\n",
    "            # Simple heuristic: if it starts with a known scheme or has :// it's a URI\n",
    "            if value.split(':', 1)[0].lower() in ['http', 'https', 'urn', 'ftp', 'mailto', 'file', 'data']:\n",
    "                return URIRef(value)\n",
    "            # Also handle URIs with ://\n",
    "            if '://' in value:\n",
    "                return URIRef(value)\n",
    "        # Otherwise treat as literal\n",
    "        return Literal(value)\n",
    "    \n",
    "    # Numbers and booleans become typed literals\n",
    "    if isinstance(value, (int, float, bool)):\n",
    "        return Literal(value)\n",
    "    \n",
    "    # Pass through anything else (shouldn't happen normally)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fe728",
   "metadata": {},
   "source": [
    "## DatasetMeta\n",
    "\n",
    "Meta-graph navigation for RDF Dataset with lazy-cached indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DatasetMeta:\n",
    "    \"\"\"Meta-graph navigation for RDF Dataset.\n",
    "    \n",
    "    Provides lazy-cached indexes and bounded views over named graphs.\n",
    "    Indexes are invalidated on any mutation to mem graph.\n",
    "    \"\"\"\n",
    "    dataset: Dataset\n",
    "    name: str = 'ds'\n",
    "    session_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n",
    "    \n",
    "    # Lazy-cached indexes (invalidated on mutation)\n",
    "    _graph_stats: dict = field(default=None, init=False, repr=False)\n",
    "    _mem_predicates: Counter = field(default=None, init=False, repr=False)\n",
    "    _version: int = field(default=0, init=False, repr=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize graph URIs.\"\"\"\n",
    "        self._mem_uri = URIRef(f'urn:rlm:{self.name}:mem')\n",
    "        self._prov_uri = URIRef(f'urn:rlm:{self.name}:prov')\n",
    "        \n",
    "        # Create mem and prov graphs if they don't exist\n",
    "        if (None, None, None, self._mem_uri) not in self.dataset:\n",
    "            self.dataset.graph(self._mem_uri)\n",
    "        if (None, None, None, self._prov_uri) not in self.dataset:\n",
    "            self.dataset.graph(self._prov_uri)\n",
    "    \n",
    "    @property\n",
    "    def mem(self) -> Graph:\n",
    "        \"\"\"Get working memory graph.\"\"\"\n",
    "        return self.dataset.graph(self._mem_uri)\n",
    "    \n",
    "    @property\n",
    "    def prov(self) -> Graph:\n",
    "        \"\"\"Get provenance graph.\"\"\"\n",
    "        return self.dataset.graph(self._prov_uri)\n",
    "    \n",
    "    @property\n",
    "    def graph_stats(self) -> dict:\n",
    "        \"\"\"Get statistics for all graphs (cached).\"\"\"\n",
    "        if self._graph_stats is None:\n",
    "            stats = {}\n",
    "            for ctx in self.dataset.graphs():\n",
    "                graph_uri = ctx.identifier\n",
    "                stats[str(graph_uri)] = len(ctx)\n",
    "            self._graph_stats = stats\n",
    "        return self._graph_stats\n",
    "    \n",
    "    @property\n",
    "    def work_graphs(self) -> list:\n",
    "        \"\"\"List all work/* scratch graphs.\"\"\"\n",
    "        work_prefix = f'urn:rlm:{self.name}:work/'\n",
    "        return [str(ctx.identifier) for ctx in self.dataset.graphs() \n",
    "                if str(ctx.identifier).startswith(work_prefix)]\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate summary of dataset.\"\"\"\n",
    "        lines = [\n",
    "            f\"Dataset '{self.name}' (session: {self.session_id})\",\n",
    "            f\"mem: {len(self.mem)} triples\",\n",
    "            f\"prov: {len(self.prov)} events\",\n",
    "            f\"work graphs: {len(self.work_graphs)}\",\n",
    "            f\"onto graphs: {len([g for g in self.graph_stats.keys() if ':onto/' in g])}\"  # FIX: Use :onto/ not /onto/\n",
    "        ]\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def _invalidate_caches(self):\n",
    "        \"\"\"Invalidate all cached indexes.\"\"\"\n",
    "        self._graph_stats = None\n",
    "        self._mem_predicates = None\n",
    "        self._version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588434eb",
   "metadata": {},
   "source": [
    "## Setup Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2dc997",
   "metadata": {},
   "source": [
    "## Memory Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_add(ds_meta: DatasetMeta, subject, predicate, obj, \n",
    "            source: str = 'agent', reason: str = None) -> str:\n",
    "    \"\"\"Add fact to mem with provenance tracking.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        subject: Subject URI or literal\n",
    "        predicate: Predicate URI\n",
    "        obj: Object URI or literal\n",
    "        source: Source of this fact (default: 'agent')\n",
    "        reason: Optional reason for adding\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    # Convert to RDF terms (handles all URI schemes: http, https, urn, etc.)\n",
    "    s = _to_rdf_term(subject)\n",
    "    p = URIRef(predicate) if isinstance(predicate, str) else predicate\n",
    "    o = _to_rdf_term(obj)\n",
    "    \n",
    "    # Add to mem\n",
    "    ds_meta.mem.add((s, p, o))\n",
    "    \n",
    "    # Record provenance\n",
    "    event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n",
    "    RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "    \n",
    "    ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.AddEvent))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.subject, s))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.predicate, p))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.object, o))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.now(timezone.utc).isoformat() + 'Z', datatype=XSD.dateTime)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n",
    "    \n",
    "    if reason:\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Added triple to mem: ({s}, {p}, {o})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_query(ds_meta: DatasetMeta, sparql: str, limit: int = 100) -> list:\n",
    "    \"\"\"Query mem graph, return bounded results.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        sparql: SPARQL query string\n",
    "        limit: Maximum results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of result rows (as dicts)\n",
    "    \"\"\"\n",
    "    # Inject LIMIT if not present\n",
    "    if 'LIMIT' not in sparql.upper():\n",
    "        sparql = sparql.rstrip() + f' LIMIT {limit}'\n",
    "    \n",
    "    results = ds_meta.mem.query(sparql)\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    rows = []\n",
    "    for row in results:\n",
    "        row_dict = {}\n",
    "        for var in results.vars:\n",
    "            row_dict[str(var)] = str(row[var]) if row[var] else None\n",
    "        rows.append(row_dict)\n",
    "    \n",
    "    return rows[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_retract(ds_meta: DatasetMeta, subject=None, predicate=None, obj=None,\n",
    "                source: str = 'agent', reason: str = None) -> str:\n",
    "    \"\"\"Remove triples with provenance.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        subject: Subject URI or None (wildcard)\n",
    "        predicate: Predicate URI or None (wildcard)\n",
    "        obj: Object URI/literal or None (wildcard)\n",
    "        source: Source of this retraction\n",
    "        reason: Optional reason for removing\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    # Convert to RDF terms or None (handles all URI schemes)\n",
    "    s = _to_rdf_term(subject) if subject is not None else None\n",
    "    p = URIRef(predicate) if predicate and isinstance(predicate, str) else predicate\n",
    "    o = _to_rdf_term(obj) if obj is not None else None\n",
    "    \n",
    "    # Find matching triples\n",
    "    to_remove = list(ds_meta.mem.triples((s, p, o)))\n",
    "    \n",
    "    # Remove each triple and record provenance\n",
    "    RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "    for triple in to_remove:\n",
    "        ds_meta.mem.remove(triple)\n",
    "        \n",
    "        # Record provenance\n",
    "        event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n",
    "        ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.RetractEvent))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.subject, triple[0]))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.predicate, triple[1]))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.object, triple[2]))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.now(timezone.utc).isoformat() + 'Z', datatype=XSD.dateTime)))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n",
    "        \n",
    "        if reason:\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Removed {len(to_remove)} triples from mem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_describe(ds_meta: DatasetMeta, uri: str, limit: int = 20) -> dict:\n",
    "    \"\"\"Get bounded entity description from mem.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        uri: URI of entity to describe\n",
    "        limit: Maximum triples to include\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'as_subject' and 'as_object' triple lists\n",
    "    \"\"\"\n",
    "    entity = URIRef(uri)\n",
    "    \n",
    "    # Get triples where entity is subject\n",
    "    as_subject = [(str(s), str(p), str(o)) for s, p, o in list(ds_meta.mem.triples((entity, None, None)))[:limit]]\n",
    "    \n",
    "    # Get triples where entity is object\n",
    "    as_object = [(str(s), str(p), str(o)) for s, p, o in list(ds_meta.mem.triples((None, None, entity)))[:limit]]\n",
    "    \n",
    "    return {\n",
    "        'uri': uri,\n",
    "        'as_subject': as_subject,\n",
    "        'as_object': as_object\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205f14d",
   "metadata": {},
   "source": [
    "## Scratch Graph Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89433477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_create(ds_meta: DatasetMeta, task_id: str = None) -> tuple:\n",
    "    \"\"\"Create a scratch graph for intermediate results.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Task identifier (default: auto-generated)\n",
    "        \n",
    "    Returns:\n",
    "        (graph_uri, graph) tuple\n",
    "    \"\"\"\n",
    "    if task_id is None:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "    graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    return (str(graph_uri), graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_cleanup(ds_meta: DatasetMeta, task_id: str = None, all: bool = False) -> str:\n",
    "    \"\"\"Remove scratch graph(s).\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Specific task to clean up, or None\n",
    "        all: If True, remove all work/* graphs\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    removed = 0\n",
    "    \n",
    "    if all:\n",
    "        for graph_uri in ds_meta.work_graphs:\n",
    "            ds_meta.dataset.remove_graph(URIRef(graph_uri))\n",
    "            removed += 1\n",
    "    elif task_id:\n",
    "        graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "        ds_meta.dataset.remove_graph(graph_uri)\n",
    "        removed = 1\n",
    "    \n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Removed {removed} work graph(s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_to_mem(ds_meta: DatasetMeta, task_id: str, \n",
    "                source: str = 'work', reason: str = None) -> str:\n",
    "    \"\"\"Promote triples from scratch graph to mem with provenance.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Task identifier for work graph\n",
    "        source: Source label for provenance\n",
    "        reason: Optional reason for promotion\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "    work_graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    # Get all triples from work graph\n",
    "    triples = list(work_graph.triples((None, None, None)))\n",
    "    \n",
    "    # Add each to mem\n",
    "    for s, p, o in triples:\n",
    "        ds_meta.mem.add((s, p, o))\n",
    "    \n",
    "    # Record single provenance event for the promotion\n",
    "    event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n",
    "    RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "    \n",
    "    ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.PromoteEvent))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.fromGraph, graph_uri))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.tripleCount, Literal(len(triples))))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.now(timezone.utc).isoformat() + 'Z', datatype=XSD.dateTime)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n",
    "    \n",
    "    if reason:\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Promoted {len(triples)} triples from work/{task_id} to mem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1399638",
   "metadata": {},
   "source": [
    "## Snapshot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def snapshot_dataset(ds_meta: DatasetMeta, path: str = None, \n",
    "                     format: str = 'trig') -> str:\n",
    "    \"\"\"Serialize dataset to TriG/N-Quads for debugging.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta to snapshot\n",
    "        path: Output path (default: auto-generated with timestamp)\n",
    "        format: 'trig' or 'nquads'\n",
    "        \n",
    "    Returns:\n",
    "        Path to snapshot file\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "        ext = 'trig' if format == 'trig' else 'nq'\n",
    "        path = f\"snapshot_{ds_meta.name}_{timestamp}.{ext}\"\n",
    "\n",
    "    # Ensure session_id is stored in prov graph\n",
    "    from rdflib import Namespace\n",
    "    RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "    session_uri = URIRef(f'urn:rlm:{ds_meta.name}:session')\n",
    "    ds_meta.prov.add((session_uri, RLM_PROV.sessionId, Literal(ds_meta.session_id)))\n",
    "\n",
    "    ds_meta.dataset.serialize(destination=path, format=format)\n",
    "\n",
    "    return f\"Snapshot saved to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_snapshot(path: str, ns: dict, name: str = 'ds') -> str:\n",
    "    \"\"\"Load dataset from TriG/N-Quads snapshot.\n",
    "    \n",
    "    Useful for debugging/replay. Note: The snapshot preserves the original\n",
    "    dataset name in graph URIs, so if you want to use the original name,\n",
    "    extract it from the graph URIs.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to snapshot file\n",
    "        ns: Namespace dict where Dataset will be stored\n",
    "        name: Variable name for the Dataset handle\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    # Detect format from extension\n",
    "    ext = Path(path).suffix.lower()\n",
    "    \n",
    "    # FIX: Properly distinguish Turtle from TriG\n",
    "    if ext == '.trig':\n",
    "        format = 'trig'\n",
    "    elif ext == '.ttl':\n",
    "        format = 'turtle'  # Turtle is single-graph, not TriG!\n",
    "    elif ext in ['.nq', '.nquads']:\n",
    "        format = 'nquads'\n",
    "    else:\n",
    "        # Default to trig for unknown extensions (datasets are multi-graph)\n",
    "        format = 'trig'\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = Dataset()\n",
    "    ds.parse(path, format=format)\n",
    "    \n",
    "    # Try to detect original name from graph URIs\n",
    "    original_name = None\n",
    "    for ctx in ds.graphs():\n",
    "        uri = str(ctx.identifier)\n",
    "        if ':mem' in uri:\n",
    "            # Extract name from urn:rlm:{name}:mem\n",
    "            parts = uri.split(':')\n",
    "            if len(parts) >= 3:\n",
    "                original_name = parts[2]\n",
    "                break\n",
    "    \n",
    "    # Use detected name or provided name\n",
    "    detected_name = original_name if original_name else name\n",
    "\n",
    "    # Try to restore session_id from provenance graph\n",
    "    from rdflib import URIRef\n",
    "    prov_uri = URIRef(f'urn:rlm:{detected_name}:prov')\n",
    "    session_id = None\n",
    "    try:\n",
    "        prov_graph = ds.graph(prov_uri)\n",
    "        # Try new format first (explicit sessionId triple)\n",
    "        query = \"\"\"SELECT ?session WHERE { ?s <urn:rlm:prov:sessionId> ?session } LIMIT 1\"\"\"\n",
    "        results = list(prov_graph.query(query))\n",
    "        if results:\n",
    "            session_id = str(results[0][0])\n",
    "        else:\n",
    "            # Fallback to old format (from prov events)\n",
    "            query = \"\"\"SELECT ?session WHERE { ?event <urn:rlm:prov:session> ?session } LIMIT 1\"\"\"\n",
    "            results = list(prov_graph.query(query))\n",
    "            if results:\n",
    "                session_id = str(results[0][0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Create meta with detected name and restored session_id if found\n",
    "    if session_id:\n",
    "        ds_meta = DatasetMeta(ds, name=detected_name, session_id=session_id)\n",
    "    else:\n",
    "        ds_meta = DatasetMeta(ds, name=detected_name)\n",
    "    \n",
    "    # Store in namespace with provided name\n",
    "    ns[name] = ds\n",
    "    ns[f\"{name}_meta\"] = ds_meta\n",
    "    \n",
    "    # Count graphs\n",
    "    graph_count = len(list(ds.graphs()))\n",
    "    \n",
    "    msg = f\"Loaded snapshot from {path}: {graph_count} graphs\"\n",
    "    if original_name and original_name != name:\n",
    "        msg += f\" (original name '{original_name}' preserved in graph URIs)\"\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436792f0",
   "metadata": {},
   "source": [
    "## Bounded View Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de767a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def res_head(result, n: int = 10) -> list:\n",
    "    \"\"\"Get first N rows of a result set.\n",
    "\n",
    "    Args:\n",
    "        result: ResultTable, list of dicts, or list of tuples\n",
    "        n: Number of rows to return\n",
    "\n",
    "    Returns:\n",
    "        List of rows (same format as input)\n",
    "    \"\"\"\n",
    "    if isinstance(result, ResultTable):\n",
    "        return result.rows[:n]\n",
    "    return result[:n]\n",
    "\n",
    "\n",
    "def res_where(result, column: str, pattern: str = None, value: str = None,\n",
    "              limit: int = 100) -> list:\n",
    "    \"\"\"Filter result rows by column value or regex pattern.\n",
    "\n",
    "    Args:\n",
    "        result: ResultTable or list of dicts\n",
    "        column: Column name to filter on\n",
    "        pattern: Optional regex pattern to match\n",
    "        value: Optional exact value to match\n",
    "        limit: Maximum matching rows to return (default: 100)\n",
    "\n",
    "    Returns:\n",
    "        List of matching rows\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    rows = result.rows if isinstance(result, ResultTable) else result\n",
    "    filtered = []\n",
    "\n",
    "    for row in rows:\n",
    "        # Stop if we've reached the limit\n",
    "        if len(filtered) >= limit:\n",
    "            break\n",
    "            \n",
    "        if column not in row:\n",
    "            continue\n",
    "\n",
    "        cell_value = str(row[column]) if row[column] is not None else ''\n",
    "\n",
    "        # Exact value match\n",
    "        if value is not None:\n",
    "            if cell_value == str(value):\n",
    "                filtered.append(row)\n",
    "\n",
    "        # Regex pattern match\n",
    "        elif pattern is not None:\n",
    "            if re.search(pattern, cell_value, re.IGNORECASE):\n",
    "                filtered.append(row)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def res_group(result, column: str, limit: int = 20) -> list:\n",
    "    \"\"\"Get counts grouped by column value.\n",
    "\n",
    "    Args:\n",
    "        result: ResultTable or list of dicts\n",
    "        column: Column to group by\n",
    "        limit: Maximum groups to return\n",
    "\n",
    "    Returns:\n",
    "        List of (value, count) tuples, sorted by count descending\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    rows = result.rows if isinstance(result, ResultTable) else result\n",
    "    values = [str(row[column]) for row in rows if column in row and row[column] is not None]\n",
    "\n",
    "    counts = Counter(values)\n",
    "    return counts.most_common(limit)\n",
    "\n",
    "\n",
    "def res_distinct(result, column: str, limit: int = 50) -> list:\n",
    "    \"\"\"Get distinct values in a column.\n",
    "\n",
    "    Args:\n",
    "        result: ResultTable or list of dicts\n",
    "        column: Column to get distinct values from\n",
    "        limit: Maximum distinct values to return\n",
    "\n",
    "    Returns:\n",
    "        List of distinct values\n",
    "    \"\"\"\n",
    "    rows = result.rows if isinstance(result, ResultTable) else result\n",
    "    distinct_values = set()\n",
    "\n",
    "    for row in rows:\n",
    "        if column in row and row[column] is not None:\n",
    "            distinct_values.add(str(row[column]))\n",
    "\n",
    "        if len(distinct_values) >= limit:\n",
    "            break\n",
    "\n",
    "    return sorted(list(distinct_values))[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ResultTable:\n",
    "    \"\"\"Wrapper for SPARQL query results with bounded view operations.\"\"\"\n",
    "    rows: list          # list of dicts\n",
    "    columns: list       # column names\n",
    "    query: str          # Original SPARQL query\n",
    "    total_rows: int     # Total before limit\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of rows.\"\"\"\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation.\"\"\"\n",
    "        return f\"ResultTable({len(self.rows)} rows, columns={self.columns})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b460",
   "metadata": {},
   "source": [
    "## Result Table Views (Stage 2)\n",
    "\n",
    "Bounded view operations over SPARQL query results enable iterative exploration without overwhelming context:\n",
    "\n",
    "- **res_head()**: Preview first N rows\n",
    "- **res_where()**: Filter by column value or regex\n",
    "- **res_group()**: Aggregate and count by column\n",
    "- **res_distinct()**: Find unique values\n",
    "\n",
    "These work with `ResultTable` wrapper or plain list-of-dicts from `mem_query()`.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Previewing large result sets: `res_head(results, 10)`\n",
    "- Finding specific entities: `res_where(results, 'name', pattern='Alice')`\n",
    "- Understanding data distribution: `res_group(results, 'category')`\n",
    "- Exploring unique values: `res_distinct(results, 'author')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_stats(ds_meta: DatasetMeta) -> str:\n",
    "    \"\"\"Get dataset statistics summary.\"\"\"\n",
    "    return ds_meta.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_graphs(ds_meta: DatasetMeta, pattern: str = None) -> list:\n",
    "    \"\"\"List named graphs, optionally filtered.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        pattern: Optional substring to filter graph URIs\n",
    "        \n",
    "    Returns:\n",
    "        List of (graph_uri, triple_count) tuples\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    for ctx in ds_meta.dataset.graphs():\n",
    "        uri = str(ctx.identifier)\n",
    "        if pattern is None or pattern in uri:\n",
    "            graphs.append((uri, len(ctx)))\n",
    "    \n",
    "    return sorted(graphs, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6914ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def graph_sample(ds_meta: DatasetMeta, graph_uri: str, limit: int = 10) -> list:\n",
    "    \"\"\"Get sample triples from a graph.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        graph_uri: URI of graph to sample\n",
    "        limit: Maximum triples to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (s, p, o) tuples as strings\n",
    "    \"\"\"\n",
    "    graph = ds_meta.dataset.graph(URIRef(graph_uri))\n",
    "    triples = [(str(s), str(p), str(o)) for s, p, o in list(graph.triples((None, None, None)))[:limit]]\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a752c1a",
   "metadata": {},
   "source": [
    "## Ontology Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb690227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mount_ontology(ds_meta: DatasetMeta, ns: dict, path: str, ont_name: str, \n",
    "                   index_shacl: bool = True, index_queries: bool = True) -> str:\n",
    "    \"\"\"Mount ontology into dataset as read-only onto/<name> graph.\n",
    "    \n",
    "    If index_shacl=True and SHACL content detected, also builds\n",
    "    SHACLIndex and stores in ns['{ont_name}_shacl'].\n",
    "    \n",
    "    If index_queries=True and sh:SPARQLExecutable detected, also builds\n",
    "    QueryIndex and stores in ns['{ont_name}_queries'].\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        ns: Namespace dict (for compatibility with setup_ontology_context)\n",
    "        path: Path to ontology file\n",
    "        ont_name: Name for the ontology\n",
    "        index_shacl: Whether to detect and index SHACL shapes (default: True)\n",
    "        index_queries: Whether to detect and index query templates (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:onto/{ont_name}')\n",
    "    graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    # Parse ontology into the graph\n",
    "    graph.parse(path)\n",
    "    \n",
    "    result = f\"Mounted {len(graph)} triples from {Path(path).name} into onto/{ont_name}\"\n",
    "    \n",
    "    # Optional SHACL indexing\n",
    "    if index_shacl:\n",
    "        try:\n",
    "            from rlm.shacl_examples import detect_shacl, build_shacl_index\n",
    "            detection = detect_shacl(graph)\n",
    "            if detection['has_shacl']:\n",
    "                index = build_shacl_index(graph)\n",
    "                ns[f'{ont_name}_shacl'] = index\n",
    "                result += f\"\\n  SHACL: {index.summary()}\"\n",
    "        except ImportError:\n",
    "            pass  # shacl_examples not available\n",
    "    \n",
    "    # Optional query template indexing\n",
    "    if index_queries:\n",
    "        try:\n",
    "            from rlm.shacl_examples import detect_sparql_executables, build_query_index\n",
    "            detection = detect_sparql_executables(graph)\n",
    "            if detection['has_executables']:\n",
    "                index = build_query_index(graph, str(path))\n",
    "                ns[f'{ont_name}_queries'] = index\n",
    "                result += f\"\\n  Queries: {index.summary()}\"\n",
    "        except ImportError:\n",
    "            pass  # shacl_examples not available\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d52332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_dataset_context(ns: dict, name: str = 'ds') -> str:\n",
    "    \"\"\"Initialize Dataset with mem/prov graphs, bind helper functions.\n",
    "    \n",
    "    Args:\n",
    "        ns: Namespace dict where Dataset will be stored\n",
    "        name: Variable name for the Dataset handle\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing what was created\n",
    "    \"\"\"\n",
    "    # Create dataset and meta\n",
    "    ds = Dataset()\n",
    "    ds_meta = DatasetMeta(ds, name=name)\n",
    "    \n",
    "    # Store in namespace\n",
    "    ns[name] = ds\n",
    "    ns[f\"{name}_meta\"] = ds_meta\n",
    "    \n",
    "    # Bind helper functions\n",
    "    ns['mem_add'] = partial(mem_add, ds_meta)\n",
    "    ns['mem_query'] = partial(mem_query, ds_meta)\n",
    "    ns['mem_retract'] = partial(mem_retract, ds_meta)\n",
    "    ns['mem_describe'] = partial(mem_describe, ds_meta)\n",
    "    ns['dataset_stats'] = partial(dataset_stats, ds_meta)\n",
    "    ns['work_create'] = partial(work_create, ds_meta)\n",
    "    ns['work_cleanup'] = partial(work_cleanup, ds_meta)\n",
    "    ns['work_to_mem'] = partial(work_to_mem, ds_meta)\n",
    "    ns['snapshot_dataset'] = partial(snapshot_dataset, ds_meta)\n",
    "    ns['mount_ontology'] = partial(mount_ontology, ds_meta, ns)\n",
    "    ns['list_graphs'] = partial(list_graphs, ds_meta)\n",
    "    ns['graph_sample'] = partial(graph_sample, ds_meta)\n",
    "    \n",
    "    # NEW: Bind Stage 2 result table view functions\n",
    "    ns['res_head'] = res_head\n",
    "    ns['res_where'] = res_where\n",
    "    ns['res_group'] = res_group\n",
    "    ns['res_distinct'] = res_distinct\n",
    "    \n",
    "    return f\"Created dataset '{name}' with session_id={ds_meta.session_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ res_head works: 2 rows\n",
      "✓ res_where (exact) works: 2 rows with age=30\n",
      "✓ res_where (pattern) works: 1 rows matching 'alice'\n",
      "✓ res_group works: [('30', 2), ('25', 1)]\n",
      "✓ res_distinct works: ['25', '30']\n",
      "✓ ResultTable works: ResultTable(3 rows, columns=['s', 'age'])\n",
      "✓ res_head works with ResultTable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test result table views\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Add test data\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "mem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\n",
    "mem_add(ds_meta, 'http://ex.org/charlie', 'http://ex.org/age', '30')\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/city', 'Boston')\n",
    "mem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/city', 'NYC')\n",
    "\n",
    "# Query and get results as list\n",
    "results = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }')\n",
    "\n",
    "# Test res_head\n",
    "head = res_head(results, n=2)\n",
    "assert len(head) == 2\n",
    "print(f\"✓ res_head works: {len(head)} rows\")\n",
    "\n",
    "# Test res_where with exact value\n",
    "filtered = res_where(results, 'age', value='30')\n",
    "assert len(filtered) == 2\n",
    "print(f\"✓ res_where (exact) works: {len(filtered)} rows with age=30\")\n",
    "\n",
    "# Test res_where with pattern\n",
    "filtered_pattern = res_where(results, 's', pattern='alice')\n",
    "assert len(filtered_pattern) == 1\n",
    "print(f\"✓ res_where (pattern) works: {len(filtered_pattern)} rows matching 'alice'\")\n",
    "\n",
    "# Test res_group\n",
    "groups = res_group(results, 'age')\n",
    "assert len(groups) == 2  # Two distinct ages\n",
    "assert groups[0][1] == 2  # Age '30' appears twice\n",
    "print(f\"✓ res_group works: {groups}\")\n",
    "\n",
    "# Test res_distinct\n",
    "distinct_ages = res_distinct(results, 'age')\n",
    "assert len(distinct_ages) == 2\n",
    "assert '25' in distinct_ages and '30' in distinct_ages\n",
    "print(f\"✓ res_distinct works: {distinct_ages}\")\n",
    "\n",
    "# Test ResultTable wrapper\n",
    "result_table = ResultTable(\n",
    "    rows=results,\n",
    "    columns=['s', 'age'],\n",
    "    query='SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }',\n",
    "    total_rows=len(results)\n",
    ")\n",
    "assert len(result_table) == 3\n",
    "print(f\"✓ ResultTable works: {result_table}\")\n",
    "\n",
    "# Test result table views work with ResultTable\n",
    "head_from_table = res_head(result_table, n=2)\n",
    "assert len(head_from_table) == 2\n",
    "print(f\"✓ res_head works with ResultTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500465ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset creation works\n",
      "Created dataset 'test_ds' with session_id=cb576fb5\n"
     ]
    }
   ],
   "source": [
    "# Test dataset creation\n",
    "test_ns = {}\n",
    "result = setup_dataset_context(test_ns, name='test_ds')\n",
    "assert 'test_ds' in test_ns\n",
    "assert 'test_ds_meta' in test_ns\n",
    "assert len(test_ns['test_ds_meta'].session_id) == 8\n",
    "print(\"✓ Dataset creation works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e9cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mem_add works\n",
      "Added triple to mem: (http://ex.org/alice, http://ex.org/knows, http://ex.org/bob)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test mem_add with provenance\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "result = mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob', \n",
    "                 source='test', reason='Testing')\n",
    "assert len(ds_meta.mem) == 1\n",
    "assert len(ds_meta.prov) > 0\n",
    "print(\"✓ mem_add works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722addb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mem_query works\n",
      "[{'s': 'http://ex.org/alice', 'age': '30'}, {'s': 'http://ex.org/bob', 'age': '25'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test mem_query\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "mem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\n",
    "\n",
    "results = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }')\n",
    "assert len(results) == 2\n",
    "assert all('s' in r and 'age' in r for r in results)\n",
    "print(\"✓ mem_query works\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mem_retract works\n",
      "Removed 1 triples from mem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n",
      "<ipython-input-1-e3f77e94d507>:36: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test mem_retract\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "assert len(ds_meta.mem) == 1\n",
    "\n",
    "result = mem_retract(ds_meta, predicate='http://ex.org/age', source='test', reason='Correction')\n",
    "assert len(ds_meta.mem) == 0\n",
    "assert 'Removed 1 triples' in result\n",
    "print(\"✓ mem_retract works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mem_describe works\n",
      "{'uri': 'http://ex.org/alice', 'as_subject': [('http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob'), ('http://ex.org/alice', 'http://ex.org/age', '30')], 'as_object': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test mem_describe\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "\n",
    "desc = mem_describe(ds_meta, 'http://ex.org/alice')\n",
    "assert 'as_subject' in desc\n",
    "assert 'as_object' in desc\n",
    "assert len(desc['as_subject']) == 2\n",
    "print(\"✓ mem_describe works\")\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde065d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Index invalidation works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-338468221890>:44: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  for ctx in self.dataset.contexts():\n",
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test index invalidation\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Access cached property\n",
    "initial_version = ds_meta._version\n",
    "_ = ds_meta.graph_stats\n",
    "\n",
    "# Mutate\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "\n",
    "# Check version incremented\n",
    "assert ds_meta._version > initial_version\n",
    "print(\"✓ Index invalidation works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Work graph lifecycle works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-338468221890>:54: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  return [str(ctx.identifier) for ctx in self.dataset.contexts()\n",
      "<ipython-input-1-661dda18a793>:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n"
     ]
    }
   ],
   "source": [
    "# Test work graph lifecycle\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Create work graph\n",
    "uri, graph = work_create(ds_meta, task_id='test_task')\n",
    "assert 'work/test_task' in uri\n",
    "assert len(ds_meta.work_graphs) == 1\n",
    "\n",
    "# Add some triples to work graph\n",
    "graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/temp'), Literal('value')))\n",
    "assert len(graph) == 1\n",
    "\n",
    "# Promote to mem\n",
    "result = work_to_mem(ds_meta, 'test_task', reason='Test promotion')\n",
    "assert len(ds_meta.mem) == 1\n",
    "assert 'Promoted 1 triples' in result\n",
    "\n",
    "# Cleanup\n",
    "result = work_cleanup(ds_meta, task_id='test_task')\n",
    "assert 'Removed 1 work' in result\n",
    "assert len(ds_meta.work_graphs) == 0\n",
    "\n",
    "print(\"✓ Work graph lifecycle works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f29c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Snapshot roundtrip works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n",
      "<ipython-input-1-40048cec62e5>:37: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  for ctx in ds.contexts():\n",
      "<ipython-input-1-40048cec62e5>:80: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  graph_count = len(list(ds.contexts()))\n"
     ]
    }
   ],
   "source": [
    "# Test snapshot\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Add some data\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "\n",
    "# Take snapshot\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.trig', delete=False) as f:\n",
    "    snapshot_path = f.name\n",
    "\n",
    "result = snapshot_dataset(ds_meta, path=snapshot_path)\n",
    "assert os.path.exists(snapshot_path)\n",
    "assert 'Snapshot saved' in result\n",
    "\n",
    "# Load snapshot (let it auto-detect the name 'ds' from graph URIs)\n",
    "test_ns2 = {}\n",
    "result = load_snapshot(snapshot_path, test_ns2, name='restored')\n",
    "assert 'restored' in test_ns2\n",
    "assert 'restored_meta' in test_ns2\n",
    "# Should auto-detect original name 'ds' and use it for URIs\n",
    "assert len(test_ns2['restored_meta'].mem) == 1\n",
    "\n",
    "# Also test loading with same name\n",
    "test_ns3 = {}\n",
    "result = load_snapshot(snapshot_path, test_ns3, name='ds')\n",
    "assert 'ds' in test_ns3\n",
    "assert 'ds_meta' in test_ns3\n",
    "assert len(test_ns3['ds_meta'].mem) == 1\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(snapshot_path)\n",
    "\n",
    "print(\"✓ Snapshot roundtrip works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bounded view functions work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-3a8dafc08295>:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n",
      "<ipython-input-1-338468221890>:54: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  return [str(ctx.identifier) for ctx in self.dataset.contexts()\n",
      "<ipython-input-1-338468221890>:44: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  for ctx in self.dataset.contexts():\n",
      "<ipython-input-1-0d2d1cee68f5>:13: DeprecationWarning: Dataset.contexts is deprecated, use Dataset.graphs instead.\n",
      "  for ctx in ds_meta.dataset.contexts():\n"
     ]
    }
   ],
   "source": [
    "# Test bounded view functions\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Add some data\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "work_create(ds_meta, 'task1')\n",
    "work_create(ds_meta, 'task2')\n",
    "\n",
    "# Test dataset_stats\n",
    "stats = dataset_stats(ds_meta)\n",
    "assert 'mem: 1 triples' in stats\n",
    "assert 'work graphs: 2' in stats\n",
    "\n",
    "# Test list_graphs\n",
    "graphs = list_graphs(ds_meta)\n",
    "assert len(graphs) >= 4  # mem, prov, work/task1, work/task2\n",
    "\n",
    "# Test list_graphs with pattern\n",
    "work_graphs = list_graphs(ds_meta, pattern='work/')\n",
    "assert len(work_graphs) == 2\n",
    "\n",
    "# Test graph_sample\n",
    "mem_uri = f'urn:rlm:{ds_meta.name}:mem'\n",
    "sample = graph_sample(ds_meta, mem_uri)\n",
    "assert len(sample) == 1\n",
    "\n",
    "print(\"✓ Bounded view functions work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a61bee",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d505f",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Basic usage in RLM context\n",
    "ns = {}\n",
    "setup_dataset_context(ns)\n",
    "\n",
    "# RLM can now use: mem_add, mem_query, mem_describe, etc.\n",
    "ns['mem_add']('http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\n",
    "results = ns['mem_query']('SELECT ?s ?p ?o WHERE { ?s ?p ?o }')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d832d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Integration with ontology\n",
    "from rlm.ontology import setup_ontology_context\n",
    "\n",
    "ns = {}\n",
    "setup_dataset_context(ns)\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "\n",
    "# Mount ontology into dataset\n",
    "ns['mount_ontology']('ontology/prov.ttl', 'prov')\n",
    "\n",
    "# Now ontology is in dataset as onto/prov graph\n",
    "graphs = ns['list_graphs']()\n",
    "print(graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
