{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> RDF Dataset-based session memory for RLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements RDF Dataset-based memory for RLM sessions using named graphs:\n",
    "\n",
    "- `onto/<name>` - Read-only ontology graphs\n",
    "- `mem` - Mutable working memory for current session\n",
    "- `prov` - Provenance/audit trail\n",
    "- `work/<task_id>` - Scratch graphs for intermediate results\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Session-scoped**: `mem` is working memory for current RLM run\n",
    "- **Handle-based access**: Model sees bounded views, never raw quads\n",
    "- **Provenance tracking**: All `mem` changes recorded with timestamp/source/reason\n",
    "- **Lazy indexing**: Caches invalidated on mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom rdflib import Dataset, Graph, Namespace, RDF, RDFS, URIRef, Literal, XSD, BNode\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom datetime import datetime\nimport uuid\n\n# Helper function for URI/Literal conversion\ndef _to_rdf_term(value):\n    \"\"\"Convert value to appropriate RDF term.\n    \n    Handles URIs (any scheme), literals, and existing RDF terms.\n    \"\"\"\n    # Already an RDF term\n    if isinstance(value, (URIRef, Literal, BNode)):\n        return value\n    \n    # String conversion\n    if isinstance(value, str):\n        # Check if it looks like a URI (has scheme like http:, urn:, https:, etc.)\n        # Exclude blank nodes (_:) which are handled separately\n        if ':' in value and not value.startswith('_:'):\n            # Could be URI - check if it's likely a URI vs a literal with colon\n            # Simple heuristic: if it starts with a known scheme or has :// it's a URI\n            if value.split(':', 1)[0].lower() in ['http', 'https', 'urn', 'ftp', 'mailto', 'file', 'data']:\n                return URIRef(value)\n            # Also handle URIs with ://\n            if '://' in value:\n                return URIRef(value)\n        # Otherwise treat as literal\n        return Literal(value)\n    \n    # Numbers and booleans become typed literals\n    if isinstance(value, (int, float, bool)):\n        return Literal(value)\n    \n    # Pass through anything else (shouldn't happen normally)\n    return value"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetMeta\n",
    "\n",
    "Meta-graph navigation for RDF Dataset with lazy-cached indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\n@dataclass\nclass DatasetMeta:\n    \"\"\"Meta-graph navigation for RDF Dataset.\n    \n    Provides lazy-cached indexes and bounded views over named graphs.\n    Indexes are invalidated on any mutation to mem graph.\n    \"\"\"\n    dataset: Dataset\n    name: str = 'ds'\n    session_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])\n    \n    # Lazy-cached indexes (invalidated on mutation)\n    _graph_stats: dict = field(default=None, init=False, repr=False)\n    _mem_predicates: Counter = field(default=None, init=False, repr=False)\n    _version: int = field(default=0, init=False, repr=False)\n    \n    def __post_init__(self):\n        \"\"\"Initialize graph URIs.\"\"\"\n        self._mem_uri = URIRef(f'urn:rlm:{self.name}:mem')\n        self._prov_uri = URIRef(f'urn:rlm:{self.name}:prov')\n        \n        # Create mem and prov graphs if they don't exist\n        if (None, None, None, self._mem_uri) not in self.dataset:\n            self.dataset.graph(self._mem_uri)\n        if (None, None, None, self._prov_uri) not in self.dataset:\n            self.dataset.graph(self._prov_uri)\n    \n    @property\n    def mem(self) -> Graph:\n        \"\"\"Get working memory graph.\"\"\"\n        return self.dataset.graph(self._mem_uri)\n    \n    @property\n    def prov(self) -> Graph:\n        \"\"\"Get provenance graph.\"\"\"\n        return self.dataset.graph(self._prov_uri)\n    \n    @property\n    def graph_stats(self) -> dict:\n        \"\"\"Get statistics for all graphs (cached).\"\"\"\n        if self._graph_stats is None:\n            stats = {}\n            for ctx in self.dataset.contexts():\n                graph_uri = ctx.identifier\n                stats[str(graph_uri)] = len(ctx)\n            self._graph_stats = stats\n        return self._graph_stats\n    \n    @property\n    def work_graphs(self) -> list:\n        \"\"\"List all work/* scratch graphs.\"\"\"\n        work_prefix = f'urn:rlm:{self.name}:work/'\n        return [str(ctx.identifier) for ctx in self.dataset.contexts() \n                if str(ctx.identifier).startswith(work_prefix)]\n    \n    def summary(self) -> str:\n        \"\"\"Generate summary of dataset.\"\"\"\n        lines = [\n            f\"Dataset '{self.name}' (session: {self.session_id})\",\n            f\"mem: {len(self.mem)} triples\",\n            f\"prov: {len(self.prov)} events\",\n            f\"work graphs: {len(self.work_graphs)}\",\n            f\"onto graphs: {len([g for g in self.graph_stats.keys() if ':onto/' in g])}\"  # FIX: Use :onto/ not /onto/\n        ]\n        return '\\n'.join(lines)\n    \n    def _invalidate_caches(self):\n        \"\"\"Invalidate all cached indexes.\"\"\"\n        self._graph_stats = None\n        self._mem_predicates = None\n        self._version += 1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef mem_add(ds_meta: DatasetMeta, subject, predicate, obj, \n            source: str = 'agent', reason: str = None) -> str:\n    \"\"\"Add fact to mem with provenance tracking.\n    \n    Args:\n        ds_meta: DatasetMeta containing the dataset\n        subject: Subject URI or literal\n        predicate: Predicate URI\n        obj: Object URI or literal\n        source: Source of this fact (default: 'agent')\n        reason: Optional reason for adding\n        \n    Returns:\n        Summary string\n    \"\"\"\n    # Convert to RDF terms (handles all URI schemes: http, https, urn, etc.)\n    s = _to_rdf_term(subject)\n    p = URIRef(predicate) if isinstance(predicate, str) else predicate\n    o = _to_rdf_term(obj)\n    \n    # Add to mem\n    ds_meta.mem.add((s, p, o))\n    \n    # Record provenance\n    event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n    RLM_PROV = Namespace('urn:rlm:prov:')\n    \n    ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.AddEvent))\n    ds_meta.prov.add((event_uri, RLM_PROV.subject, s))\n    ds_meta.prov.add((event_uri, RLM_PROV.predicate, p))\n    ds_meta.prov.add((event_uri, RLM_PROV.object, o))\n    ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n    ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n    ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n    \n    if reason:\n        ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n    \n    # Invalidate caches\n    ds_meta._invalidate_caches()\n    \n    return f\"Added triple to mem: ({s}, {p}, {o})\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_query(ds_meta: DatasetMeta, sparql: str, limit: int = 100) -> list:\n",
    "    \"\"\"Query mem graph, return bounded results.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        sparql: SPARQL query string\n",
    "        limit: Maximum results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of result rows (as dicts)\n",
    "    \"\"\"\n",
    "    # Inject LIMIT if not present\n",
    "    if 'LIMIT' not in sparql.upper():\n",
    "        sparql = sparql.rstrip() + f' LIMIT {limit}'\n",
    "    \n",
    "    results = ds_meta.mem.query(sparql)\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    rows = []\n",
    "    for row in results:\n",
    "        row_dict = {}\n",
    "        for var in results.vars:\n",
    "            row_dict[str(var)] = str(row[var]) if row[var] else None\n",
    "        rows.append(row_dict)\n",
    "    \n",
    "    return rows[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef mem_retract(ds_meta: DatasetMeta, subject=None, predicate=None, obj=None,\n                source: str = 'agent', reason: str = None) -> str:\n    \"\"\"Remove triples with provenance.\n    \n    Args:\n        ds_meta: DatasetMeta containing the dataset\n        subject: Subject URI or None (wildcard)\n        predicate: Predicate URI or None (wildcard)\n        obj: Object URI/literal or None (wildcard)\n        source: Source of this retraction\n        reason: Optional reason for removing\n        \n    Returns:\n        Summary string\n    \"\"\"\n    # Convert to RDF terms or None (handles all URI schemes)\n    s = _to_rdf_term(subject) if subject is not None else None\n    p = URIRef(predicate) if predicate and isinstance(predicate, str) else predicate\n    o = _to_rdf_term(obj) if obj is not None else None\n    \n    # Find matching triples\n    to_remove = list(ds_meta.mem.triples((s, p, o)))\n    \n    # Remove each triple and record provenance\n    RLM_PROV = Namespace('urn:rlm:prov:')\n    for triple in to_remove:\n        ds_meta.mem.remove(triple)\n        \n        # Record provenance\n        event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n        ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.RetractEvent))\n        ds_meta.prov.add((event_uri, RLM_PROV.subject, triple[0]))\n        ds_meta.prov.add((event_uri, RLM_PROV.predicate, triple[1]))\n        ds_meta.prov.add((event_uri, RLM_PROV.object, triple[2]))\n        ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n        ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n        ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n        \n        if reason:\n            ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n    \n    # Invalidate caches\n    ds_meta._invalidate_caches()\n    \n    return f\"Removed {len(to_remove)} triples from mem\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mem_describe(ds_meta: DatasetMeta, uri: str, limit: int = 20) -> dict:\n",
    "    \"\"\"Get bounded entity description from mem.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        uri: URI of entity to describe\n",
    "        limit: Maximum triples to include\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'as_subject' and 'as_object' triple lists\n",
    "    \"\"\"\n",
    "    entity = URIRef(uri)\n",
    "    \n",
    "    # Get triples where entity is subject\n",
    "    as_subject = [(str(s), str(p), str(o)) for s, p, o in list(ds_meta.mem.triples((entity, None, None)))[:limit]]\n",
    "    \n",
    "    # Get triples where entity is object\n",
    "    as_object = [(str(s), str(p), str(o)) for s, p, o in list(ds_meta.mem.triples((None, None, entity)))[:limit]]\n",
    "    \n",
    "    return {\n",
    "        'uri': uri,\n",
    "        'as_subject': as_subject,\n",
    "        'as_object': as_object\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Graph Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_create(ds_meta: DatasetMeta, task_id: str = None) -> tuple:\n",
    "    \"\"\"Create a scratch graph for intermediate results.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Task identifier (default: auto-generated)\n",
    "        \n",
    "    Returns:\n",
    "        (graph_uri, graph) tuple\n",
    "    \"\"\"\n",
    "    if task_id is None:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "    graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    return (str(graph_uri), graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_cleanup(ds_meta: DatasetMeta, task_id: str = None, all: bool = False) -> str:\n",
    "    \"\"\"Remove scratch graph(s).\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Specific task to clean up, or None\n",
    "        all: If True, remove all work/* graphs\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    removed = 0\n",
    "    \n",
    "    if all:\n",
    "        for graph_uri in ds_meta.work_graphs:\n",
    "            ds_meta.dataset.remove_graph(URIRef(graph_uri))\n",
    "            removed += 1\n",
    "    elif task_id:\n",
    "        graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "        ds_meta.dataset.remove_graph(graph_uri)\n",
    "        removed = 1\n",
    "    \n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Removed {removed} work graph(s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def work_to_mem(ds_meta: DatasetMeta, task_id: str, \n",
    "                source: str = 'work', reason: str = None) -> str:\n",
    "    \"\"\"Promote triples from scratch graph to mem with provenance.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        task_id: Task identifier for work graph\n",
    "        source: Source label for provenance\n",
    "        reason: Optional reason for promotion\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:work/{task_id}')\n",
    "    work_graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    # Get all triples from work graph\n",
    "    triples = list(work_graph.triples((None, None, None)))\n",
    "    \n",
    "    # Add each to mem\n",
    "    for s, p, o in triples:\n",
    "        ds_meta.mem.add((s, p, o))\n",
    "    \n",
    "    # Record single provenance event for the promotion\n",
    "    event_uri = URIRef(f'urn:rlm:prov:event_{uuid.uuid4().hex[:8]}')\n",
    "    RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "    \n",
    "    ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.PromoteEvent))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.fromGraph, graph_uri))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.tripleCount, Literal(len(triples))))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(datetime.utcnow().isoformat() + 'Z', datatype=XSD.dateTime)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.source, Literal(source)))\n",
    "    ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n",
    "    \n",
    "    if reason:\n",
    "        ds_meta.prov.add((event_uri, RLM_PROV.reason, Literal(reason)))\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Promoted {len(triples)} triples from work/{task_id} to mem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def snapshot_dataset(ds_meta: DatasetMeta, path: str = None, \n",
    "                     format: str = 'trig') -> str:\n",
    "    \"\"\"Serialize dataset to TriG/N-Quads for debugging.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta to snapshot\n",
    "        path: Output path (default: auto-generated with timestamp)\n",
    "        format: 'trig' or 'nquads'\n",
    "        \n",
    "    Returns:\n",
    "        Path to snapshot file\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "        ext = 'trig' if format == 'trig' else 'nq'\n",
    "        path = f\"snapshot_{ds_meta.name}_{timestamp}.{ext}\"\n",
    "    \n",
    "    ds_meta.dataset.serialize(destination=path, format=format)\n",
    "    \n",
    "    return f\"Snapshot saved to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef load_snapshot(path: str, ns: dict, name: str = 'ds') -> str:\n    \"\"\"Load dataset from TriG/N-Quads snapshot.\n    \n    Useful for debugging/replay. Note: The snapshot preserves the original\n    dataset name in graph URIs, so if you want to use the original name,\n    extract it from the graph URIs.\n    \n    Args:\n        path: Path to snapshot file\n        ns: Namespace dict where Dataset will be stored\n        name: Variable name for the Dataset handle\n        \n    Returns:\n        Summary string\n    \"\"\"\n    # Detect format from extension\n    ext = Path(path).suffix.lower()\n    \n    # FIX: Properly distinguish Turtle from TriG\n    if ext == '.trig':\n        format = 'trig'\n    elif ext == '.ttl':\n        format = 'turtle'  # Turtle is single-graph, not TriG!\n    elif ext in ['.nq', '.nquads']:\n        format = 'nquads'\n    else:\n        # Default to trig for unknown extensions (datasets are multi-graph)\n        format = 'trig'\n    \n    # Load dataset\n    ds = Dataset()\n    ds.parse(path, format=format)\n    \n    # Try to detect original name from graph URIs\n    original_name = None\n    for ctx in ds.contexts():\n        uri = str(ctx.identifier)\n        if ':mem' in uri:\n            # Extract name from urn:rlm:{name}:mem\n            parts = uri.split(':')\n            if len(parts) >= 3:\n                original_name = parts[2]\n                break\n    \n    # Use detected name or provided name\n    detected_name = original_name if original_name else name\n    \n    # Create meta with detected name so URIs match\n    ds_meta = DatasetMeta(ds, name=detected_name)\n    \n    # Store in namespace with provided name\n    ns[name] = ds\n    ns[f\"{name}_meta\"] = ds_meta\n    \n    # Count graphs\n    graph_count = len(list(ds.contexts()))\n    \n    msg = f\"Loaded snapshot from {path}: {graph_count} graphs\"\n    if original_name and original_name != name:\n        msg += f\" (original name '{original_name}' preserved in graph URIs)\"\n    \n    return msg"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounded View Functions"
   ]
  },
  {
   "cell_type": "code",
   "source": "#| export\ndef res_head(result, n: int = 10) -> list:\n    \"\"\"Get first N rows of a result set.\n\n    Args:\n        result: ResultTable, list of dicts, or list of tuples\n        n: Number of rows to return\n\n    Returns:\n        List of rows (same format as input)\n    \"\"\"\n    if isinstance(result, ResultTable):\n        return result.rows[:n]\n    return result[:n]\n\n\ndef res_where(result, column: str, pattern: str = None, value: str = None) -> list:\n    \"\"\"Filter result rows by column value or regex pattern.\n\n    Args:\n        result: ResultTable or list of dicts\n        column: Column name to filter on\n        pattern: Optional regex pattern to match\n        value: Optional exact value to match\n\n    Returns:\n        List of matching rows\n    \"\"\"\n    import re\n\n    rows = result.rows if isinstance(result, ResultTable) else result\n    filtered = []\n\n    for row in rows:\n        if column not in row:\n            continue\n\n        cell_value = str(row[column]) if row[column] is not None else ''\n\n        # Exact value match\n        if value is not None:\n            if cell_value == str(value):\n                filtered.append(row)\n\n        # Regex pattern match\n        elif pattern is not None:\n            if re.search(pattern, cell_value, re.IGNORECASE):\n                filtered.append(row)\n\n    return filtered\n\n\ndef res_group(result, column: str, limit: int = 20) -> list:\n    \"\"\"Get counts grouped by column value.\n\n    Args:\n        result: ResultTable or list of dicts\n        column: Column to group by\n        limit: Maximum groups to return\n\n    Returns:\n        List of (value, count) tuples, sorted by count descending\n    \"\"\"\n    from collections import Counter\n\n    rows = result.rows if isinstance(result, ResultTable) else result\n    values = [str(row[column]) for row in rows if column in row and row[column] is not None]\n\n    counts = Counter(values)\n    return counts.most_common(limit)\n\n\ndef res_distinct(result, column: str, limit: int = 50) -> list:\n    \"\"\"Get distinct values in a column.\n\n    Args:\n        result: ResultTable or list of dicts\n        column: Column to get distinct values from\n        limit: Maximum distinct values to return\n\n    Returns:\n        List of distinct values\n    \"\"\"\n    rows = result.rows if isinstance(result, ResultTable) else result\n    distinct_values = set()\n\n    for row in rows:\n        if column in row and row[column] is not None:\n            distinct_values.add(str(row[column]))\n\n        if len(distinct_values) >= limit:\n            break\n\n    return sorted(list(distinct_values))[:limit]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#| export\n@dataclass\nclass ResultTable:\n    \"\"\"Wrapper for SPARQL query results with bounded view operations.\"\"\"\n    rows: list          # list of dicts\n    columns: list       # column names\n    query: str          # Original SPARQL query\n    total_rows: int     # Total before limit\n\n    def __len__(self):\n        \"\"\"Return number of rows.\"\"\"\n        return len(self.rows)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return f\"ResultTable({len(self.rows)} rows, columns={self.columns})\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Result Table Views\n\nBounded view operations over SPARQL query results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataset_stats(ds_meta: DatasetMeta) -> str:\n",
    "    \"\"\"Get dataset statistics summary.\"\"\"\n",
    "    return ds_meta.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_graphs(ds_meta: DatasetMeta, pattern: str = None) -> list:\n",
    "    \"\"\"List named graphs, optionally filtered.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        pattern: Optional substring to filter graph URIs\n",
    "        \n",
    "    Returns:\n",
    "        List of (graph_uri, triple_count) tuples\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    for ctx in ds_meta.dataset.contexts():\n",
    "        uri = str(ctx.identifier)\n",
    "        if pattern is None or pattern in uri:\n",
    "            graphs.append((uri, len(ctx)))\n",
    "    \n",
    "    return sorted(graphs, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def graph_sample(ds_meta: DatasetMeta, graph_uri: str, limit: int = 10) -> list:\n",
    "    \"\"\"Get sample triples from a graph.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        graph_uri: URI of graph to sample\n",
    "        limit: Maximum triples to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (s, p, o) tuples as strings\n",
    "    \"\"\"\n",
    "    graph = ds_meta.dataset.graph(URIRef(graph_uri))\n",
    "    triples = [(str(s), str(p), str(o)) for s, p, o in list(graph.triples((None, None, None)))[:limit]]\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mount_ontology(ds_meta: DatasetMeta, ns: dict, path: str, ont_name: str) -> str:\n",
    "    \"\"\"Mount ontology into dataset as read-only onto/<name> graph.\n",
    "    \n",
    "    Args:\n",
    "        ds_meta: DatasetMeta containing the dataset\n",
    "        ns: Namespace dict (for compatibility with setup_ontology_context)\n",
    "        path: Path to ontology file\n",
    "        ont_name: Name for the ontology\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    graph_uri = URIRef(f'urn:rlm:{ds_meta.name}:onto/{ont_name}')\n",
    "    graph = ds_meta.dataset.graph(graph_uri)\n",
    "    \n",
    "    # Parse ontology into the graph\n",
    "    graph.parse(path)\n",
    "    \n",
    "    # Invalidate caches\n",
    "    ds_meta._invalidate_caches()\n",
    "    \n",
    "    return f\"Mounted {len(graph)} triples from {Path(path).name} into onto/{ont_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef setup_dataset_context(ns: dict, name: str = 'ds') -> str:\n    \"\"\"Initialize Dataset with mem/prov graphs, bind helper functions.\n    \n    Args:\n        ns: Namespace dict where Dataset will be stored\n        name: Variable name for the Dataset handle\n        \n    Returns:\n        Summary string describing what was created\n    \"\"\"\n    # Create dataset and meta\n    ds = Dataset()\n    ds_meta = DatasetMeta(ds, name=name)\n    \n    # Store in namespace\n    ns[name] = ds\n    ns[f\"{name}_meta\"] = ds_meta\n    \n    # Bind helper functions\n    ns['mem_add'] = partial(mem_add, ds_meta)\n    ns['mem_query'] = partial(mem_query, ds_meta)\n    ns['mem_retract'] = partial(mem_retract, ds_meta)\n    ns['mem_describe'] = partial(mem_describe, ds_meta)\n    ns['dataset_stats'] = partial(dataset_stats, ds_meta)\n    ns['work_create'] = partial(work_create, ds_meta)\n    ns['work_cleanup'] = partial(work_cleanup, ds_meta)\n    ns['work_to_mem'] = partial(work_to_mem, ds_meta)\n    ns['snapshot_dataset'] = partial(snapshot_dataset, ds_meta)\n    ns['mount_ontology'] = partial(mount_ontology, ds_meta, ns)\n    ns['list_graphs'] = partial(list_graphs, ds_meta)\n    ns['graph_sample'] = partial(graph_sample, ds_meta)\n    \n    # NEW: Bind Stage 2 result table view functions\n    ns['res_head'] = res_head\n    ns['res_where'] = res_where\n    ns['res_group'] = res_group\n    ns['res_distinct'] = res_distinct\n    \n    return f\"Created dataset '{name}' with session_id={ds_meta.session_id}\""
  },
  {
   "cell_type": "code",
   "source": "# Test result table views\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Add test data\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\nmem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\nmem_add(ds_meta, 'http://ex.org/charlie', 'http://ex.org/age', '30')\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/city', 'Boston')\nmem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/city', 'NYC')\n\n# Query and get results as list\nresults = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }')\n\n# Test res_head\nhead = res_head(results, n=2)\nassert len(head) == 2\nprint(f\"\u2713 res_head works: {len(head)} rows\")\n\n# Test res_where with exact value\nfiltered = res_where(results, 'age', value='30')\nassert len(filtered) == 2\nprint(f\"\u2713 res_where (exact) works: {len(filtered)} rows with age=30\")\n\n# Test res_where with pattern\nfiltered_pattern = res_where(results, 's', pattern='alice')\nassert len(filtered_pattern) == 1\nprint(f\"\u2713 res_where (pattern) works: {len(filtered_pattern)} rows matching 'alice'\")\n\n# Test res_group\ngroups = res_group(results, 'age')\nassert len(groups) == 2  # Two distinct ages\nassert groups[0][1] == 2  # Age '30' appears twice\nprint(f\"\u2713 res_group works: {groups}\")\n\n# Test res_distinct\ndistinct_ages = res_distinct(results, 'age')\nassert len(distinct_ages) == 2\nassert '25' in distinct_ages and '30' in distinct_ages\nprint(f\"\u2713 res_distinct works: {distinct_ages}\")\n\n# Test ResultTable wrapper\nresult_table = ResultTable(\n    rows=results,\n    columns=['s', 'age'],\n    query='SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }',\n    total_rows=len(results)\n)\nassert len(result_table) == 3\nprint(f\"\u2713 ResultTable works: {result_table}\")\n\n# Test result table views work with ResultTable\nhead_from_table = res_head(result_table, n=2)\nassert len(head_from_table) == 2\nprint(f\"\u2713 res_head works with ResultTable\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset creation\n",
    "test_ns = {}\n",
    "result = setup_dataset_context(test_ns, name='test_ds')\n",
    "assert 'test_ds' in test_ns\n",
    "assert 'test_ds_meta' in test_ns\n",
    "assert len(test_ns['test_ds_meta'].session_id) == 8\n",
    "print(\"\u2713 Dataset creation works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mem_add with provenance\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "result = mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob', \n",
    "                 source='test', reason='Testing')\n",
    "assert len(ds_meta.mem) == 1\n",
    "assert len(ds_meta.prov) > 0\n",
    "print(\"\u2713 mem_add works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mem_query\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "mem_add(ds_meta, 'http://ex.org/bob', 'http://ex.org/age', '25')\n",
    "\n",
    "results = mem_query(ds_meta, 'SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }')\n",
    "assert len(results) == 2\n",
    "assert all('s' in r and 'age' in r for r in results)\n",
    "print(\"\u2713 mem_query works\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mem_retract\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "assert len(ds_meta.mem) == 1\n",
    "\n",
    "result = mem_retract(ds_meta, predicate='http://ex.org/age', source='test', reason='Correction')\n",
    "assert len(ds_meta.mem) == 0\n",
    "assert 'Removed 1 triples' in result\n",
    "print(\"\u2713 mem_retract works\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mem_describe\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "\n",
    "desc = mem_describe(ds_meta, 'http://ex.org/alice')\n",
    "assert 'as_subject' in desc\n",
    "assert 'as_object' in desc\n",
    "assert len(desc['as_subject']) == 2\n",
    "print(\"\u2713 mem_describe works\")\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test index invalidation\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Access cached property\n",
    "initial_version = ds_meta._version\n",
    "_ = ds_meta.graph_stats\n",
    "\n",
    "# Mutate\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "\n",
    "# Check version incremented\n",
    "assert ds_meta._version > initial_version\n",
    "print(\"\u2713 Index invalidation works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test work graph lifecycle\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Create work graph\n",
    "uri, graph = work_create(ds_meta, task_id='test_task')\n",
    "assert 'work/test_task' in uri\n",
    "assert len(ds_meta.work_graphs) == 1\n",
    "\n",
    "# Add some triples to work graph\n",
    "graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/temp'), Literal('value')))\n",
    "assert len(graph) == 1\n",
    "\n",
    "# Promote to mem\n",
    "result = work_to_mem(ds_meta, 'test_task', reason='Test promotion')\n",
    "assert len(ds_meta.mem) == 1\n",
    "assert 'Promoted 1 triples' in result\n",
    "\n",
    "# Cleanup\n",
    "result = work_cleanup(ds_meta, task_id='test_task')\n",
    "assert 'Removed 1 work' in result\n",
    "assert len(ds_meta.work_graphs) == 0\n",
    "\n",
    "print(\"\u2713 Work graph lifecycle works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test snapshot\nimport tempfile\nimport os\n\ntest_ns = {}\nsetup_dataset_context(test_ns)\nds_meta = test_ns['ds_meta']\n\n# Add some data\nmem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n\n# Take snapshot\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.trig', delete=False) as f:\n    snapshot_path = f.name\n\nresult = snapshot_dataset(ds_meta, path=snapshot_path)\nassert os.path.exists(snapshot_path)\nassert 'Snapshot saved' in result\n\n# Load snapshot (let it auto-detect the name 'ds' from graph URIs)\ntest_ns2 = {}\nresult = load_snapshot(snapshot_path, test_ns2, name='restored')\nassert 'restored' in test_ns2\nassert 'restored_meta' in test_ns2\n# Should auto-detect original name 'ds' and use it for URIs\nassert len(test_ns2['restored_meta'].mem) == 1\n\n# Also test loading with same name\ntest_ns3 = {}\nresult = load_snapshot(snapshot_path, test_ns3, name='ds')\nassert 'ds' in test_ns3\nassert 'ds_meta' in test_ns3\nassert len(test_ns3['ds_meta'].mem) == 1\n\n# Cleanup\nos.unlink(snapshot_path)\n\nprint(\"\u2713 Snapshot roundtrip works\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bounded view functions\n",
    "test_ns = {}\n",
    "setup_dataset_context(test_ns)\n",
    "ds_meta = test_ns['ds_meta']\n",
    "\n",
    "# Add some data\n",
    "mem_add(ds_meta, 'http://ex.org/alice', 'http://ex.org/age', '30')\n",
    "work_create(ds_meta, 'task1')\n",
    "work_create(ds_meta, 'task2')\n",
    "\n",
    "# Test dataset_stats\n",
    "stats = dataset_stats(ds_meta)\n",
    "assert 'mem: 1 triples' in stats\n",
    "assert 'work graphs: 2' in stats\n",
    "\n",
    "# Test list_graphs\n",
    "graphs = list_graphs(ds_meta)\n",
    "assert len(graphs) >= 4  # mem, prov, work/task1, work/task2\n",
    "\n",
    "# Test list_graphs with pattern\n",
    "work_graphs = list_graphs(ds_meta, pattern='work/')\n",
    "assert len(work_graphs) == 2\n",
    "\n",
    "# Test graph_sample\n",
    "mem_uri = f'urn:rlm:{ds_meta.name}:mem'\n",
    "sample = graph_sample(ds_meta, mem_uri)\n",
    "assert len(sample) == 1\n",
    "\n",
    "print(\"\u2713 Bounded view functions work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Basic usage in RLM context\n",
    "ns = {}\n",
    "setup_dataset_context(ns)\n",
    "\n",
    "# RLM can now use: mem_add, mem_query, mem_describe, etc.\n",
    "ns['mem_add']('http://ex.org/alice', 'http://ex.org/knows', 'http://ex.org/bob')\n",
    "results = ns['mem_query']('SELECT ?s ?p ?o WHERE { ?s ?p ?o }')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Integration with ontology\n",
    "from rlm.ontology import setup_ontology_context\n",
    "\n",
    "ns = {}\n",
    "setup_dataset_context(ns)\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "\n",
    "# Mount ontology into dataset\n",
    "ns['mount_ontology']('ontology/prov.ttl', 'prov')\n",
    "\n",
    "# Now ontology is in dataset as onto/prov graph\n",
    "graphs = ns['list_graphs']()\n",
    "print(graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}