{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1f0c41",
   "metadata": {},
   "source": [
    "# Evaluation Reports\n",
    "\n",
    "> Dashboard for RLM evaluation task results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd87e8",
   "metadata": {},
   "source": [
    "This notebook loads and visualizes results from the RLM evaluation framework. Eval tasks are defined in `evals/tasks/` and executed via the CLI:\n",
    "\n",
    "```bash\n",
    "# Run all tasks\n",
    "python -m evals.cli run\n",
    "\n",
    "# Run specific category\n",
    "python -m evals.cli run 'regression/*'\n",
    "```\n",
    "\n",
    "Results are saved to `evals/results/` as JSON files and loaded here for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1c42b",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load all result JSON files from the results directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4966d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../evals/results')\n",
    "\n",
    "def load_results(results_dir: Path) -> list:\n",
    "    \"\"\"Load all evaluation results from directory.\"\"\"\n",
    "    if not results_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for result_file in results_dir.glob('*.json'):\n",
    "        try:\n",
    "            with open(result_file) as f:\n",
    "                data = json.load(f)\n",
    "                results.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {result_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = load_results(results_dir)\n",
    "print(f\"Loaded {len(results)} result files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a665d99",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Overall pass rates across all tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if results:\n",
    "    total_trials = sum(r.get('total_trials', 0) for r in results)\n",
    "    total_passed = sum(r.get('passed_trials', 0) for r in results)\n",
    "    avg_pass_rate = total_passed / total_trials if total_trials > 0 else 0\n",
    "    \n",
    "    print(f\"Tasks evaluated: {len(results)}\")\n",
    "    print(f\"Total trials: {total_trials}\")\n",
    "    print(f\"Total passed: {total_passed}\")\n",
    "    print(f\"Overall pass rate: {avg_pass_rate:.1%}\")\n",
    "else:\n",
    "    print(\"No results found. Run evals first:\")\n",
    "    print(\"  python -m evals.cli run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb324c3c",
   "metadata": {},
   "source": [
    "## Results by Category\n",
    "\n",
    "Break down performance by task category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if results:\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    by_category = defaultdict(list)\n",
    "    \n",
    "    for r in results:\n",
    "        task_id = r.get('task_id', 'unknown')\n",
    "        category = task_id.split('_')[0] if '_' in task_id else 'unknown'\n",
    "        by_category[category].append(r)\n",
    "    \n",
    "    print(\"\\nResults by Category:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for category in sorted(by_category.keys()):\n",
    "        cat_results = by_category[category]\n",
    "        cat_trials = sum(r.get('total_trials', 0) for r in cat_results)\n",
    "        cat_passed = sum(r.get('passed_trials', 0) for r in cat_results)\n",
    "        cat_rate = cat_passed / cat_trials if cat_trials > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{category.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for r in cat_results:\n",
    "            task_id = r.get('task_id', 'unknown')\n",
    "            passed = r.get('passed_trials', 0)\n",
    "            total = r.get('total_trials', 0)\n",
    "            pass_at_k = r.get('pass_at_k', 0)\n",
    "            status = \"PASS\" if passed == total else \"FAIL\"\n",
    "            print(f\"  [{status}] {task_id}: {passed}/{total} ({pass_at_k:.0%})\")\n",
    "        \n",
    "        print(f\"  Category total: {cat_passed}/{cat_trials} ({cat_rate:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d639f",
   "metadata": {},
   "source": [
    "## Detailed Results\n",
    "\n",
    "Show full details for each task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if results:\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for r in results:\n",
    "        task_id = r.get('task_id', 'unknown')\n",
    "        print(f\"\\n{task_id}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  pass@{r.get('total_trials', 0)}: {r.get('pass_at_k', 0):.1%}\")\n",
    "        print(f\"  pass^{r.get('total_trials', 0)}: {r.get('pass_power_k', 0):.1%}\")\n",
    "        print(f\"  Passed: {r.get('passed_trials', 0)}/{r.get('total_trials', 0)}\")\n",
    "        print(f\"  Avg iterations: {r.get('avg_iterations', 0):.1f}\")\n",
    "        \n",
    "        if 'trials' in r:\n",
    "            print(f\"  Trial details:\")\n",
    "            for i, trial in enumerate(r['trials'], 1):\n",
    "                passed = trial.get('passed', False)\n",
    "                status = \"✓\" if passed else \"✗\"\n",
    "                print(f\"    {status} Trial {i}: {trial.get('iterations', 0)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60c9a2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To run evaluations and generate results:\n",
    "\n",
    "```bash\n",
    "# Run all tasks\n",
    "python -m evals.cli run\n",
    "\n",
    "# Execute this notebook to show results\n",
    "nbdev exec_nb --path nbs/eval_reports.ipynb\n",
    "\n",
    "# Regenerate documentation\n",
    "nbdev_docs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
