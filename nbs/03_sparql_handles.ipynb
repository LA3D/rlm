{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# sparql_handles\n",
    "\n",
    "> SPARQL query execution with first-class result handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparql_handles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements Stage 3 from the trajectory: SPARQL query execution with first-class result handles.\n",
    "\n",
    "### Result Handle Pattern\n",
    "\n",
    "Every SPARQL execution produces a `SPARQLResultHandle` with:\n",
    "- `meta`: query, endpoint/local, timestamp, row count, columns\n",
    "- `rows`: stored internally as list of dicts (SELECT) or Graph (CONSTRUCT/DESCRIBE)\n",
    "- Bounded view operations: `res_head()`, `res_where()`, `res_group()`, `res_sample()`\n",
    "\n",
    "### Progressive Disclosure\n",
    "\n",
    "Result handles enable the root model to refine queries by inspecting metadata and small slices, not rerunning blind queries.\n",
    "\n",
    "### Dataset Integration\n",
    "\n",
    "SPARQL results can optionally be stored in dataset work graphs with full provenance tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sparqlx import SPARQLWrapper\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "import random\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## SPARQLResultHandle\n",
    "\n",
    "Unified wrapper for all SPARQL result types with metadata and bounded view operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SPARQLResultHandle:\n",
    "    \"\"\"Wrapper for SPARQL results with metadata and bounded view operations.\"\"\"\n",
    "\n",
    "    # Result data (never dumped wholesale to LLM)\n",
    "    rows: list | Graph          # SELECT rows or CONSTRUCT/DESCRIBE graph\n",
    "    result_type: str            # 'select' | 'ask' | 'construct' | 'describe'\n",
    "\n",
    "    # Metadata\n",
    "    query: str                  # Original SPARQL query\n",
    "    endpoint: str               # Where executed (URL or 'local')\n",
    "    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n",
    "\n",
    "    # For SELECT results\n",
    "    columns: list = None        # Column names\n",
    "    total_rows: int = 0         # Total before limit (may be same as len(rows) if not truncated)\n",
    "\n",
    "    # For Graph results (CONSTRUCT/DESCRIBE)\n",
    "    triple_count: int = 0       # Number of triples actually stored in self.rows\n",
    "    total_triples: int = 0      # Total before limit (may be same as triple_count if not truncated)\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Bounded summary for LLM - reports actual stored count.\"\"\"\n",
    "        if self.result_type == 'select':\n",
    "            stored = len(self.rows)\n",
    "            if self.total_rows > stored:\n",
    "                return f\"SELECT: {stored} rows (of {self.total_rows} total), columns={self.columns}\"\n",
    "            return f\"SELECT: {stored} rows, columns={self.columns}\"\n",
    "        elif self.result_type == 'ask':\n",
    "            return f\"ASK: {self.rows}\"\n",
    "        else:\n",
    "            # For CONSTRUCT/DESCRIBE, report actual stored count\n",
    "            if self.total_triples > self.triple_count:\n",
    "                return f\"{self.result_type.upper()}: {self.triple_count} triples (of {self.total_triples} total)\"\n",
    "            return f\"{self.result_type.upper()}: {self.triple_count} triples\"\n",
    "\n",
    "    def __len__(self):\n",
    "        if isinstance(self.rows, bool):\n",
    "            return 1  # ASK result\n",
    "        return len(self.rows) if hasattr(self.rows, '__len__') else 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if isinstance(self.rows, bool):\n",
    "            return iter([self.rows])\n",
    "        return iter(self.rows)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SPARQLResultHandle({self.summary()})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "Test SPARQLResultHandle with different result types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SELECT handle: SPARQLResultHandle(SELECT: 1 rows, columns=['s', 'age'])\n",
      "✓ Truncated SELECT handle: SPARQLResultHandle(SELECT: 1 rows (of 100 total), columns=['s', 'age'])\n",
      "✓ ASK handle: SPARQLResultHandle(ASK: True)\n",
      "✓ CONSTRUCT handle: SPARQLResultHandle(CONSTRUCT: 1 triples)\n",
      "✓ Truncated CONSTRUCT handle: SPARQLResultHandle(CONSTRUCT: 1 triples (of 500 total))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    }
   ],
   "source": [
    "# Test SELECT result\n",
    "select_handle = SPARQLResultHandle(\n",
    "    rows=[{'s': 'http://ex.org/alice', 'age': '30'}],\n",
    "    result_type='select',\n",
    "    query='SELECT ?s ?age WHERE { ?s :age ?age }',\n",
    "    endpoint='local',\n",
    "    columns=['s', 'age'],\n",
    "    total_rows=1\n",
    ")\n",
    "assert select_handle.summary() == \"SELECT: 1 rows, columns=['s', 'age']\"\n",
    "assert len(select_handle) == 1\n",
    "print(f\"✓ SELECT handle: {select_handle}\")\n",
    "\n",
    "# Test SELECT with truncation\n",
    "truncated_select = SPARQLResultHandle(\n",
    "    rows=[{'s': 'http://ex.org/alice', 'age': '30'}],\n",
    "    result_type='select',\n",
    "    query='SELECT ?s ?age WHERE { ?s :age ?age }',\n",
    "    endpoint='local',\n",
    "    columns=['s', 'age'],\n",
    "    total_rows=100  # More than stored\n",
    ")\n",
    "assert '(of 100 total)' in truncated_select.summary()\n",
    "print(f\"✓ Truncated SELECT handle: {truncated_select}\")\n",
    "\n",
    "# Test ASK result\n",
    "ask_handle = SPARQLResultHandle(\n",
    "    rows=True,\n",
    "    result_type='ask',\n",
    "    query='ASK { ?s ?p ?o }',\n",
    "    endpoint='local'\n",
    ")\n",
    "assert ask_handle.summary() == \"ASK: True\"\n",
    "print(f\"✓ ASK handle: {ask_handle}\")\n",
    "\n",
    "# Test CONSTRUCT result\n",
    "g = Graph()\n",
    "g.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/age'), Literal('30')))\n",
    "construct_handle = SPARQLResultHandle(\n",
    "    rows=g,\n",
    "    result_type='construct',\n",
    "    query='CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }',\n",
    "    endpoint='local',\n",
    "    triple_count=1,\n",
    "    total_triples=1\n",
    ")\n",
    "assert construct_handle.summary() == \"CONSTRUCT: 1 triples\"\n",
    "print(f\"✓ CONSTRUCT handle: {construct_handle}\")\n",
    "\n",
    "# Test CONSTRUCT with truncation\n",
    "truncated_construct = SPARQLResultHandle(\n",
    "    rows=g,\n",
    "    result_type='construct',\n",
    "    query='CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }',\n",
    "    endpoint='local',\n",
    "    triple_count=1,\n",
    "    total_triples=500  # More than stored\n",
    ")\n",
    "assert '(of 500 total)' in truncated_construct.summary()\n",
    "print(f\"✓ Truncated CONSTRUCT handle: {truncated_construct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Remote SPARQL Query\n",
    "\n",
    "Execute SPARQL queries against remote endpoints and return result handles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlxqdenv67a",
   "metadata": {},
   "source": [
    "## Query Rewriting Helper\n",
    "\n",
    "Helper to inject LIMIT clauses into SELECT queries to bound server-side work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af555rzsdkd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "\n",
    "def _inject_limit(query: str, limit: int) -> tuple[str, bool]:\n",
    "    \"\"\"Inject LIMIT clause into SELECT query if not already present.\n",
    "    \n",
    "    Args:\n",
    "        query: SPARQL query string\n",
    "        limit: LIMIT value to inject\n",
    "        \n",
    "    Returns:\n",
    "        (modified_query, was_modified) tuple\n",
    "    \"\"\"\n",
    "    query_upper = query.upper()\n",
    "    \n",
    "    # Only inject for SELECT queries\n",
    "    if 'SELECT' not in query_upper:\n",
    "        return query, False\n",
    "    \n",
    "    # Check if LIMIT already present\n",
    "    if re.search(r'\\bLIMIT\\s+\\d+', query_upper):\n",
    "        return query, False\n",
    "    \n",
    "    # Find where to inject LIMIT (before ORDER BY if present, otherwise at end)\n",
    "    order_match = re.search(r'\\bORDER\\s+BY\\b', query_upper)\n",
    "    if order_match:\n",
    "        # Insert LIMIT before ORDER BY\n",
    "        pos = order_match.start()\n",
    "        modified = query[:pos] + f' LIMIT {limit} ' + query[pos:]\n",
    "        return modified, True\n",
    "    else:\n",
    "        # Append LIMIT at end (before any trailing whitespace/comments)\n",
    "        modified = query.rstrip() + f' LIMIT {limit}'\n",
    "        return modified, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jd3p3j7k9ng",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic injection: SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 100\n",
      "✓ Existing LIMIT preserved: SELECT ?s WHERE { ?s ?p ?o } LIMIT 50\n",
      "✓ Injection before ORDER BY: SELECT ?s ?o WHERE { ?s ?p ?o }  LIMIT 100 ORDER BY ?s\n",
      "✓ CONSTRUCT not modified: CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\n"
     ]
    }
   ],
   "source": [
    "# Test LIMIT injection\n",
    "q1 = \"SELECT ?s ?p ?o WHERE { ?s ?p ?o }\"\n",
    "modified, injected = _inject_limit(q1, 100)\n",
    "assert injected == True\n",
    "assert 'LIMIT 100' in modified\n",
    "print(f\"✓ Basic injection: {modified}\")\n",
    "\n",
    "# Test with existing LIMIT (should not modify)\n",
    "q2 = \"SELECT ?s WHERE { ?s ?p ?o } LIMIT 50\"\n",
    "modified, injected = _inject_limit(q2, 100)\n",
    "assert injected == False\n",
    "assert modified == q2\n",
    "print(f\"✓ Existing LIMIT preserved: {modified}\")\n",
    "\n",
    "# Test with ORDER BY (inject before it)\n",
    "q3 = \"SELECT ?s ?o WHERE { ?s ?p ?o } ORDER BY ?s\"\n",
    "modified, injected = _inject_limit(q3, 100)\n",
    "assert injected == True\n",
    "assert 'LIMIT 100' in modified\n",
    "assert modified.index('LIMIT') < modified.index('ORDER')\n",
    "print(f\"✓ Injection before ORDER BY: {modified}\")\n",
    "\n",
    "# Test CONSTRUCT (should not inject)\n",
    "q4 = \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\"\n",
    "modified, injected = _inject_limit(q4, 100)\n",
    "assert injected == False\n",
    "print(f\"✓ CONSTRUCT not modified: {modified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sparql_query(\n",
    "    query: str,\n",
    "    endpoint: str = \"https://query.wikidata.org/sparql\",\n",
    "    max_results: int = 100,\n",
    "    name: str = 'res',\n",
    "    ns: dict = None,\n",
    "    timeout: float = 30.0,\n",
    "    # Dataset integration\n",
    "    ds_meta = None,\n",
    "    store_in_work: bool = False,\n",
    "    work_task_id: str = None\n",
    ") -> str:\n",
    "    \"\"\"Execute SPARQL query, store SPARQLResultHandle in namespace.\n",
    "\n",
    "    For SELECT: Stores SPARQLResultHandle with rows as list of dicts\n",
    "    For CONSTRUCT/DESCRIBE: Stores SPARQLResultHandle with rdflib.Graph\n",
    "    For ASK: Stores SPARQLResultHandle with boolean result\n",
    "\n",
    "    IMPORTANT - Work Bounds:\n",
    "    - For SELECT: Automatically injects LIMIT clause to bound server-side work\n",
    "    - For CONSTRUCT/DESCRIBE: max_results only truncates locally; full results \n",
    "      still fetched from endpoint (SPARQL 1.1 has no standard LIMIT for graphs)\n",
    "\n",
    "    If ds_meta provided and store_in_work=True:\n",
    "    - CONSTRUCT results stored in work/<task_id> graph\n",
    "    - Query logged to prov graph\n",
    "    \n",
    "    Args:\n",
    "        query: SPARQL query string\n",
    "        endpoint: SPARQL endpoint URL\n",
    "        max_results: Maximum results to return (for SELECT/CONSTRUCT)\n",
    "        name: Variable name to store result handle\n",
    "        ns: Namespace dict (defaults to globals())\n",
    "        timeout: Query timeout in seconds\n",
    "        ds_meta: Optional DatasetMeta for dataset integration\n",
    "        store_in_work: If True and ds_meta provided, store CONSTRUCT results in work graph\n",
    "        work_task_id: Task ID for work graph (auto-generated if None)\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing the result\n",
    "    \"\"\"\n",
    "    if ns is None:\n",
    "        ns = globals()\n",
    "    \n",
    "    # Try to inject LIMIT for SELECT queries to bound server-side work\n",
    "    modified_query, limit_injected = _inject_limit(query, max_results)\n",
    "    \n",
    "    # Configure wrapper with timeout and headers\n",
    "    headers = {\"User-Agent\": \"RLM/1.0 (https://github.com/LA3D/rlm)\"}\n",
    "    wrapper = SPARQLWrapper(\n",
    "        sparql_endpoint=endpoint,\n",
    "        client_config=dict(timeout=timeout, headers=headers)\n",
    "    )\n",
    "    \n",
    "    # Execute query with rdflib conversion (use modified_query if LIMIT was injected)\n",
    "    result = wrapper.query(modified_query, convert=True)\n",
    "    \n",
    "    # Determine result type and create handle\n",
    "    if isinstance(result, bool):\n",
    "        # ASK query\n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=result,\n",
    "            result_type='ask',\n",
    "            query=query,  # Store original query\n",
    "            endpoint=endpoint\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        return f\"ASK result: {result}, stored in '{name}'\"\n",
    "    \n",
    "    elif hasattr(result, 'serialize'):\n",
    "        # CONSTRUCT or DESCRIBE query - result is rdflib.Graph\n",
    "        # NOTE: SPARQL 1.1 has no standard LIMIT for graph patterns, so we still\n",
    "        # fetch full results from endpoint and truncate locally. This is a known\n",
    "        # limitation - max_results is output truncation, not work bound.\n",
    "        \n",
    "        # Capture original count before truncation\n",
    "        original_count = len(result)\n",
    "        \n",
    "        # Use islice to limit triples\n",
    "        g = Graph()\n",
    "        for triple in islice(result, max_results):\n",
    "            g.add(triple)\n",
    "        \n",
    "        # Determine if CONSTRUCT or DESCRIBE\n",
    "        query_upper = query.upper()\n",
    "        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'\n",
    "        \n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=g,\n",
    "            result_type=result_type,\n",
    "            query=query,\n",
    "            endpoint=endpoint,\n",
    "            triple_count=len(g),        # Actual stored count\n",
    "            total_triples=original_count  # Pre-truncation count\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        \n",
    "        # Dataset integration for CONSTRUCT results\n",
    "        if ds_meta is not None and store_in_work:\n",
    "            from rlm.dataset import work_create\n",
    "            import uuid\n",
    "            \n",
    "            task_id = work_task_id if work_task_id else f\"sparql_{uuid.uuid4().hex[:8]}\"\n",
    "            graph_uri, work_graph = work_create(ds_meta, task_id)\n",
    "            \n",
    "            # Copy triples to work graph\n",
    "            for s, p, o in g:\n",
    "                work_graph.add((s, p, o))\n",
    "            \n",
    "            # Log query to prov\n",
    "            from rdflib import Namespace, RDF, XSD\n",
    "            RLM_PROV = Namespace('urn:rlm:prov:')\n",
    "            event_uri = URIRef(f'urn:rlm:prov:sparql_{uuid.uuid4().hex[:8]}')\n",
    "            ds_meta.prov.add((event_uri, RDF.type, RLM_PROV.SPARQLQuery))\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.query, Literal(query)))\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.endpoint, Literal(endpoint)))\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.resultGraph, URIRef(graph_uri)))\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.timestamp, Literal(handle.timestamp, datatype=XSD.dateTime)))\n",
    "            ds_meta.prov.add((event_uri, RLM_PROV.session, Literal(ds_meta.session_id)))\n",
    "            \n",
    "            return f\"Graph with {len(g)} triples stored in '{name}' and work/{task_id}\" + \\\n",
    "                   (f\" (of {original_count} total)\" if original_count > len(g) else \"\")\n",
    "        \n",
    "        return f\"Graph with {len(g)} triples stored in '{name}'\" + \\\n",
    "               (f\" (of {original_count} total)\" if original_count > len(g) else \"\")\n",
    "    \n",
    "    else:\n",
    "        # SELECT query - result is list of dicts\n",
    "        # If we injected LIMIT, the result is already bounded\n",
    "        result_list = list(result)\n",
    "        original_count = len(result_list)\n",
    "        \n",
    "        # Apply additional client-side truncation if needed\n",
    "        # (this should rarely happen if LIMIT injection worked)\n",
    "        rows = list(islice(result_list, max_results))\n",
    "        cols = list(rows[0].keys()) if rows else []\n",
    "        \n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=rows,\n",
    "            result_type='select',\n",
    "            query=query,  # Store original query\n",
    "            endpoint=endpoint,\n",
    "            columns=cols,\n",
    "            total_rows=original_count  # This may equal len(rows) if LIMIT injection worked\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        \n",
    "        msg = f\"SELECT result with {len(rows)} rows, columns: {cols}, stored in '{name}'\"\n",
    "        if limit_injected:\n",
    "            msg += f\" (LIMIT {max_results} injected)\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "Test against Wikidata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT result with 5 rows, columns: ['s', 'p', 'o'], stored in 'wikidata_test'\n",
      "✓ SELECT query works: SELECT: 5 rows, columns=['s', 'p', 'o']\n",
      "Graph with 3 triples stored in 'graph_test'\n",
      "✓ CONSTRUCT query works: CONSTRUCT: 3 triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test SELECT query against Wikidata\n",
    "test_ns = {}\n",
    "result = sparql_query(\n",
    "    \"SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 5\",\n",
    "    ns=test_ns,\n",
    "    name='wikidata_test'\n",
    ")\n",
    "print(result)\n",
    "assert 'wikidata_test' in test_ns\n",
    "assert isinstance(test_ns['wikidata_test'], SPARQLResultHandle)\n",
    "assert test_ns['wikidata_test'].result_type == 'select'\n",
    "assert len(test_ns['wikidata_test'].rows) == 5\n",
    "print(f\"✓ SELECT query works: {test_ns['wikidata_test'].summary()}\")\n",
    "\n",
    "# Test CONSTRUCT query\n",
    "result = sparql_query(\n",
    "    \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o } LIMIT 3\",\n",
    "    ns=test_ns,\n",
    "    name='graph_test'\n",
    ")\n",
    "print(result)\n",
    "assert test_ns['graph_test'].result_type == 'construct'\n",
    "assert isinstance(test_ns['graph_test'].rows, Graph)\n",
    "print(f\"✓ CONSTRUCT query works: {test_ns['graph_test'].summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Local Graph Query\n",
    "\n",
    "Execute SPARQL queries against local rdflib graphs (mounted ontologies or work graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sparql_local(\n",
    "    query: str,\n",
    "    graph: Graph | str,\n",
    "    max_results: int = 100,\n",
    "    name: str = 'res',\n",
    "    ns: dict = None\n",
    ") -> str:\n",
    "    \"\"\"Execute SPARQL query on local rdflib Graph.\n",
    "\n",
    "    Useful for querying mounted ontologies or work graphs.\n",
    "    Returns SPARQLResultHandle same as sparql_query().\n",
    "    \n",
    "    IMPORTANT - Work Bounds:\n",
    "    - max_results is output truncation only; full result set is materialized\n",
    "    - For large local graphs, consider filtering in the SPARQL query itself\n",
    "    \n",
    "    Args:\n",
    "        query: SPARQL query string\n",
    "        graph: rdflib.Graph object or name of graph in namespace\n",
    "        max_results: Maximum results to return\n",
    "        name: Variable name to store result handle\n",
    "        ns: Namespace dict (defaults to globals())\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing the result\n",
    "    \"\"\"\n",
    "    if ns is None:\n",
    "        ns = globals()\n",
    "    \n",
    "    # Resolve graph if string name provided\n",
    "    if isinstance(graph, str):\n",
    "        if graph not in ns:\n",
    "            return f\"Error: Graph '{graph}' not found in namespace\"\n",
    "        graph_obj = ns[graph]\n",
    "        # Handle GraphMeta wrapper\n",
    "        if hasattr(graph_obj, 'graph'):\n",
    "            graph_obj = graph_obj.graph\n",
    "    else:\n",
    "        graph_obj = graph\n",
    "    \n",
    "    if not isinstance(graph_obj, Graph):\n",
    "        return f\"Error: Expected rdflib.Graph, got {type(graph_obj)}\"\n",
    "    \n",
    "    # Execute query on local graph\n",
    "    result = graph_obj.query(query)\n",
    "    \n",
    "    # Determine result type\n",
    "    query_upper = query.upper()\n",
    "    \n",
    "    if 'ASK' in query_upper:\n",
    "        # ASK query\n",
    "        ask_result = bool(result)\n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=ask_result,\n",
    "            result_type='ask',\n",
    "            query=query,\n",
    "            endpoint='local'\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        return f\"ASK result: {ask_result}, stored in '{name}'\"\n",
    "    \n",
    "    elif 'CONSTRUCT' in query_upper or 'DESCRIBE' in query_upper:\n",
    "        # CONSTRUCT or DESCRIBE query\n",
    "        result_type = 'construct' if 'CONSTRUCT' in query_upper else 'describe'\n",
    "        \n",
    "        # Capture original count before truncation\n",
    "        result_list = list(result)\n",
    "        original_count = len(result_list)\n",
    "        \n",
    "        # Use islice to limit triples\n",
    "        g = Graph()\n",
    "        for triple in islice(result_list, max_results):\n",
    "            g.add(triple)\n",
    "        \n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=g,\n",
    "            result_type=result_type,\n",
    "            query=query,\n",
    "            endpoint='local',\n",
    "            triple_count=len(g),        # Actual stored count\n",
    "            total_triples=original_count  # Pre-truncation count\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        \n",
    "        msg = f\"Graph with {len(g)} triples stored in '{name}'\"\n",
    "        if original_count > len(g):\n",
    "            msg += f\" (of {original_count} total)\"\n",
    "        return msg\n",
    "    \n",
    "    else:\n",
    "        # SELECT query\n",
    "        # Capture original count before truncation\n",
    "        result_list = list(result)\n",
    "        original_count = len(result_list)\n",
    "        \n",
    "        rows = []\n",
    "        for row in islice(result_list, max_results):\n",
    "            row_dict = {}\n",
    "            for var in result.vars:\n",
    "                row_dict[str(var)] = row[var] if row[var] else None\n",
    "            rows.append(row_dict)\n",
    "        \n",
    "        cols = [str(v) for v in result.vars] if result.vars else []\n",
    "        \n",
    "        handle = SPARQLResultHandle(\n",
    "            rows=rows,\n",
    "            result_type='select',\n",
    "            query=query,\n",
    "            endpoint='local',\n",
    "            columns=cols,\n",
    "            total_rows=original_count  # Pre-truncation count\n",
    "        )\n",
    "        ns[name] = handle\n",
    "        \n",
    "        msg = f\"SELECT result with {len(rows)} rows, columns: {cols}, stored in '{name}'\"\n",
    "        if original_count > len(rows):\n",
    "            msg += f\" (of {original_count} total)\"\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Test with local graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT result with 2 rows, columns: ['s', 'age'], stored in 'local_res'\n",
      "✓ Local SELECT query works: [{'s': rdflib.term.URIRef('http://ex.org/alice'), 'age': rdflib.term.Literal('30')}, {'s': rdflib.term.URIRef('http://ex.org/bob'), 'age': rdflib.term.Literal('25')}]\n",
      "Graph with 2 triples stored in 'local_graph'\n",
      "✓ Local CONSTRUCT query works\n",
      "Graph with 2 triples stored in 'truncated' (of 3 total)\n",
      "✓ Truncation works correctly: CONSTRUCT: 2 triples (of 3 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    }
   ],
   "source": [
    "# Create test graph\n",
    "test_graph = Graph()\n",
    "test_graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/age'), Literal('30')))\n",
    "test_graph.add((URIRef('http://ex.org/bob'), URIRef('http://ex.org/age'), Literal('25')))\n",
    "test_graph.add((URIRef('http://ex.org/alice'), URIRef('http://ex.org/city'), Literal('Boston')))\n",
    "\n",
    "test_ns = {'my_graph': test_graph}\n",
    "\n",
    "# Test SELECT query on local graph\n",
    "result = sparql_local(\n",
    "    \"SELECT ?s ?age WHERE { ?s <http://ex.org/age> ?age }\",\n",
    "    'my_graph',\n",
    "    ns=test_ns,\n",
    "    name='local_res'\n",
    ")\n",
    "print(result)\n",
    "assert 'local_res' in test_ns\n",
    "assert test_ns['local_res'].result_type == 'select'\n",
    "assert len(test_ns['local_res'].rows) == 2\n",
    "assert test_ns['local_res'].total_rows == 2\n",
    "print(f\"✓ Local SELECT query works: {test_ns['local_res'].rows}\")\n",
    "\n",
    "# Test CONSTRUCT on local graph\n",
    "result = sparql_local(\n",
    "    \"CONSTRUCT { ?s <http://ex.org/age> ?age } WHERE { ?s <http://ex.org/age> ?age }\",\n",
    "    test_graph,\n",
    "    ns=test_ns,\n",
    "    name='local_graph'\n",
    ")\n",
    "print(result)\n",
    "assert test_ns['local_graph'].result_type == 'construct'\n",
    "assert len(test_ns['local_graph'].rows) == 2\n",
    "assert test_ns['local_graph'].triple_count == 2\n",
    "assert test_ns['local_graph'].total_triples == 2\n",
    "print(f\"✓ Local CONSTRUCT query works\")\n",
    "\n",
    "# Test truncation\n",
    "result = sparql_local(\n",
    "    \"CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o }\",\n",
    "    test_graph,\n",
    "    max_results=2,\n",
    "    ns=test_ns,\n",
    "    name='truncated'\n",
    ")\n",
    "print(result)\n",
    "assert len(test_ns['truncated'].rows) == 2\n",
    "assert test_ns['truncated'].triple_count == 2\n",
    "assert test_ns['truncated'].total_triples == 3  # Original had 3\n",
    "assert '(of 3 total)' in test_ns['truncated'].summary()\n",
    "print(f\"✓ Truncation works correctly: {test_ns['truncated'].summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## View Operations\n",
    "\n",
    "Bounded view functions for progressive disclosure over result sets.\n",
    "\n",
    "These functions work with `SPARQLResultHandle`, `ResultTable`, or plain lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def res_sample(result, n: int = 10, seed: int = None) -> list:\n",
    "    \"\"\"Get random sample of N rows from result.\n",
    "\n",
    "    Args:\n",
    "        result: SPARQLResultHandle, ResultTable, or list\n",
    "        n: Number of rows to sample\n",
    "        seed: Optional random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        List of sampled rows\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Extract rows from different result types\n",
    "    if isinstance(result, SPARQLResultHandle):\n",
    "        if result.result_type in ['construct', 'describe']:\n",
    "            # For graphs, sample triples\n",
    "            rows = list(result.rows)\n",
    "        elif result.result_type == 'ask':\n",
    "            # ASK has no rows to sample\n",
    "            return [result.rows]\n",
    "        else:\n",
    "            rows = result.rows\n",
    "    elif hasattr(result, 'rows'):\n",
    "        # ResultTable or similar\n",
    "        rows = result.rows\n",
    "    else:\n",
    "        # Plain list\n",
    "        rows = result\n",
    "\n",
    "    if len(rows) <= n:\n",
    "        return list(rows)\n",
    "    return random.sample(list(rows), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "Test res_sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ res_sample works with list: 5 items\n",
      "✓ res_sample works with SPARQLResultHandle\n",
      "✓ res_sample handles small results correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    }
   ],
   "source": [
    "# Test with list\n",
    "test_list = [{'x': i} for i in range(20)]\n",
    "sample = res_sample(test_list, n=5, seed=42)\n",
    "assert len(sample) == 5\n",
    "assert all(isinstance(item, dict) for item in sample)\n",
    "print(f\"✓ res_sample works with list: {len(sample)} items\")\n",
    "\n",
    "# Test with SPARQLResultHandle\n",
    "handle = SPARQLResultHandle(\n",
    "    rows=[{'s': f'http://ex.org/item{i}'} for i in range(15)],\n",
    "    result_type='select',\n",
    "    query='SELECT ?s WHERE { ?s ?p ?o }',\n",
    "    endpoint='local',\n",
    "    columns=['s'],\n",
    "    total_rows=15\n",
    ")\n",
    "sample = res_sample(handle, n=3, seed=42)\n",
    "assert len(sample) == 3\n",
    "print(f\"✓ res_sample works with SPARQLResultHandle\")\n",
    "\n",
    "# Test with small result (no sampling needed)\n",
    "small_list = [1, 2, 3]\n",
    "sample = res_sample(small_list, n=10)\n",
    "assert len(sample) == 3\n",
    "print(f\"✓ res_sample handles small results correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Setup Function\n",
    "\n",
    "Initialize SPARQL tools in namespace for RLM sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_sparql_context(\n",
    "    ns: dict,\n",
    "    default_endpoint: str = \"https://query.wikidata.org/sparql\",\n",
    "    ds_meta = None\n",
    ") -> str:\n",
    "    \"\"\"Initialize SPARQL tools in namespace.\n",
    "\n",
    "    Binds:\n",
    "    - sparql_query() with default endpoint\n",
    "    - sparql_local() if ds_meta provided\n",
    "    - res_head(), res_where(), res_group(), res_distinct(), res_sample()\n",
    "\n",
    "    Args:\n",
    "        ns: Namespace dict where functions will be bound\n",
    "        default_endpoint: Default SPARQL endpoint URL\n",
    "        ds_meta: Optional DatasetMeta for dataset integration\n",
    "        \n",
    "    Returns:\n",
    "        Status message\n",
    "    \"\"\"\n",
    "    # Import view functions from dataset module\n",
    "    try:\n",
    "        from rlm.dataset import res_head, res_where, res_group, res_distinct\n",
    "    except ImportError:\n",
    "        # Fallback if dataset module not available\n",
    "        res_head = res_where = res_group = res_distinct = None\n",
    "    \n",
    "    # Bind sparql_query with default endpoint and dataset integration\n",
    "    if ds_meta is not None:\n",
    "        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns, ds_meta=ds_meta)\n",
    "    else:\n",
    "        ns['sparql_query'] = partial(sparql_query, endpoint=default_endpoint, ns=ns)\n",
    "    \n",
    "    # Bind sparql_local\n",
    "    ns['sparql_local'] = partial(sparql_local, ns=ns)\n",
    "    \n",
    "    # Bind view operations\n",
    "    if res_head is not None:\n",
    "        ns['res_head'] = res_head\n",
    "        ns['res_where'] = res_where\n",
    "        ns['res_group'] = res_group\n",
    "        ns['res_distinct'] = res_distinct\n",
    "    ns['res_sample'] = res_sample\n",
    "    \n",
    "    bound_funcs = ['sparql_query', 'sparql_local', 'res_sample']\n",
    "    if res_head is not None:\n",
    "        bound_funcs.extend(['res_head', 'res_where', 'res_group', 'res_distinct'])\n",
    "    \n",
    "    msg = f\"SPARQL context initialized with endpoint: {default_endpoint}\"\n",
    "    if ds_meta is not None:\n",
    "        msg += f\"\\nDataset integration enabled (session: {ds_meta.session_id})\"\n",
    "    msg += f\"\\nBound functions: {', '.join(bound_funcs)}\"\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "Test setup function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARQL context initialized with endpoint: https://query.wikidata.org/sparql\n",
      "Bound functions: sparql_query, sparql_local, res_sample, res_head, res_where, res_group, res_distinct\n",
      "✓ Setup function works\n",
      "SPARQL context initialized with endpoint: https://query.wikidata.org/sparql\n",
      "Dataset integration enabled (session: ee270120)\n",
      "Bound functions: sparql_query, sparql_local, res_sample, res_head, res_where, res_group, res_distinct\n",
      "✓ Setup with dataset integration works\n"
     ]
    }
   ],
   "source": [
    "# Test basic setup\n",
    "test_ns = {}\n",
    "result = setup_sparql_context(test_ns)\n",
    "print(result)\n",
    "assert 'sparql_query' in test_ns\n",
    "assert 'sparql_local' in test_ns\n",
    "assert 'res_sample' in test_ns\n",
    "print(f\"✓ Setup function works\")\n",
    "\n",
    "# Test with dataset integration\n",
    "try:\n",
    "    from rlm.dataset import DatasetMeta\n",
    "    from rdflib import Dataset\n",
    "    \n",
    "    ds = Dataset()\n",
    "    ds_meta = DatasetMeta(ds, name='test')\n",
    "    \n",
    "    test_ns2 = {}\n",
    "    result = setup_sparql_context(test_ns2, ds_meta=ds_meta)\n",
    "    print(result)\n",
    "    assert 'session:' in result\n",
    "    print(f\"✓ Setup with dataset integration works\")\n",
    "except ImportError:\n",
    "    print(\"⊘ Dataset module not available, skipping integration test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "End-to-end examples showing SPARQL handles in RLM context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT: 10 rows, columns=['s', 'p', 'o']\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Inspect results\u001b[39;00m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(ns[\u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m].summary())\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mres_head\u001b[49m(ns[\u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m5\u001b[39m))\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(res_sample(ns[\u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m3\u001b[39m))\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'res_head' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'res_head' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-ec3b22407f9e>\", line 11, in <module>\n    print(res_head(ns['results'], 5))\n          ^^^^^^^^\n",
      "NameError: name 'res_head' is not defined\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example 1: Basic SPARQL workflow\n",
    "ns = {}\n",
    "setup_sparql_context(ns)\n",
    "\n",
    "# Execute query (LLM would do this)\n",
    "ns['sparql_query']('SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 10', name='results')\n",
    "\n",
    "# Inspect results\n",
    "print(ns['results'].summary())\n",
    "print(res_head(ns['results'], 5))\n",
    "print(res_sample(ns['results'], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'ds' (session: 2fbcf046)\n",
      "mem: 0 triples\n",
      "prov: 6 events\n",
      "work graphs: 1\n",
      "onto graphs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example 2: Dataset integration\n",
    "from rlm.dataset import setup_dataset_context\n",
    "\n",
    "ns = {}\n",
    "setup_dataset_context(ns)\n",
    "setup_sparql_context(ns, ds_meta=ns['ds_meta'])\n",
    "\n",
    "# Query and store in work graph\n",
    "ns['sparql_query'](\n",
    "    'CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o } LIMIT 5',\n",
    "    name='discovered_triples',\n",
    "    store_in_work=True,\n",
    "    work_task_id='discovery_1'\n",
    ")\n",
    "\n",
    "# Check provenance\n",
    "print(ns['dataset_stats']())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 classes\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n",
      "\u001b[32m     10\u001b[39m ns[\u001b[33m'\u001b[39m\u001b[33msparql_local\u001b[39m\u001b[33m'\u001b[39m](\n",
      "\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSELECT ?c WHERE \u001b[39m\u001b[33m{\u001b[39m\u001b[33m ?c a <http://www.w3.org/2002/07/owl#Class> }\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprov\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m     13\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m     14\u001b[39m )\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ns[\u001b[33m'\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m'\u001b[39m].rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m classes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mres_head\u001b[49m(ns[\u001b[33m'\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m10\u001b[39m))\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'res_head' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cbf62e56f2a5>:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'res_head' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/cvardema/uvws/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-897581f12616>\", line 17, in <module>\n    print(res_head(ns['classes'], 10))\n          ^^^^^^^^\n",
      "NameError: name 'res_head' is not defined\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example 3: Local graph queries\n",
    "from rlm.ontology import setup_ontology_context\n",
    "\n",
    "ns = {}\n",
    "setup_sparql_context(ns)\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "\n",
    "# Query mounted ontology\n",
    "ns['sparql_local'](\n",
    "    'SELECT ?c WHERE { ?c a <http://www.w3.org/2002/07/owl#Class> }',\n",
    "    'prov',\n",
    "    name='classes'\n",
    ")\n",
    "\n",
    "print(f\"Found {len(ns['classes'].rows)} classes\")\n",
    "print(res_head(ns['classes'], 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
