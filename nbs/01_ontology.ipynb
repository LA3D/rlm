{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986bf3c4",
   "metadata": {},
   "source": [
    "# ontology\n",
    "\n",
    "> RDF ontology loading and meta-graph navigation for RLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d60cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842332ec",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module implements Stages 1-2 of the trajectory: Define the Ontology \"Context Model\" and provide bounded view primitives for progressive disclosure.\n",
    "\n",
    "**Stage 1**: Meta-graph scaffolding with navigation indexes  \n",
    "**Stage 2**: Bounded view primitives for safe graph exploration\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "- **Handles, not dumps**: Return graph handles with bounded view operations\n",
    "- **Meta-graph scaffolding**: Build navigation indexes (labels, hierarchy, properties)\n",
    "- **Progressive disclosure**: Small summaries guide exploration\n",
    "- **RLM-compatible**: Works with namespace-explicit `rlm_run()`\n",
    "\n",
    "### Context Model\n",
    "\n",
    "From the trajectory document:\n",
    "> The *root model never gets a graph dump*. It gets a handle name (e.g. `ont`, `res_0`) and uses bounded view operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79078f06",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, URIRef, Literal, SKOS, DCTERMS, FOAF\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from fastcore.basics import AttrDict\n",
    "from itertools import islice\n",
    "\n",
    "# Additional namespaces for ontology metadata (Widoco guide)\n",
    "VANN = Namespace('http://purl.org/vocab/vann/')\n",
    "DC = Namespace('http://purl.org/dc/elements/1.1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9576dd0",
   "metadata": {},
   "source": [
    "## Graph Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_ontology(path: str | Path, ns: dict, name: str = 'ont') -> str:\n",
    "    \"\"\"Load an RDF ontology file into namespace as a Graph handle.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to ontology file (.ttl, .rdf, .owl)\n",
    "        ns: Namespace dict where Graph will be stored\n",
    "        name: Variable name for the Graph handle\n",
    "        \n",
    "    Returns:\n",
    "        Summary string describing what was loaded\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    g.parse(path)\n",
    "    ns[name] = g\n",
    "    \n",
    "    return f\"Loaded {len(g)} triples from {Path(path).name} into '{name}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f00a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1664 triples from prov.ttl into 'prov_ont'\n",
      "✓ Loaded 1664 triples\n"
     ]
    }
   ],
   "source": [
    "# Test loading prov.ttl\n",
    "test_ns = {}\n",
    "result = load_ontology('ontology/prov.ttl', test_ns, name='prov_ont')\n",
    "print(result)\n",
    "assert 'prov_ont' in test_ns\n",
    "assert isinstance(test_ns['prov_ont'], Graph)\n",
    "assert len(test_ns['prov_ont']) > 0\n",
    "print(f\"✓ Loaded {len(test_ns['prov_ont'])} triples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196d6ec",
   "metadata": {},
   "source": [
    "## Meta-Graph Navigation\n",
    "\n",
    "Build navigation scaffolding from a Graph to enable progressive disclosure.\n",
    "This is what goes in the REPL environment, not the graph itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50704ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Namespace-specific label property mappings (Widoco guide)\n",
    "NAMESPACE_LABEL_MAPPINGS = {\n",
    "    'skos': [SKOS.prefLabel, SKOS.altLabel],\n",
    "    'dc': [DC.title],\n",
    "    'dcterms': [DCTERMS.title],\n",
    "    'foaf': [FOAF.name],\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class GraphMeta:\n",
    "    \"\"\"Meta-graph navigation scaffolding for an RDF Graph.\n",
    "    \n",
    "    This is REPL-resident and provides bounded views over the graph.\n",
    "    Indexes discovered in dialogs/inspect_tools.ipynb exploration.\n",
    "    \"\"\"\n",
    "    graph: Graph\n",
    "    name: str = 'ont'\n",
    "    \n",
    "    # Computed lazily\n",
    "    _namespaces: dict = field(default=None, init=False, repr=False)\n",
    "    _classes: list = field(default=None, init=False, repr=False)\n",
    "    _properties: list = field(default=None, init=False, repr=False)\n",
    "    _individuals: list = field(default=None, init=False, repr=False)\n",
    "    _labels: dict = field(default=None, init=False, repr=False)\n",
    "    _by_label: dict = field(default=None, init=False, repr=False)\n",
    "    _subs: dict = field(default=None, init=False, repr=False)\n",
    "    _supers: dict = field(default=None, init=False, repr=False)\n",
    "    _doms: dict = field(default=None, init=False, repr=False)\n",
    "    _rngs: dict = field(default=None, init=False, repr=False)\n",
    "    _pred_freq: Counter = field(default=None, init=False, repr=False)\n",
    "    \n",
    "    @property\n",
    "    def triple_count(self) -> int:\n",
    "        \"\"\"Total number of triples in graph.\"\"\"\n",
    "        return len(self.graph)\n",
    "    \n",
    "    @property\n",
    "    def namespaces(self) -> dict:\n",
    "        \"\"\"Get namespace prefix bindings.\"\"\"\n",
    "        if self._namespaces is None:\n",
    "            self._namespaces = {prefix: str(ns) for prefix, ns in self.graph.namespaces()}\n",
    "        return self._namespaces\n",
    "    \n",
    "    @property\n",
    "    def classes(self) -> list:\n",
    "        \"\"\"Get all OWL/RDFS classes (URIs only, sorted).\"\"\"\n",
    "        if self._classes is None:\n",
    "            classes = set(\n",
    "                self.graph.subjects(RDF.type, OWL.Class)\n",
    "            ).union(\n",
    "                self.graph.subjects(RDF.type, RDFS.Class)\n",
    "            )\n",
    "            self._classes = sorted([str(c) for c in classes])\n",
    "        return self._classes\n",
    "    \n",
    "    @property\n",
    "    def properties(self) -> list:\n",
    "        \"\"\"Get all properties (URIs only, sorted).\"\"\"\n",
    "        if self._properties is None:\n",
    "            props = set(\n",
    "                self.graph.subjects(RDF.type, OWL.ObjectProperty)\n",
    "            ).union(\n",
    "                self.graph.subjects(RDF.type, OWL.DatatypeProperty)\n",
    "            ).union(\n",
    "                self.graph.subjects(RDF.type, OWL.AnnotationProperty)\n",
    "            ).union(\n",
    "                self.graph.subjects(RDF.type, RDF.Property)\n",
    "            )\n",
    "            self._properties = sorted([str(p) for p in props])\n",
    "        return self._properties\n",
    "    \n",
    "    @property\n",
    "    def individuals(self) -> list:\n",
    "        \"\"\"Get all named individuals (URIs only, sorted).\"\"\"\n",
    "        if self._individuals is None:\n",
    "            inds = set(self.graph.subjects(RDF.type, OWL.NamedIndividual))\n",
    "            self._individuals = sorted([str(i) for i in inds])\n",
    "        return self._individuals\n",
    "    \n",
    "    @property\n",
    "    def labels(self) -> dict:\n",
    "        \"\"\"Get label index: URI -> label string.\n",
    "        \n",
    "        Uses prefix-guided label indexing per Widoco guide:\n",
    "        - Always indexes rdfs:label\n",
    "        - Conditionally indexes namespace-specific label properties\n",
    "          (skos:prefLabel, dc:title, etc.) based on bound namespaces\n",
    "        \"\"\"\n",
    "        if self._labels is None:\n",
    "            self._labels = {}\n",
    "            \n",
    "            # Always index rdfs:label\n",
    "            for s, o in self.graph.subject_objects(RDFS.label):\n",
    "                self._labels[str(s)] = str(o)\n",
    "            \n",
    "            # Prefix-guided indexing for bound namespaces\n",
    "            for prefix, predicates in NAMESPACE_LABEL_MAPPINGS.items():\n",
    "                if prefix in self.namespaces:\n",
    "                    for pred in predicates:\n",
    "                        for s, o in self.graph.subject_objects(pred):\n",
    "                            # Only add if not already labeled (rdfs:label takes precedence)\n",
    "                            if str(s) not in self._labels:\n",
    "                                self._labels[str(s)] = str(o)\n",
    "        \n",
    "        return self._labels\n",
    "    \n",
    "    @property\n",
    "    def by_label(self) -> dict:\n",
    "        \"\"\"Get inverted label index: label_text -> list of URIs.\"\"\"\n",
    "        if self._by_label is None:\n",
    "            inv = defaultdict(list)\n",
    "            for uri, lbl in self.labels.items():\n",
    "                inv[lbl.lower()].append(uri)\n",
    "            self._by_label = dict(inv)\n",
    "        return self._by_label\n",
    "    \n",
    "    @property\n",
    "    def subs(self) -> dict:\n",
    "        \"\"\"Get subclass relationships: superclass_uri -> list of subclass_uris.\"\"\"\n",
    "        if self._subs is None:\n",
    "            subs_dict = defaultdict(list)\n",
    "            for s, _, o in self.graph.triples((None, RDFS.subClassOf, None)):\n",
    "                if isinstance(o, URIRef):\n",
    "                    subs_dict[str(o)].append(str(s))\n",
    "            self._subs = dict(subs_dict)\n",
    "        return self._subs\n",
    "    \n",
    "    @property\n",
    "    def supers(self) -> dict:\n",
    "        \"\"\"Get superclass relationships: subclass_uri -> list of superclass_uris.\"\"\"\n",
    "        if self._supers is None:\n",
    "            supers_dict = defaultdict(list)\n",
    "            for s, _, o in self.graph.triples((None, RDFS.subClassOf, None)):\n",
    "                if isinstance(o, URIRef):\n",
    "                    supers_dict[str(s)].append(str(o))\n",
    "            self._supers = dict(supers_dict)\n",
    "        return self._supers\n",
    "    \n",
    "    @property\n",
    "    def doms(self) -> dict:\n",
    "        \"\"\"Get property domains: property_uri -> domain_uri.\"\"\"\n",
    "        if self._doms is None:\n",
    "            self._doms = {str(s): str(o) for s, _, o in self.graph.triples((None, RDFS.domain, None))}\n",
    "        return self._doms\n",
    "    \n",
    "    @property\n",
    "    def rngs(self) -> dict:\n",
    "        \"\"\"Get property ranges: property_uri -> range_uri.\"\"\"\n",
    "        if self._rngs is None:\n",
    "            self._rngs = {str(s): str(o) for s, _, o in self.graph.triples((None, RDFS.range, None))}\n",
    "        return self._rngs\n",
    "    \n",
    "    @property\n",
    "    def pred_freq(self) -> Counter:\n",
    "        \"\"\"Get predicate frequency counts (cached).\"\"\"\n",
    "        if self._pred_freq is None:\n",
    "            from collections import Counter\n",
    "            self._pred_freq = Counter(str(p) for s, p, o in self.graph.triples((None, None, None)))\n",
    "        return self._pred_freq\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Generate a summary of the graph for display.\"\"\"\n",
    "        lines = [\n",
    "            f\"Graph '{self.name}': {self.triple_count:,} triples\",\n",
    "            f\"Classes: {len(self.classes)}\",\n",
    "            f\"Properties: {len(self.properties)}\",\n",
    "            f\"Individuals: {len(self.individuals)}\",\n",
    "            f\"Namespaces: {', '.join(self.namespaces.keys())}\"\n",
    "        ]\n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 'prov': 1,664 triples\n",
      "Classes: 59\n",
      "Properties: 89\n",
      "Individuals: 1\n",
      "Namespaces: brick, csvw, dc, dcat, dcmitype, dcterms, dcam, doap, foaf, geo, odrl, org, prof, qb, schema, sh, skos, sosa, ssn, time, vann, void, wgs, owl, rdf, rdfs, xsd, xml, prov\n",
      "\n",
      "Sample classes (first 5): ['http://www.w3.org/2002/07/owl#Thing', 'http://www.w3.org/ns/prov#Accept', 'http://www.w3.org/ns/prov#Activity', 'http://www.w3.org/ns/prov#ActivityInfluence', 'http://www.w3.org/ns/prov#Agent']\n",
      "Sample properties (first 5): ['http://www.w3.org/2000/01/rdf-schema#comment', 'http://www.w3.org/2000/01/rdf-schema#isDefinedBy', 'http://www.w3.org/2000/01/rdf-schema#label', 'http://www.w3.org/2000/01/rdf-schema#seeAlso', 'http://www.w3.org/2002/07/owl#topObjectProperty']\n",
      "Namespaces: ['brick', 'csvw', 'dc', 'dcat', 'dcmitype', 'dcterms', 'dcam', 'doap', 'foaf', 'geo', 'odrl', 'org', 'prof', 'qb', 'schema', 'sh', 'skos', 'sosa', 'ssn', 'time', 'vann', 'void', 'wgs', 'owl', 'rdf', 'rdfs', 'xsd', 'xml', 'prov']\n"
     ]
    }
   ],
   "source": [
    "# Test GraphMeta with prov ontology\n",
    "prov_g = test_ns['prov_ont']\n",
    "meta = GraphMeta(prov_g, name='prov')\n",
    "\n",
    "print(meta.summary())\n",
    "print()\n",
    "print(f\"Sample classes (first 5): {meta.classes[:5]}\")\n",
    "print(f\"Sample properties (first 5): {meta.properties[:5]}\")\n",
    "print(f\"Namespaces: {list(meta.namespaces.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b80e1",
   "metadata": {},
   "source": [
    "## Bounded View Functions (Stage 1)\n",
    "\n",
    "Basic operations on GraphMeta that return small, bounded summaries:\n",
    "\n",
    "- **graph_stats()**: Overall graph statistics\n",
    "- **search_by_label()**: Simple label-based search\n",
    "- **describe_entity()**: Get entity description with sample triples\n",
    "\n",
    "These provide the foundation for progressive disclosure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def graph_stats(meta: GraphMeta) -> str:\n",
    "    \"\"\"Get graph statistics summary.\"\"\"\n",
    "    return meta.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def search_entity(meta: GraphMeta, query: str, limit: int = 10,\n",
    "                  search_in: str = 'all') -> list:\n",
    "    \"\"\"Search for entities by label, IRI, or localname.\n",
    "\n",
    "    Args:\n",
    "        meta: GraphMeta to search\n",
    "        query: Search string (case-insensitive substring match)\n",
    "        limit: Maximum results to return\n",
    "        search_in: Where to search - 'label', 'iri', 'localname', or 'all'\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: [{'uri': str, 'label': str, 'match_type': str}, ...]\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    matches = []\n",
    "\n",
    "    # Search in labels\n",
    "    if search_in in ('label', 'all'):\n",
    "        for uri, label in meta.labels.items():\n",
    "            if query_lower in label.lower():\n",
    "                matches.append({\n",
    "                    'uri': uri,\n",
    "                    'label': label,\n",
    "                    'match_type': 'label'\n",
    "                })\n",
    "\n",
    "    # Search in full IRIs\n",
    "    if search_in in ('iri', 'all'):\n",
    "        all_uris = set(meta.classes + meta.properties + meta.individuals)\n",
    "        for uri in all_uris:\n",
    "            if query_lower in uri.lower() and not any(m['uri'] == uri for m in matches):\n",
    "                label = meta.labels.get(uri, uri)\n",
    "                matches.append({\n",
    "                    'uri': uri,\n",
    "                    'label': label,\n",
    "                    'match_type': 'iri'\n",
    "                })\n",
    "\n",
    "    # Search in localnames (fragment or last path segment)\n",
    "    if search_in in ('localname', 'all'):\n",
    "        all_uris = set(meta.classes + meta.properties + meta.individuals)\n",
    "        for uri in all_uris:\n",
    "            # Extract localname (after # or last /)\n",
    "            if '#' in uri:\n",
    "                localname = uri.split('#')[-1]\n",
    "            else:\n",
    "                localname = uri.split('/')[-1]\n",
    "\n",
    "            if query_lower in localname.lower() and not any(m['uri'] == uri for m in matches):\n",
    "                label = meta.labels.get(uri, uri)\n",
    "                matches.append({\n",
    "                    'uri': uri,\n",
    "                    'label': label,\n",
    "                    'match_type': 'localname'\n",
    "                })\n",
    "\n",
    "    return matches[:limit]\n",
    "\n",
    "\n",
    "def search_by_label(meta: GraphMeta, search: str, limit: int = 10) -> list:\n",
    "    \"\"\"Search for entities by label substring (case-insensitive).\n",
    "\n",
    "    Backward-compatible wrapper around search_entity().\n",
    "\n",
    "    Args:\n",
    "        meta: GraphMeta to search\n",
    "        search: Substring to search for in labels\n",
    "        limit: Maximum results to return\n",
    "\n",
    "    Returns:\n",
    "        List of (URI, label) tuples\n",
    "    \"\"\"\n",
    "    results = search_entity(meta, search, limit=limit, search_in='label')\n",
    "    return [(r['uri'], r['label']) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 matches for 'activity':\n",
      "  Activity: http://www.w3.org/ns/prov#Activity (label)\n",
      "  ActivityInfluence: http://www.w3.org/ns/prov#ActivityInfluence (label)\n",
      "  activity: http://www.w3.org/ns/prov#activity (label)\n",
      "  hadActivity: http://www.w3.org/ns/prov#hadActivity (label)\n",
      "  activityOfInfluence: http://www.w3.org/ns/prov#activityOfInfluence (label)\n",
      "\n",
      "Search by IRI only:\n",
      "  Attribution: http://www.w3.org/ns/prov#Attribution\n",
      "  invalidatedAtTime: http://www.w3.org/ns/prov#invalidatedAtTime\n",
      "  Derivation: http://www.w3.org/ns/prov#Derivation\n",
      "\n",
      "Backward compatibility test:\n",
      "Found 5 matches using search_by_label():\n",
      "  Activity: http://www.w3.org/ns/prov#Activity\n",
      "  ActivityInfluence: http://www.w3.org/ns/prov#ActivityInfluence\n",
      "  activity: http://www.w3.org/ns/prov#activity\n",
      "  hadActivity: http://www.w3.org/ns/prov#hadActivity\n",
      "  activityOfInfluence: http://www.w3.org/ns/prov#activityOfInfluence\n"
     ]
    }
   ],
   "source": [
    "# Test search_entity\n",
    "results = search_entity(meta, 'activity', limit=5)\n",
    "print(f\"Found {len(results)} matches for 'activity':\")\n",
    "for r in results:\n",
    "    print(f\"  {r['label']}: {r['uri']} ({r['match_type']})\")\n",
    "\n",
    "# Test different search modes\n",
    "print(\"\\nSearch by IRI only:\")\n",
    "iri_results = search_entity(meta, 'prov', search_in='iri', limit=3)\n",
    "for r in iri_results:\n",
    "    print(f\"  {r['label']}: {r['uri']}\")\n",
    "\n",
    "# Test backward compatibility\n",
    "print(\"\\nBackward compatibility test:\")\n",
    "legacy_results = search_by_label(meta, 'activity', limit=5)\n",
    "print(f\"Found {len(legacy_results)} matches using search_by_label():\")\n",
    "for uri, label in legacy_results:\n",
    "    print(f\"  {label}: {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972bd98",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef _expand_uri(meta: GraphMeta, uri: str) -> URIRef:\n    \"\"\"Expand prefixed URI (e.g., 'prov:Activity') to full URI.\n\n    Args:\n        meta: GraphMeta containing namespace bindings\n        uri: URI string (may be prefixed like 'prov:Activity' or full URI)\n\n    Returns:\n        URIRef with expanded URI\n    \"\"\"\n    from rdflib import URIRef\n\n    # If already a full URI, return as-is\n    if uri.startswith('http://') or uri.startswith('https://'):\n        return URIRef(uri)\n\n    # Try to expand as CURIE (prefix:localname)\n    if ':' in uri:\n        try:\n            return meta.graph.namespace_manager.expand_curie(uri)\n        except:\n            pass  # Fall through to URIRef if expansion fails\n\n    return URIRef(uri)\n\ndef describe_entity(meta: GraphMeta, uri: str, limit: int = 20) -> dict:\n    \"\"\"Get bounded description of an entity.\n\n    Args:\n        meta: GraphMeta containing the entity\n        uri: URI of entity to describe (supports prefixed forms like 'prov:Activity')\n        limit: Max number of triples to include\n\n    Returns:\n        Dict with label, types, and sample triples\n    \"\"\"\n    # Expand URI (handles prefixed forms like 'prov:Activity')\n    entity = _expand_uri(meta, uri)\n    uri_str = str(entity)\n\n    # Get label\n    label = meta.labels.get(uri_str, uri_str)\n\n    # Get types\n    types = [str(t) for t in meta.graph.objects(entity, RDF.type)]\n\n    # Get sample of outgoing triples using islice\n    outgoing = []\n    for p, o in islice(meta.graph.predicate_objects(entity), limit):\n        outgoing.append((str(p), str(o)))\n\n    # Get comment if available\n    comments = list(meta.graph.objects(entity, RDFS.comment))\n    comment = str(comments[0]) if comments else None\n\n    return {\n        'uri': uri_str,\n        'label': label,\n        'types': types,\n        'comment': comment,\n        'outgoing_sample': outgoing\n    }\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6892e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Activity\n",
      "Types: ['http://www.w3.org/2002/07/owl#Class']\n",
      "No comment\n",
      "Outgoing triples: 10\n"
     ]
    }
   ],
   "source": [
    "# Test describe_entity\n",
    "# Find the Activity class\n",
    "activity_uri = 'http://www.w3.org/ns/prov#Activity'\n",
    "desc = describe_entity(meta, activity_uri)\n",
    "\n",
    "print(f\"Label: {desc['label']}\")\n",
    "print(f\"Types: {desc['types']}\")\n",
    "print(f\"Comment: {desc['comment'][:100]}...\" if desc['comment'] else \"No comment\")\n",
    "print(f\"Outgoing triples: {len(desc['outgoing_sample'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36464bb4",
   "metadata": {},
   "source": [
    "### Stage 2: Progressive Disclosure Primitives\n",
    "\n",
    "Advanced bounded view operations that enable root models to explore graphs iteratively:\n",
    "\n",
    "- **search_entity()**: Multi-mode entity search (label/IRI/localname)\n",
    "- **probe_relationships()**: One-hop neighbor exploration with filtering\n",
    "- **find_path()**: BFS path finding between entities\n",
    "- **predicate_frequency()**: Usage analysis for understanding graph structure\n",
    "\n",
    "These primitives answer questions like:\n",
    "- \"Is X defined?\" → `search_entity()`\n",
    "- \"What connects A to B?\" → `find_path()`\n",
    "- \"What are the most important predicates?\" → `predicate_frequency()`\n",
    "- \"What does X relate to?\" → `probe_relationships()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859c169",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef probe_relationships(meta: GraphMeta, uri: str, predicate: str = None,\n                        direction: str = 'both', limit: int = 20) -> dict:\n    \"\"\"Get one-hop neighbors of an entity, optionally filtered by predicate.\n\n    Args:\n        meta: GraphMeta containing the entity\n        uri: URI of entity to probe (supports prefixed forms like 'prov:Activity')\n        predicate: Optional predicate URI to filter by (supports prefixed forms)\n        direction: 'out', 'in', or 'both' (default: 'both')\n        limit: Maximum neighbors to return per direction\n\n    Returns:\n        {\n            'uri': str, 'label': str,\n            'outgoing': [{'predicate': str, 'pred_label': str,\n                          'object': str, 'obj_label': str}, ...],\n            'incoming': [{'subject': str, 'subj_label': str,\n                          'predicate': str, 'pred_label': str}, ...],\n            'outgoing_count': int, 'incoming_count': int\n        }\n    \"\"\"\n    from rdflib import URIRef\n\n    # Expand URIs (handles prefixed forms)\n    entity = _expand_uri(meta, uri)\n    uri_str = str(entity)\n    entity_label = meta.labels.get(uri_str, uri_str)\n\n    outgoing = []\n    incoming = []\n\n    # Get outgoing triples (entity as subject)\n    if direction in ('out', 'both'):\n        pred_filter = _expand_uri(meta, predicate) if predicate else None\n        triples = list(meta.graph.triples((entity, pred_filter, None)))\n\n        for s, p, o in triples[:limit]:\n            pred_uri = str(p)\n            obj_uri = str(o)\n            outgoing.append({\n                'predicate': pred_uri,\n                'pred_label': meta.labels.get(pred_uri, pred_uri),\n                'object': obj_uri,\n                'obj_label': meta.labels.get(obj_uri, obj_uri)\n            })\n\n    # Get incoming triples (entity as object)\n    if direction in ('in', 'both'):\n        pred_filter = _expand_uri(meta, predicate) if predicate else None\n        triples = list(meta.graph.triples((None, pred_filter, entity)))\n\n        for s, p, o in triples[:limit]:\n            subj_uri = str(s)\n            pred_uri = str(p)\n            incoming.append({\n                'subject': subj_uri,\n                'subj_label': meta.labels.get(subj_uri, subj_uri),\n                'predicate': pred_uri,\n                'pred_label': meta.labels.get(pred_uri, pred_uri)\n            })\n\n    # Count total (not just limited sample)\n    pred_filter = _expand_uri(meta, predicate) if predicate else None\n    if direction in ('out', 'both'):\n        outgoing_count = sum(1 for _ in meta.graph.triples((entity, pred_filter, None)))\n    else:\n        outgoing_count = 0\n\n    if direction in ('in', 'both'):\n        incoming_count = sum(1 for _ in meta.graph.triples((None, pred_filter, entity)))\n    else:\n        incoming_count = 0\n\n    return {\n        'uri': uri_str,\n        'label': entity_label,\n        'outgoing': outgoing,\n        'incoming': incoming,\n        'outgoing_count': outgoing_count,\n        'incoming_count': incoming_count\n    }\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1d5fc",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef find_path(meta: GraphMeta, source: str, target: str,\n              max_depth: int = 2, limit: int = 10) -> list:\n    \"\"\"Find predicates connecting two entities using BFS.\n\n    Answers \"What predicates connect A to B?\"\n\n    Args:\n        meta: GraphMeta to search\n        source: Source entity URI (supports prefixed forms like 'prov:Activity')\n        target: Target entity URI (supports prefixed forms like 'prov:Entity')\n        max_depth: Maximum path length (default: 2)\n        limit: Maximum paths to return\n\n    Returns:\n        List of paths, each path is list of steps:\n        [{'from': uri, 'predicate': uri, 'to': uri, 'direction': 'out'|'in'}, ...]\n    \"\"\"\n    from rdflib import URIRef\n    from collections import deque\n\n    # Expand URIs (handles prefixed forms)\n    source_uri = _expand_uri(meta, source)\n    target_uri = _expand_uri(meta, target)\n\n    # BFS to find paths\n    queue = deque([(source_uri, [])])  # (current_node, path_so_far)\n    visited = set()\n    paths_found = []\n\n    while queue and len(paths_found) < limit:\n        current, path = queue.popleft()\n\n        # Skip if we've exceeded max depth\n        if len(path) >= max_depth:\n            continue\n\n        # Skip if visited (but allow revisiting in different paths up to limit)\n        path_key = (current, tuple(step['predicate'] for step in path))\n        if path_key in visited:\n            continue\n        visited.add(path_key)\n\n        # Check if we reached the target\n        if current == target_uri:\n            paths_found.append(path)\n            continue\n\n        # Explore outgoing edges\n        for s, p, o in meta.graph.triples((current, None, None)):\n            if isinstance(o, URIRef):\n                step = {\n                    'from': str(s),\n                    'predicate': str(p),\n                    'to': str(o),\n                    'direction': 'out'\n                }\n                queue.append((o, path + [step]))\n\n        # Explore incoming edges\n        for s, p, o in meta.graph.triples((None, None, current)):\n            if isinstance(s, URIRef):\n                step = {\n                    'from': str(current),\n                    'predicate': str(p),\n                    'to': str(s),\n                    'direction': 'in'\n                }\n                queue.append((s, path + [step]))\n\n    return paths_found\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predicate_frequency(meta: GraphMeta, limit: int = 20,\n",
    "                        predicate_type: str = None) -> list:\n",
    "    \"\"\"Get predicates ranked by frequency of use.\n",
    "\n",
    "    Args:\n",
    "        meta: GraphMeta to analyze\n",
    "        limit: Maximum predicates to return\n",
    "        predicate_type: Optional filter - 'object', 'datatype', 'annotation'\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: [{'predicate': str, 'label': str, 'count': int,\n",
    "                         'sample_subject': str, 'sample_object': str}, ...]\n",
    "    \"\"\"\n",
    "    from rdflib import URIRef\n",
    "    from collections import Counter\n",
    "\n",
    "    # Get frequency counter (cached)\n",
    "    freq = meta.pred_freq\n",
    "\n",
    "    # Filter by type if requested\n",
    "    if predicate_type:\n",
    "        type_props = set()\n",
    "\n",
    "        if predicate_type == 'object':\n",
    "            type_props = set(meta.graph.subjects(RDF.type, OWL.ObjectProperty))\n",
    "        elif predicate_type == 'datatype':\n",
    "            type_props = set(meta.graph.subjects(RDF.type, OWL.DatatypeProperty))\n",
    "        elif predicate_type == 'annotation':\n",
    "            type_props = set(meta.graph.subjects(RDF.type, OWL.AnnotationProperty))\n",
    "\n",
    "        # Filter frequency counts to only include predicates of this type\n",
    "        # FIX: Wrap in Counter so most_common() works\n",
    "        filtered_freq = Counter({str(p): count for p, count in freq.items() if URIRef(p) in type_props})\n",
    "    else:\n",
    "        filtered_freq = freq\n",
    "\n",
    "    # Get top N by frequency\n",
    "    top_predicates = filtered_freq.most_common(limit)\n",
    "\n",
    "    # Build result list with samples\n",
    "    results = []\n",
    "    for pred_uri, count in top_predicates:\n",
    "        # Get label\n",
    "        pred_label = meta.labels.get(pred_uri, pred_uri)\n",
    "\n",
    "        # Get sample triple\n",
    "        sample_triple = next(meta.graph.triples((None, URIRef(pred_uri), None)), None)\n",
    "\n",
    "        if sample_triple:\n",
    "            sample_subj = str(sample_triple[0])\n",
    "            sample_obj = str(sample_triple[2])\n",
    "        else:\n",
    "            sample_subj = None\n",
    "            sample_obj = None\n",
    "\n",
    "        results.append({\n",
    "            'predicate': pred_uri,\n",
    "            'label': pred_label,\n",
    "            'count': count,\n",
    "            'sample_subject': sample_subj,\n",
    "            'sample_object': sample_obj\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing: Activity\n",
      "Outgoing relationships: 10 total, showing 5\n",
      "  --http://www.w3.org/1999/02/22-rdf-syntax-ns#type--> http://www.w3.org/2002/07/owl#Class\n",
      "  --http://www.w3.org/2000/01/rdf-schema#isDefinedBy--> W3C PROVenance Interchange Ontology (PROV-O)\n",
      "  --http://www.w3.org/2000/01/rdf-schema#label--> Activity\n",
      "\n",
      "Incoming relationships: 34 total, showing 5\n",
      "  <--http://www.w3.org/2000/01/rdf-schema#range-- activity\n",
      "  <--http://www.w3.org/1999/02/22-rdf-syntax-ns#first-- n0fe42a034f254bbc9cc97fe482231e2cb5\n",
      "  <--http://www.w3.org/2000/01/rdf-schema#domain-- endedAtTime\n",
      "\n",
      "\n",
      "Paths from Activity to Entity:\n",
      "Path 1:\n",
      "  --> http://www.w3.org/2002/07/owl#disjointWith\n"
     ]
    }
   ],
   "source": [
    "# Test probe_relationships\n",
    "activity_uri = 'http://www.w3.org/ns/prov#Activity'\n",
    "probe_result = probe_relationships(meta, activity_uri, limit=5)\n",
    "\n",
    "print(f\"Probing: {probe_result['label']}\")\n",
    "print(f\"Outgoing relationships: {probe_result['outgoing_count']} total, showing {len(probe_result['outgoing'])}\")\n",
    "for rel in probe_result['outgoing'][:3]:\n",
    "    print(f\"  --{rel['pred_label']}--> {rel['obj_label']}\")\n",
    "\n",
    "print(f\"\\nIncoming relationships: {probe_result['incoming_count']} total, showing {len(probe_result['incoming'])}\")\n",
    "for rel in probe_result['incoming'][:3]:\n",
    "    print(f\"  <--{rel['pred_label']}-- {rel['subj_label']}\")\n",
    "\n",
    "# Test find_path\n",
    "# Find path between two PROV classes\n",
    "entity_uri = 'http://www.w3.org/ns/prov#Entity'\n",
    "paths = find_path(meta, activity_uri, entity_uri, max_depth=2, limit=3)\n",
    "\n",
    "print(f\"\\n\\nPaths from Activity to Entity:\")\n",
    "if paths:\n",
    "    for i, path in enumerate(paths, 1):\n",
    "        print(f\"Path {i}:\")\n",
    "        for step in path:\n",
    "            direction_sym = '-->' if step['direction'] == 'out' else '<--'\n",
    "            pred_label = meta.labels.get(step['predicate'], step['predicate'])\n",
    "            print(f\"  {direction_sym} {pred_label}\")\n",
    "else:\n",
    "    print(\"  No paths found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60400f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predicates by frequency:\n",
      "   184 uses - http://www.w3.org/2000/01/rdf-schema#isDefinedBy\n",
      "   175 uses - http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "   161 uses - http://www.w3.org/2000/01/rdf-schema#label\n",
      "   107 uses - http://www.w3.org/2000/01/rdf-schema#comment\n",
      "   104 uses - http://www.w3.org/ns/prov#category\n",
      "    85 uses - http://www.w3.org/ns/prov#component\n",
      "    64 uses - http://www.w3.org/2000/01/rdf-schema#domain\n",
      "    63 uses - http://www.w3.org/ns/prov#definition\n",
      "    60 uses - http://www.w3.org/2000/01/rdf-schema#range\n",
      "    55 uses - http://www.w3.org/2000/01/rdf-schema#subClassOf\n",
      "\n",
      "Top 5 object properties:\n",
      "     7 uses - wasDerivedFrom\n",
      "     3 uses - wasRevisionOf\n",
      "     3 uses - specializationOf\n"
     ]
    }
   ],
   "source": [
    "# Test predicate_frequency\n",
    "print(\"Top 10 predicates by frequency:\")\n",
    "freq_results = predicate_frequency(meta, limit=10)\n",
    "for r in freq_results:\n",
    "    print(f\"  {r['count']:4d} uses - {r['label']}\")\n",
    "\n",
    "# Test filtering by predicate type\n",
    "print(\"\\nTop 5 object properties:\")\n",
    "obj_props = predicate_frequency(meta, limit=5, predicate_type='object')\n",
    "for r in obj_props:\n",
    "    print(f\"  {r['count']:4d} uses - {r['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c439cb",
   "metadata": {},
   "source": [
    "## Additional Exploration Functions\n",
    "\n",
    "Functions discovered in `dialogs/inspect_tools.ipynb` for deeper ontology exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ont_describe(ont: str, uri: str, name: str = 'desc', ns: dict = None,\n",
    "                 limit: int = 100) -> str:\n",
    "    \"\"\"Get triples about a URI, store in namespace.\n",
    "    \n",
    "    Returns both triples where URI is subject and where it's object.\n",
    "    \n",
    "    Args:\n",
    "        ont: Name of ontology variable in namespace\n",
    "        uri: URI to describe\n",
    "        name: Variable name for storing result\n",
    "        ns: Namespace dict\n",
    "        limit: Maximum triples to return per direction (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    if ns is None: ns = globals()\n",
    "    o = ns[ont]\n",
    "    u = URIRef(uri) if not isinstance(uri, URIRef) else uri\n",
    "    \n",
    "    # Get triples where URI is subject (bounded)\n",
    "    subj_triples = [(str(s), str(p), str(obj)) \n",
    "                    for s, p, obj in islice(o.graph.triples((u, None, None)), limit)]\n",
    "    \n",
    "    # Get triples where URI is object (bounded)\n",
    "    obj_triples = [(str(s), str(p), str(obj)) \n",
    "                   for s, p, obj in islice(o.graph.triples((None, None, u)), limit)]\n",
    "    \n",
    "    result = {\n",
    "        'as_subject': subj_triples,\n",
    "        'as_object': obj_triples\n",
    "    }\n",
    "    ns[name] = result\n",
    "    return f\"Stored {len(subj_triples)} + {len(obj_triples)} triples about '{uri}' into '{name}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4471fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ont_meta(ont: str, name: str = 'meta', ns: dict = None) -> str:\n",
    "    \"\"\"Extract ontology metadata (prefixes, annotation predicates, imports).\n",
    "    \n",
    "    Args:\n",
    "        ont: Name of ontology variable in namespace\n",
    "        name: Variable name for storing result\n",
    "        ns: Namespace dict\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    if ns is None: ns = globals()\n",
    "    o = ns[ont]\n",
    "    \n",
    "    prefixes = dict(o.graph.namespaces())\n",
    "    ann_preds = set(str(p) for s, p, obj in o.graph.triples((None, None, None)) if isinstance(obj, Literal))\n",
    "    imports = [str(obj) for s, p, obj in o.graph.triples((None, OWL.imports, None))]\n",
    "    \n",
    "    res = AttrDict(\n",
    "        prefixes=prefixes,\n",
    "        ann_preds=list(ann_preds)[:50],  # Limit to first 50\n",
    "        imports=imports\n",
    "    )\n",
    "    ns[name] = res\n",
    "    return f\"Stored metadata into '{name}': {len(prefixes)} prefixes, {len(ann_preds)} annotation predicates, {len(imports)} imports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ont_roots(ont: str, name: str = 'roots', ns: dict = None) -> str:\n",
    "    \"\"\"Find root classes (no declared superclass), store in namespace.\n",
    "    \n",
    "    Args:\n",
    "        ont: Name of ontology variable in namespace\n",
    "        name: Variable name for storing result\n",
    "        ns: Namespace dict\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    if ns is None: ns = globals()\n",
    "    o = ns[ont]\n",
    "    \n",
    "    has_super = set(o.supers.keys())\n",
    "    roots = [str(c) for c in o.classes if str(c).startswith('http') and str(c) not in has_super]\n",
    "    \n",
    "    ns[name] = roots\n",
    "    return f\"Stored {len(roots)} root classes into '{name}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_ontology_context(path: str | Path, ns: dict, name: str = 'ont', dataset_meta=None) -> str:\n",
    "    \"\"\"Load ontology and create meta-graph for RLM use.\n",
    "    \n",
    "    This sets up both the Graph and GraphMeta in the namespace.\n",
    "    \n",
    "    NEW: Dataset integration - if dataset_meta provided, automatically mounts\n",
    "    the ontology into the dataset as onto/<name> graph.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to ontology file\n",
    "        ns: Namespace dict\n",
    "        name: Base name for graph handle\n",
    "        dataset_meta: Optional DatasetMeta for auto-mounting\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    # Load graph\n",
    "    load_msg = load_ontology(path, ns, name=name)\n",
    "    \n",
    "    # Create meta-graph\n",
    "    g = ns[name]\n",
    "    meta = GraphMeta(g, name=name)\n",
    "    ns[f\"{name}_meta\"] = meta\n",
    "    \n",
    "    # NEW: Auto-mount in dataset if provided\n",
    "    if dataset_meta is not None:\n",
    "        try:\n",
    "            from rlm.dataset import mount_ontology\n",
    "            mount_msg = mount_ontology(dataset_meta, ns, str(path), name)\n",
    "            load_msg += f\"\\n{mount_msg}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to mount ontology in dataset: {e}\")\n",
    "    \n",
    "    # FIX: Namespace helper functions by ontology name to avoid overwriting\n",
    "    # This allows multiple ontologies to coexist\n",
    "    from functools import partial\n",
    "    \n",
    "    # Bind existing functions\n",
    "    ns[f'{name}_graph_stats'] = partial(graph_stats, meta)\n",
    "    ns[f'{name}_search_by_label'] = partial(search_by_label, meta)\n",
    "    ns[f'{name}_describe_entity'] = partial(describe_entity, meta)\n",
    "    \n",
    "    # NEW: Bind Stage 2 bounded view primitives\n",
    "    ns[f'{name}_search_entity'] = partial(search_entity, meta)\n",
    "    ns[f'{name}_probe_relationships'] = partial(probe_relationships, meta)\n",
    "    ns[f'{name}_find_path'] = partial(find_path, meta)\n",
    "    ns[f'{name}_predicate_frequency'] = partial(predicate_frequency, meta)\n",
    "    \n",
    "    # Also bind without prefix for single-ontology convenience\n",
    "    # (will be overwritten if multiple ontologies loaded, but prefixed versions persist)\n",
    "    ns['graph_stats'] = partial(graph_stats, meta)\n",
    "    ns['search_by_label'] = partial(search_by_label, meta)\n",
    "    ns['describe_entity'] = partial(describe_entity, meta)\n",
    "    ns['search_entity'] = partial(search_entity, meta)\n",
    "    ns['probe_relationships'] = partial(probe_relationships, meta)\n",
    "    ns['find_path'] = partial(find_path, meta)\n",
    "    ns['predicate_frequency'] = partial(predicate_frequency, meta)\n",
    "    \n",
    "    return f\"{load_msg}\\nCreated meta-graph '{name}_meta' with {len(meta.classes)} classes, {len(meta.properties)} properties\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f256a",
   "metadata": {},
   "source": [
    "## Ontology Sense Building\n",
    "\n",
    "### What is a \"Sense Document\"?\n",
    "\n",
    "When an LLM needs to work with an ontology, loading the entire graph into context is wasteful and may exceed limits. Instead, we build a **sense document** - a compact summary that captures:\n",
    "\n",
    "- **Formalism**: Which OWL/RDFS/SKOS constructs are used\n",
    "- **Metadata structure**: Which annotation properties exist (labels, descriptions, etc.)\n",
    "- **Domain/scope**: What the ontology is about\n",
    "- **Navigation hints**: How to effectively search and traverse\n",
    "\n",
    "This approach was developed through experiments in `dialogs/inspect_tools.ipynb` exploring progressive disclosure patterns.\n",
    "\n",
    "### Why Sense Building Matters\n",
    "\n",
    "**Design Decision Response** (from ISSUE_ANALYSIS.md):\n",
    "> *GraphMeta.labels only uses rdfs:label* - This is a limitation because different ontologies use different annotation properties:\n",
    "> - `rdfs:label`, `skos:prefLabel`, `skos:altLabel` for labels\n",
    "> - `rdfs:comment`, `skos:definition`, `dcterms:description` for descriptions  \n",
    "> - `vann:preferredNamespacePrefix`, `owl:versionInfo` for metadata\n",
    "\n",
    "Rather than hardcode support for all possible properties, `build_sense()` **detects which annotation properties this specific ontology uses**, enabling intelligent search.\n",
    "\n",
    "### References\n",
    "\n",
    "- [Widoco Metadata Guide](https://github.com/dgarijo/Widoco/blob/master/doc/metadataGuide/guide.md) - Recommended ontology metadata properties\n",
    "- [Anthropic: Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) - Orchestrator-workers pattern\n",
    "- [Anthropic: Progressive Disclosure](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) - Context engineering strategy\n",
    "\n",
    "### Implementation Pattern\n",
    "\n",
    "The sense-building workflow (not agentic):\n",
    "1. **Metadata collection** - Extract prefixes, detect annotation predicates, find ontology-level metadata\n",
    "2. **Structural exploration** - Build hierarchy, property signatures, detect OWL axioms\n",
    "3. **LLM synthesis** - One LLM call to identify domain, patterns, navigation hints\n",
    "4. **Structured storage** - Store as retrievable AttrDict in REPL namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_sense(path: str, name: str = 'sense', ns: dict = None) -> str:\n",
    "    \"\"\"Build ontology sense document using workflow + LLM synthesis.\n",
    "    \n",
    "    Detects annotation properties per Widoco metadata guide:\n",
    "    - Label properties: rdfs:label, skos:prefLabel, skos:altLabel, dcterms:title\n",
    "    - Description properties: rdfs:comment, skos:definition, dcterms:description\n",
    "    - Ontology metadata: vann:preferredNamespacePrefix, owl:versionInfo, etc.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads ontology and extracts metadata/roots programmatically\n",
    "    2. Detects which annotation properties are actually used\n",
    "    3. Builds hierarchy (2 levels), property info, characteristics\n",
    "    4. Makes one LLM call to synthesize domain/scope/patterns/hints\n",
    "    5. Returns structured AttrDict stored in namespace\n",
    "    \n",
    "    Args:\n",
    "        path: Path to ontology file\n",
    "        name: Variable name for sense document (default: 'sense')\n",
    "        ns: Namespace dict\n",
    "        \n",
    "    Returns:\n",
    "        Summary string\n",
    "    \"\"\"\n",
    "    if ns is None: ns = {}\n",
    "    \n",
    "    # Derive ontology name from sense name\n",
    "    ont_name = name.replace('_sense', '').replace('sense', 'ont')\n",
    "    \n",
    "    # Setup ontology context (loads graph + creates GraphMeta)\n",
    "    setup_ontology_context(path, ns, name=ont_name)\n",
    "    \n",
    "    # Get metadata and roots\n",
    "    ont_meta(f'{ont_name}_meta', name=f'{ont_name}_metadata', ns=ns)\n",
    "    ont_roots(f'{ont_name}_meta', name=f'{ont_name}_roots', ns=ns)\n",
    "    \n",
    "    # Get references\n",
    "    meta_obj = ns[f'{ont_name}_meta']\n",
    "    metadata = ns[f'{ont_name}_metadata']\n",
    "    roots = ns[f'{ont_name}_roots']\n",
    "    g = meta_obj.graph\n",
    "    \n",
    "    # Detect ontology-level metadata (Widoco guide properties)\n",
    "    ont_uri = None\n",
    "    for s in g.subjects(RDF.type, OWL.Ontology):\n",
    "        ont_uri = str(s)\n",
    "        break\n",
    "    \n",
    "    ont_metadata = {}\n",
    "    if ont_uri:\n",
    "        ont_ref = URIRef(ont_uri)\n",
    "        ont_metadata['uri'] = ont_uri\n",
    "        \n",
    "        # Title/label\n",
    "        for title_prop in [DCTERMS.title, DC.title, RDFS.label]:\n",
    "            titles = list(g.objects(ont_ref, title_prop))\n",
    "            if titles:\n",
    "                ont_metadata['title'] = str(titles[0])\n",
    "                break\n",
    "        \n",
    "        # Description\n",
    "        for desc_prop in [DCTERMS.description, DC.description, RDFS.comment]:\n",
    "            descs = list(g.objects(ont_ref, desc_prop))\n",
    "            if descs:\n",
    "                ont_metadata['description'] = str(descs[0])\n",
    "                break\n",
    "        \n",
    "        # Version info\n",
    "        versions = list(g.objects(ont_ref, OWL.versionInfo))\n",
    "        if versions:\n",
    "            ont_metadata['version'] = str(versions[0])\n",
    "        \n",
    "        # Preferred namespace prefix\n",
    "        prefixes = list(g.objects(ont_ref, VANN.preferredNamespacePrefix))\n",
    "        if prefixes:\n",
    "            ont_metadata['preferred_prefix'] = str(prefixes[0])\n",
    "        \n",
    "        # Preferred namespace URI\n",
    "        ns_uris = list(g.objects(ont_ref, VANN.preferredNamespaceUri))\n",
    "        if ns_uris:\n",
    "            ont_metadata['preferred_namespace'] = str(ns_uris[0])\n",
    "    \n",
    "    # Detect which annotation properties are actually used (per Widoco guide)\n",
    "    label_props = []\n",
    "    desc_props = []\n",
    "    \n",
    "    # Check label properties\n",
    "    for label_prop in [RDFS.label, SKOS.prefLabel, SKOS.altLabel, DCTERMS.title, DC.title]:\n",
    "        if list(g.triples((None, label_prop, None))):\n",
    "            label_props.append(str(label_prop))\n",
    "    \n",
    "    # Check description properties\n",
    "    for desc_prop in [RDFS.comment, SKOS.definition, SKOS.note, SKOS.example, \n",
    "                      DCTERMS.description, DC.description]:\n",
    "        if list(g.triples((None, desc_prop, None))):\n",
    "            desc_props.append(str(desc_prop))\n",
    "    \n",
    "    # Build hierarchy (2 levels deep from roots)\n",
    "    hier = {}\n",
    "    for r in roots[:10]:  # Limit to first 10 roots\n",
    "        lbl = meta_obj.labels.get(r, r)\n",
    "        children = meta_obj.subs.get(r, [])\n",
    "        hier[lbl] = {\n",
    "            meta_obj.labels.get(c, c): [\n",
    "                meta_obj.labels.get(gc, gc) \n",
    "                for gc in meta_obj.subs.get(c, [])[:5]\n",
    "            ] \n",
    "            for c in children[:10]\n",
    "        }\n",
    "    \n",
    "    # Extract top properties with domains/ranges\n",
    "    top_props = []\n",
    "    for p in meta_obj.properties[:20]:\n",
    "        if p.startswith('http'):\n",
    "            prop_label = meta_obj.labels.get(p, p)\n",
    "            dom_uri = meta_obj.doms.get(p, '')\n",
    "            rng_uri = meta_obj.rngs.get(p, '')\n",
    "            dom_label = meta_obj.labels.get(dom_uri, dom_uri) if dom_uri else ''\n",
    "            rng_label = meta_obj.labels.get(rng_uri, rng_uri) if rng_uri else ''\n",
    "            top_props.append((prop_label, dom_label, rng_label))\n",
    "    \n",
    "    # Detect property characteristics (OWL axioms)\n",
    "    prop_chars = {}\n",
    "    for p in meta_obj.properties[:50]:\n",
    "        if p.startswith('http'):\n",
    "            chars = []\n",
    "            p_uri = URIRef(p)\n",
    "            \n",
    "            # Check for transitive\n",
    "            if list(g.triples((p_uri, RDF.type, OWL.TransitiveProperty))):\n",
    "                chars.append('transitive')\n",
    "            \n",
    "            # Check for symmetric\n",
    "            if list(g.triples((p_uri, RDF.type, OWL.SymmetricProperty))):\n",
    "                chars.append('symmetric')\n",
    "            \n",
    "            # Check for functional\n",
    "            if list(g.triples((p_uri, RDF.type, OWL.FunctionalProperty))):\n",
    "                chars.append('functional')\n",
    "            \n",
    "            # Check for inverse functional\n",
    "            if list(g.triples((p_uri, RDF.type, OWL.InverseFunctionalProperty))):\n",
    "                chars.append('inverse_functional')\n",
    "            \n",
    "            # Check for inverse\n",
    "            if list(g.triples((p_uri, OWL.inverseOf, None))):\n",
    "                chars.append('has_inverse')\n",
    "            \n",
    "            if chars:\n",
    "                prop_chars[p] = chars\n",
    "    \n",
    "    # Detect OWL constructs usage\n",
    "    owl_constructs = {\n",
    "        'restrictions': len(list(g.subjects(RDF.type, OWL.Restriction))),\n",
    "        'unions': len(list(g.subjects(OWL.unionOf, None))),\n",
    "        'intersections': len(list(g.subjects(OWL.intersectionOf, None))),\n",
    "        'disjointness': len(list(g.triples((None, OWL.disjointWith, None)))),\n",
    "        'equivalence': len(list(g.triples((None, OWL.equivalentClass, None))))\n",
    "    }\n",
    "    \n",
    "    # Get URI pattern samples\n",
    "    uri_sample = [c for c in meta_obj.classes[:5] if c.startswith('http')]\n",
    "    uri_pattern = uri_sample[0].rsplit('/', 1)[0] if uri_sample else ''\n",
    "    \n",
    "    # Build prompt for LLM synthesis\n",
    "    prompt = f\"\"\"Analyze this ontology and provide a sense document:\n",
    "\n",
    "**Ontology Metadata:**\n",
    "{ont_metadata}\n",
    "\n",
    "**Stats:** {len(meta_obj.classes)} classes, {len(meta_obj.properties)} properties, {len(meta_obj.labels)} labels\n",
    "\n",
    "**Annotation Properties Detected:**\n",
    "- Label properties: {label_props}\n",
    "- Description properties: {desc_props}\n",
    "\n",
    "**Structure:**\n",
    "- Prefixes: {list(metadata.prefixes.keys())[:10]}\n",
    "- Root classes: {[meta_obj.labels.get(r, r) for r in roots[:10]]}\n",
    "- Hierarchy (2 levels): {hier}\n",
    "\n",
    "**Properties:**\n",
    "- Top properties (label, domain, range): {top_props[:10]}\n",
    "- Property characteristics: {prop_chars}\n",
    "\n",
    "**OWL Constructs Usage:**\n",
    "{owl_constructs}\n",
    "\n",
    "**URI Pattern:** {uri_pattern}\n",
    "- Sample URIs: {uri_sample[:3]}\n",
    "\n",
    "Provide a concise sense document with:\n",
    "1) Domain/scope - what is this ontology about?\n",
    "2) Key branches - main conceptual areas in the hierarchy\n",
    "3) Important properties - key relationships to know\n",
    "4) Detected patterns - reification, measurement patterns, part-whole relationships, restriction patterns, etc.\n",
    "5) SPARQL navigation hints - how to effectively query this ontology given the annotation properties available\"\"\"\n",
    "    \n",
    "    # Use llm_query from rlm.core\n",
    "    from rlm.core import llm_query\n",
    "    summary = llm_query(prompt, ns=ns, name='_sense_summary')\n",
    "    \n",
    "    # Build structured sense document\n",
    "    sense_doc = AttrDict(\n",
    "        ont=ont_name,\n",
    "        ont_metadata=ont_metadata,\n",
    "        stats={'cls': len(meta_obj.classes), 'props': len(meta_obj.properties), 'lbls': len(meta_obj.labels)},\n",
    "        prefixes=metadata.prefixes,\n",
    "        label_properties=label_props,  # NEW: detected label properties\n",
    "        description_properties=desc_props,  # NEW: detected description properties\n",
    "        ann_preds=metadata.ann_preds,\n",
    "        roots=roots,\n",
    "        hier=hier,\n",
    "        top_props=top_props,\n",
    "        prop_chars=prop_chars,\n",
    "        owl_constructs=owl_constructs,  # NEW: OWL construct usage\n",
    "        uri_pattern=uri_pattern,\n",
    "        summary=ns['_sense_summary']\n",
    "    )\n",
    "    \n",
    "    ns[name] = sense_doc\n",
    "    return f\"Built sense document into '{name}': {len(label_props)} label properties, {len(desc_props)} description properties, {len(hier)} root branches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f986e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Test build_sense with PROV ontology\n",
    "# Note: Requires API key, marked eval:false to avoid CI failures\n",
    "\n",
    "test_ns = {}\n",
    "result = build_sense('ontology/prov.ttl', name='prov_sense', ns=test_ns)\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "# Inspect the sense document\n",
    "sense = test_ns['prov_sense']\n",
    "print(f\"Ontology: {sense.ont}\")\n",
    "print(f\"Ontology Metadata: {sense.ont_metadata}\")\n",
    "print(f\"Stats: {sense.stats}\")\n",
    "print()\n",
    "\n",
    "# NEW: Show detected annotation properties\n",
    "print(f\"Label properties detected: {sense.label_properties}\")\n",
    "print(f\"Description properties detected: {sense.description_properties}\")\n",
    "print()\n",
    "\n",
    "print(f\"Roots: {sense.roots}\")\n",
    "print(f\"Root branches: {list(sense.hier.keys())}\")\n",
    "print(f\"Top properties (first 3): {sense.top_props[:3]}\")\n",
    "print(f\"Property characteristics: {sense.prop_chars}\")\n",
    "print(f\"OWL constructs: {sense.owl_constructs}\")\n",
    "print(f\"URI pattern: {sense.uri_pattern}\")\n",
    "print()\n",
    "print(\"LLM Summary:\")\n",
    "print(sense.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Structured Sense Data\n\n**NEW**: JSON-schemaed sense data for ReasoningBank integration.\n\nThe original `build_sense()` produces free-form prose in the `summary` field. This new system creates:\n- **sense_card**: Compact, always-injected structured data (~500 chars)\n- **sense_brief**: Detailed sections retrieved when needed (~2000 chars)\n- **Grounding validation**: All URIs must exist in the ontology\n\nSee `docs/ont-sense-improvements.md` for full specification.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#| export\nimport json\n\n# Sense Card schema (always injected, compact)\nSENSE_CARD_SCHEMA = {\n    'ontology_id': str,\n    'domain_scope': str,  # max 300 chars\n    'triple_count': int,\n    'class_count': int,\n    'property_count': int,\n    'key_classes': list,   # max 5 items, each with uri/label/why_important\n    'key_properties': list, # max 5 items, each with uri/label/domain/range/role\n    'label_predicates': list,\n    'description_predicates': list,\n    'available_indexes': dict,  # by_label, hierarchy, domains, ranges, pred_freq counts\n    'quick_hints': list,   # max 3 items\n    'uri_pattern': str\n}\n\ndef validate_sense_grounding(sense: dict, meta: GraphMeta) -> dict:\n    \"\"\"Validate all URIs in sense exist in the ontology.\n\n    Args:\n        sense: Sense document with sense_card (and optional sense_brief)\n        meta: GraphMeta to validate against\n\n    Returns:\n        {'valid': bool, 'errors': list[str], 'error_count': int}\n    \"\"\"\n    errors = []\n\n    # Extract sense_card\n    card = sense.get('sense_card', {})\n\n    # Check key_classes URIs\n    for cls in card.get('key_classes', []):\n        if isinstance(cls, dict) and 'uri' in cls:\n            if cls['uri'] not in meta.classes:\n                errors.append(f\"key_class URI not found: {cls['uri']}\")\n\n    # Check key_properties URIs\n    for prop in card.get('key_properties', []):\n        if isinstance(prop, dict) and 'uri' in prop:\n            if prop['uri'] not in meta.properties:\n                errors.append(f\"key_property URI not found: {prop['uri']}\")\n\n    # Check sense_brief patterns if present\n    brief = sense.get('sense_brief', {})\n    patterns = brief.get('patterns', {}).get('detected_patterns', [])\n    for pattern in patterns:\n        for entity in pattern.get('entities_involved', []):\n            # Entity might be a label or URI\n            if entity.startswith('http://') or entity.startswith('https://'):\n                if entity not in meta.classes and entity not in meta.properties:\n                    errors.append(f\"pattern entity URI not found: {entity}\")\n\n    return {\n        'valid': len(errors) == 0,\n        'errors': errors,\n        'error_count': len(errors)\n    }\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#| export\ndef build_sense_structured(\n    path: str,\n    name: str = 'sense',\n    ns: dict = None\n) -> dict:\n    \"\"\"Build structured sense document with card and brief.\n\n    Returns JSON-schemaed output instead of free-form prose.\n\n    Args:\n        path: Path to ontology file\n        name: Variable name for sense document\n        ns: Namespace dict\n\n    Returns:\n        Dict with 'sense_card', 'sense_brief', and '_validation' keys\n    \"\"\"\n    if ns is None: ns = {}\n\n    # Derive ontology name from sense name\n    ont_name = name.replace('_sense', '').replace('sense', 'ont')\n\n    # Setup ontology context\n    setup_ontology_context(path, ns, name=ont_name)\n\n    # Get metadata and roots\n    ont_meta(f'{ont_name}_meta', name=f'{ont_name}_metadata', ns=ns)\n    ont_roots(f'{ont_name}_meta', name=f'{ont_name}_roots', ns=ns)\n\n    # Get references\n    meta_obj = ns[f'{ont_name}_meta']\n    metadata = ns[f'{ont_name}_metadata']\n    roots = ns[f'{ont_name}_roots']\n    g = meta_obj.graph\n\n    # Detect ontology-level metadata\n    ont_uri = None\n    for s in g.subjects(RDF.type, OWL.Ontology):\n        ont_uri = str(s)\n        break\n\n    ont_metadata = {}\n    domain_scope = \"No description available\"\n    \n    if ont_uri:\n        ont_ref = URIRef(ont_uri)\n        ont_metadata['uri'] = ont_uri\n\n        # Get description for domain_scope\n        for desc_prop in [DCTERMS.description, DC.description, RDFS.comment]:\n            descs = list(g.objects(ont_ref, desc_prop))\n            if descs:\n                domain_scope = str(descs[0])[:300]\n                ont_metadata['description'] = domain_scope\n                break\n\n    # Detect annotation properties\n    label_props = []\n    desc_props = []\n\n    for label_prop in [RDFS.label, SKOS.prefLabel, SKOS.altLabel, DCTERMS.title, DC.title]:\n        if list(g.triples((None, label_prop, None))):\n            label_props.append(str(label_prop))\n\n    for desc_prop in [RDFS.comment, SKOS.definition, SKOS.note, DCTERMS.description, DC.description]:\n        if list(g.triples((None, desc_prop, None))):\n            desc_props.append(str(desc_prop))\n\n    # Get URI pattern\n    uri_sample = [c for c in meta_obj.classes[:5] if c.startswith('http')]\n    uri_pattern = uri_sample[0].rsplit('/', 1)[0] if uri_sample else ''\n\n    # Build sense_card programmatically (100% grounded)\n    sense_card = {\n        'ontology_id': ont_name,\n        'domain_scope': domain_scope,\n        'triple_count': len(g),\n        'class_count': len(meta_obj.classes),\n        'property_count': len(meta_obj.properties),\n        'key_classes': [],\n        'key_properties': [],\n        'label_predicates': label_props,\n        'description_predicates': desc_props,\n        'available_indexes': {\n            'by_label': len(meta_obj.labels),\n            'hierarchy': len(meta_obj.subs) + len(meta_obj.supers),\n            'domains': len(meta_obj.doms),\n            'ranges': len(meta_obj.rngs),\n            'pred_freq': len(meta_obj.pred_freq)\n        },\n        'quick_hints': [],\n        'uri_pattern': uri_pattern\n    }\n\n    # Extract key classes from roots (programmatic, grounded)\n    for r in roots[:5]:\n        sense_card['key_classes'].append({\n            'uri': r,\n            'label': meta_obj.labels.get(r, r.split('/')[-1].split('#')[-1]),\n            'why_important': 'Root class in hierarchy'\n        })\n\n    # Extract key properties from metadata (programmatic, grounded)\n    for p in meta_obj.properties[:5]:\n        if p.startswith('http'):\n            dom = meta_obj.doms.get(p, '')\n            rng = meta_obj.rngs.get(p, '')\n            dom_label = meta_obj.labels.get(dom, dom.split('/')[-1].split('#')[-1]) if dom else None\n            rng_label = meta_obj.labels.get(rng, rng.split('/')[-1].split('#')[-1]) if rng else None\n            \n            sense_card['key_properties'].append({\n                'uri': p,\n                'label': meta_obj.labels.get(p, p.split('/')[-1].split('#')[-1]),\n                'domain': dom_label,\n                'range': rng_label,\n                'role': f\"Connects {dom_label} to {rng_label}\" if dom_label and rng_label else \"Common property\"\n            })\n\n    # Generate quick_hints (simple programmatic rules, no LLM)\n    hints = []\n    if label_props:\n        hints.append(f\"Use {label_props[0].split('/')[-1].split('#')[-1]} for entity labels\")\n    if sense_card['available_indexes']['hierarchy'] > 50:\n        hints.append(f\"Hierarchy index has {sense_card['available_indexes']['hierarchy']} relationships\")\n    if sense_card['available_indexes']['by_label'] > 10:\n        hints.append(f\"Label index has {sense_card['available_indexes']['by_label']} entries for quick lookup\")\n    \n    sense_card['quick_hints'] = hints[:3]\n\n    # Build sense_brief with hierarchy overview\n    sense_brief = {\n        'ontology_id': ont_name,\n        'hierarchy_overview': {\n            'root_classes': [\n                {\n                    'uri': r,\n                    'label': meta_obj.labels.get(r, r.split('/')[-1].split('#')[-1]),\n                    'direct_subclasses': [\n                        meta_obj.labels.get(c, c.split('/')[-1].split('#')[-1]) \n                        for c in meta_obj.subs.get(r, [])[:5]\n                    ]\n                }\n                for r in roots[:5]\n            ],\n            'max_depth': 2,\n            'branching_factor': 'medium'\n        }\n    }\n\n    # Package result\n    result = {\n        'sense_card': sense_card,\n        'sense_brief': sense_brief\n    }\n\n    # Validate grounding\n    validation = validate_sense_grounding(result, meta_obj)\n    result['_validation'] = validation\n\n    # Store in namespace\n    ns[name] = result\n\n    return result\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f43a5430",
   "metadata": {},
   "source": [
    "## Integration with RLM\n",
    "\n",
    "Helper to setup ontology context for `rlm_run()`."
   ]
  },
  {
   "cell_type": "code",
   "source": "#| eval: false\n# Test RLM with structured sense context\n# Note: Requires API key, marked eval:false to avoid CI failures\n\nfrom rlm.core import rlm_run\n\nprint(\"=\" * 70)\nprint(\" RLM INTEGRATION TEST: Structured Sense as Context\")\nprint(\"=\" * 70)\n\n# Setup: Build structured sense for PROV ontology\nns = {}\nsense_result = build_sense_structured('ontology/prov.ttl', name='prov_sense', ns=ns)\n\n# Get formatted sense card as context\nsense_context = format_sense_card(sense_result['sense_card'])\n\nprint(f\"\\n📋 Context Type: Structured Sense Card\")\nprint(f\"   Size: {len(sense_context)} chars\")\nprint(f\"   Grounding: {'PASS' if sense_result['_validation']['valid'] else 'FAIL'}\")\n\n# Test query\nquery = \"What is the Activity class in PROV?\"\n\nprint(f\"\\n❓ Query: {query}\")\nprint(\"\\n\" + \"-\" * 70)\nprint(\"Running RLM with sense card context...\")\nprint(\"-\" * 70)\n\n# Run RLM with sense context\nanswer, iterations, final_ns = rlm_run(\n    query,\n    sense_context,\n    ns=ns,\n    max_iters=5\n)\n\nprint(f\"\\n✓ Answer: {answer}\")\nprint(f\"\\n📊 Iterations: {len(iterations)}\")\nprint(f\"   Max allowed: 5\")\n\n# Show iteration details\nprint(f\"\\n🔍 Iteration Breakdown:\")\nfor i, iteration in enumerate(iterations, 1):\n    print(f\"   {i}. {iteration.get('action', 'unknown action')}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\" TEST RESULT\")\nprint(\"=\" * 70)\n\nif len(iterations) <= 5:\n    print(f\"\\n✓ PASS: RLM converged in {len(iterations)} iterations\")\n    print(f\"  The structured sense card provides sufficient context for RLM\")\nelse:\n    print(f\"\\n✗ FAIL: RLM did not converge within iteration limit\")\n\nprint(\"\\n💡 Benefits of Structured Sense:\")\nprint(\"  • Compact context (~600 chars vs full ontology)\")\nprint(\"  • 100% grounded (no hallucinated URIs)\")\nprint(\"  • Ontology-aware (detects label/description predicates)\")\nprint(\"  • Progressive disclosure ready (can add hierarchy brief)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Test RLM Integration with Structured Sense\n\nTest if `rlm_run()` works with the new structured sense documents as context.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#| eval: false\n# Test formatting functions (depends on previous cell state)\n# Requires: result, card, brief variables from build_sense_structured() call\nprint(\"=\" * 60)\nprint(\"FORMATTING FUNCTIONS TEST\")\nprint(\"=\" * 60)\n\n# Get card and brief from result (from previous test cell)\ncard = result['sense_card']\nbrief = result['sense_brief']\n\n# Test format_sense_card\nformatted_card = format_sense_card(card)\nprint(f\"\\n✓ Formatted Sense Card ({len(formatted_card)} chars):\")\nprint(\"-\" * 60)\nprint(formatted_card)\nprint(\"-\" * 60)\n\n# Test format_sense_brief_section\nformatted_hier = format_sense_brief_section(brief, 'hierarchy_overview')\nprint(f\"\\n✓ Formatted Hierarchy Overview ({len(formatted_hier)} chars):\")\nprint(\"-\" * 60)\nprint(formatted_hier)\nprint(\"-\" * 60)\n\n# Test get_sense_context\nquery = \"What are the subclasses of Activity?\"\ncontext = get_sense_context(query, result)\nprint(f\"\\n✓ Auto-detected Context for: '{query}'\")\nprint(f\"  Context length: {len(context)} chars\")\nprint(f\"  Includes hierarchy: {('Hierarchy Overview' in context)}\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(\"FORMATTING TESTS PASSED\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#| eval: false\n# Test build_sense_structured with PROV ontology (depends on previous state)\ntest_ns = {}\nresult = build_sense_structured('ontology/prov.ttl', name='prov_sense_structured', ns=test_ns)\n\nprint(\"=\" * 60)\nprint(\"STRUCTURED SENSE TEST\")\nprint(\"=\" * 60)\n\n# Check validation\nprint(f\"\\n✓ Validation: {'PASS' if result['_validation']['valid'] else 'FAIL'}\")\nif not result['_validation']['valid']:\n    print(f\"  Errors: {result['_validation']['errors']}\")\nelse:\n    print(\"  All URIs grounded in ontology\")\n\n# Check sense_card\ncard = result['sense_card']\nprint(f\"\\n✓ Sense Card:\")\nprint(f\"  Ontology ID: {card['ontology_id']}\")\nprint(f\"  Triple count: {card['triple_count']:,}\")\nprint(f\"  Class count: {card['class_count']}\")\nprint(f\"  Property count: {card['property_count']}\")\nprint(f\"  Label predicates: {len(card['label_predicates'])}\")\nprint(f\"  Key classes: {len(card['key_classes'])}\")\nprint(f\"  Key properties: {len(card['key_properties'])}\")\nprint(f\"  Quick hints: {len(card['quick_hints'])}\")\n\n# Verify key_classes are grounded\nprint(f\"\\n✓ Key Classes (grounded URIs):\")\nfor cls in card['key_classes'][:3]:\n    print(f\"  - {cls['label']}\")\n    print(f\"    URI: {cls['uri'][:50]}...\")\n\n# Verify key_properties are grounded\nprint(f\"\\n✓ Key Properties (grounded URIs):\")\nfor prop in card['key_properties'][:3]:\n    print(f\"  - {prop['label']}: {prop['role']}\")\n    print(f\"    URI: {prop['uri'][:50]}...\")\n\n# Check sense_brief\nbrief = result['sense_brief']\nprint(f\"\\n✓ Sense Brief:\")\nprint(f\"  Hierarchy roots: {len(brief['hierarchy_overview']['root_classes'])}\")\nprint(f\"  Max depth: {brief['hierarchy_overview']['max_depth']}\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(\"ALL TESTS PASSED\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#| export\ndef format_sense_card(card: dict) -> str:\n    \"\"\"Format sense card for context injection (~500 chars).\n\n    Args:\n        card: sense_card dict\n\n    Returns:\n        Formatted markdown string\n    \"\"\"\n    lines = [\n        f\"# Ontology: {card['ontology_id']}\",\n        \"\",\n        f\"**Domain**: {card['domain_scope'][:200]}\",\n        \"\",\n        f\"**Stats**: {card['triple_count']:,} triples, {card['class_count']} classes, {card['property_count']} properties\",\n        \"\",\n        \"**Key Classes**:\"\n    ]\n\n    for cls in card['key_classes'][:3]:\n        lines.append(f\"- {cls['label']}: {cls['why_important']}\")\n\n    lines.append(\"\")\n    lines.append(\"**Key Properties**:\")\n    for prop in card['key_properties'][:3]:\n        if prop['domain'] and prop['range']:\n            lines.append(f\"- {prop['label']}: {prop['domain']} → {prop['range']}\")\n        else:\n            lines.append(f\"- {prop['label']}: {prop['role']}\")\n\n    lines.append(\"\")\n    lines.append(f\"**Labels via**: {', '.join([p.split('/')[-1].split('#')[-1] for p in card['label_predicates']])}\")\n\n    if card['quick_hints']:\n        lines.append(\"\")\n        lines.append(\"**Quick Hints**:\")\n        for hint in card['quick_hints']:\n            lines.append(f\"- {hint}\")\n\n    return '\\n'.join(lines)\n\n\ndef format_sense_brief_section(brief: dict, section: str) -> str:\n    \"\"\"Format a specific brief section.\n\n    Args:\n        brief: sense_brief dict\n        section: Section name (e.g., 'hierarchy_overview', 'patterns')\n\n    Returns:\n        Formatted markdown string\n    \"\"\"\n    if section not in brief:\n        return \"\"\n\n    if section == 'hierarchy_overview':\n        lines = [\"## Hierarchy Overview\", \"\"]\n        overview = brief['hierarchy_overview']\n        lines.append(f\"**Root Classes** (max depth: {overview.get('max_depth', 'unknown')})\")\n        for root in overview.get('root_classes', [])[:5]:\n            lines.append(f\"- **{root['label']}**\")\n            for sub in root.get('direct_subclasses', [])[:3]:\n                lines.append(f\"  - {sub}\")\n        return '\\n'.join(lines)\n\n    return \"\"\n\n\ndef get_sense_context(query: str, sense: dict) -> str:\n    \"\"\"Auto-detect and return relevant sense sections for a query.\n\n    Args:\n        query: User query\n        sense: Full sense document (with sense_card and sense_brief)\n\n    Returns:\n        Formatted context string\n    \"\"\"\n    # Always include card\n    context = format_sense_card(sense['sense_card'])\n\n    # Auto-detect relevant brief sections\n    query_lower = query.lower()\n    brief = sense.get('sense_brief', {})\n\n    if any(word in query_lower for word in ['subclass', 'superclass', 'hierarchy', 'type', 'parent', 'child']):\n        context += \"\\n\\n\" + format_sense_brief_section(brief, 'hierarchy_overview')\n\n    return context\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: setup_ontology_context() is defined above in cell-27\n",
    "# This cell previously contained a duplicate definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a87c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1664 triples from prov.ttl into 'prov'\n",
      "Created meta-graph 'prov_meta' with 59 classes, 89 properties\n",
      "\n",
      "Namespace contains:\n",
      "  prov: Graph\n",
      "  prov_meta: GraphMeta\n",
      "  prov_graph_stats: partial\n",
      "  prov_search_by_label: partial\n",
      "  prov_describe_entity: partial\n",
      "  prov_search_entity: partial\n",
      "  prov_probe_relationships: partial\n",
      "  prov_find_path: partial\n",
      "  prov_predicate_frequency: partial\n",
      "  graph_stats: partial\n",
      "  search_by_label: partial\n",
      "  describe_entity: partial\n",
      "  search_entity: partial\n",
      "  probe_relationships: partial\n",
      "  find_path: partial\n",
      "  predicate_frequency: partial\n"
     ]
    }
   ],
   "source": [
    "# Test setup for RLM\n",
    "test_ns = {}\n",
    "result = setup_ontology_context('ontology/prov.ttl', test_ns, name='prov')\n",
    "print(result)\n",
    "print()\n",
    "print(\"Namespace contains:\")\n",
    "for k in test_ns.keys():\n",
    "    print(f\"  {k}: {type(test_ns[k]).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New GraphMeta indexes work: by_label has 156 entries\n",
      "✓ ont_describe works: Stored 10 + 34 triples about 'http://www.w3.org/ns/prov#Activity' into 'activity_desc'\n",
      "✓ ont_meta works: Stored metadata into 'prov_metadata': 29 prefixes, 16 annotation predicates, 9 imports\n",
      "✓ ont_roots works: Stored 10 root classes into 'prov_roots'\n"
     ]
    }
   ],
   "source": [
    "# Test new exploration functions\n",
    "# Reuse the test_ns from previous cell with loaded prov ontology\n",
    "# Note: prov_meta is a GraphMeta object in test_ns\n",
    "\n",
    "# Test that new indexes work\n",
    "meta = test_ns['prov_meta']\n",
    "assert len(meta.by_label) > 0  # inverted label index\n",
    "assert len(meta.subs) > 0 or len(meta.supers) > 0  # class hierarchy\n",
    "print(f\"✓ New GraphMeta indexes work: by_label has {len(meta.by_label)} entries\")\n",
    "\n",
    "# Test ont_describe (need to pass GraphMeta object as namespace entry)\n",
    "result = ont_describe('prov_meta', 'http://www.w3.org/ns/prov#Activity', name='activity_desc', ns=test_ns)\n",
    "assert 'activity_desc' in test_ns\n",
    "print(f\"✓ ont_describe works: {result}\")\n",
    "\n",
    "# Test ont_meta  \n",
    "result = ont_meta('prov_meta', name='prov_metadata', ns=test_ns)\n",
    "assert 'prov_metadata' in test_ns\n",
    "print(f\"✓ ont_meta works: {result}\")\n",
    "\n",
    "# Test ont_roots\n",
    "result = ont_roots('prov_meta', name='prov_roots', ns=test_ns)\n",
    "assert 'prov_roots' in test_ns\n",
    "print(f\"✓ ont_roots works: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1bf04",
   "metadata": {},
   "source": [
    "## Test with RLM\n",
    "\n",
    "Now let's test asking a question about the PROV ontology using `rlm_run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from rlm.core import rlm_run\n",
    "\n",
    "# Setup namespace with PROV ontology\n",
    "ns = {}\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "\n",
    "# Ask a question\n",
    "# The context is the GraphMeta summary, not the full graph\n",
    "context = ns['prov_meta'].summary()\n",
    "\n",
    "answer, iterations, ns = rlm_run(\n",
    "    \"What is the Activity class in the PROV ontology?\",\n",
    "    context,\n",
    "    ns=ns,\n",
    "    max_iters=3\n",
    ")\n",
    "\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Iterations: {len(iterations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense Validation Gate\n",
    "\n",
    "Validate sense data before RLM operations (precondition check)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#| export\n",
    "def validate_sense_precondition(sense: dict, meta) -> dict:\n",
    "    \"\"\"Gate 0: Validate sense data before RLM operations.\n",
    "    \n",
    "    Checks:\n",
    "    - URI grounding (all URIs exist in ontology)\n",
    "    - Card size (under 800 chars)\n",
    "    - Required fields present\n",
    "    \n",
    "    Args:\n",
    "        sense: Sense document from build_sense_structured()\n",
    "        meta: GraphMeta object for grounding validation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with proceed flag and validation details\n",
    "    \"\"\"\n",
    "    # Re-run grounding validation\n",
    "    grounding = validate_sense_grounding(sense, meta)\n",
    "    \n",
    "    # Check card size\n",
    "    card_text = format_sense_card(sense['sense_card'])\n",
    "    card_size_ok = len(card_text) <= 800  # Allow some buffer over 600\n",
    "    \n",
    "    # Check required fields\n",
    "    card = sense.get('sense_card', {})\n",
    "    required_fields = [\n",
    "        'ontology_id', 'domain_scope', 'key_classes',\n",
    "        'key_properties', 'label_predicates'\n",
    "    ]\n",
    "    has_required = all(field in card for field in required_fields)\n",
    "    \n",
    "    proceed = grounding['valid'] and card_size_ok and has_required\n",
    "    \n",
    "    reason = ''\n",
    "    if not proceed:\n",
    "        issues = []\n",
    "        if not grounding['valid']:\n",
    "            issues.append(f\"grounding failed ({grounding['error_count']} errors)\")\n",
    "        if not card_size_ok:\n",
    "            issues.append(f\"card too large ({len(card_text)} chars)\")\n",
    "        if not has_required:\n",
    "            issues.append(\"missing required fields\")\n",
    "        issues_text = ', '.join(issues)\n",
    "        reason = f\"Validation failed: {issues_text}\"\n",
    "    \n",
    "    return {\n",
    "        'proceed': proceed,\n",
    "        'grounding_valid': grounding['valid'],\n",
    "        'card_size': len(card_text),\n",
    "        'card_size_ok': card_size_ok,\n",
    "        'has_required_fields': has_required,\n",
    "        'reason': reason\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#| eval: false\n",
    "# Test sense validation gate (requires real ontology)\n",
    "from rlm.ontology import setup_ontology_context, build_sense_structured\n",
    "\n",
    "print(\"Test: validate_sense_precondition()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ns = {}\n",
    "setup_ontology_context('ontology/prov.ttl', ns, name='prov')\n",
    "sense = build_sense_structured('ontology/prov.ttl', name='prov_sense', ns=ns)\n",
    "\n",
    "result = validate_sense_precondition(sense, ns['prov_meta'])\n",
    "\n",
    "print(f\"Proceed: {result['proceed']}\")\n",
    "print(f\"Grounding valid: {result['grounding_valid']}\")\n",
    "print(f\"Card size: {result['card_size']} chars (ok: {result['card_size_ok']})\")\n",
    "print(f\"Has required fields: {result['has_required_fields']}\")\n",
    "\n",
    "if result['proceed']:\n",
    "    print(\"\\n✓ Sense validation gate passed\")\n",
    "else:\n",
    "    print(f\"\\n✗ Validation failed: {result['reason']}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}