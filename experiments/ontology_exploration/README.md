# Ontology Exploration Experiments

**Purpose**: Incrementally test whether LLM-driven ontology exploration can produce useful chain-of-thought artifacts for query construction.

**Approach**: Start simple, test one hypothesis at a time, build on what works.

---

## Results So Far (2026-01-29)

### Completed Experiments

1. **E1 (Basic Exploration)** âœ… - PROV ontology (1,664 triples)
   - **Result**: Model CAN explore ontologies using rdflib
   - **Cost**: $0.0343, 77.7s, 7 LM calls
   - **Key finding**: No specialized tools needed - rdflib + llm_query sufficient
   - See: `analysis/e1_results.md`

2. **E2 (llm_query Synthesis)** âœ… - PROV ontology with guidance
   - **Result**: Guidance improved quality but NO llm_query usage
   - **Cost**: $0.0241 (-30% vs E1), 86.3s, 7 LM calls
   - **Paradox**: Better output (more concise, semantic focus) WITHOUT delegation
   - See: `analysis/e2_results.md`

3. **E2-Large (Scale Test)** âœ… - UniProt ontology (2,816 triples, +69%)
   - **Result**: REFUTED scale hypothesis - larger ontology was MORE efficient
   - **Cost**: $0.0221 (-35% vs E1), 74.4s, 8 LM calls
   - **Key finding**: Scale alone doesn't trigger delegation
   - See: `analysis/e2_large_results.md`

4. **E3 (Structured Materialization)** âœ… - PROV ontology with JSON schema
   - **Result**: ðŸŽ‰ **BREAKTHROUGH** - First delegation attempt!
   - **Cost**: $0.3483 (10x higher), 78.8s, 8 LM calls
   - **Key finding**: Structured format with semantic requirements DOES trigger llm_query_batched()
   - **Issue**: Hit max_llm_calls limit before completing batch (needs higher limits)
   - **Output**: Valid JSON guide with all 59 classes, 69 properties, 5 SPARQL patterns
   - See: `analysis/e3_results.md`

5. **E3-Retry (Higher Limits)** âœ… - PROV with max_llm_calls=30
   - **Result**: âš ï¸ **UNEXPECTED** - Rich semantic content WITHOUT delegation
   - **Cost**: $0.2491 (-28% vs E3!), 217.7s, 13 LM calls
   - **Key finding**: Extended inline synthesis > delegation for small ontologies
   - **Output**: Python dict (not JSON) with 6 semantic categories, 6 use cases, 6 query patterns
   - **Semantic depth**: Use cases with "why_it_matters", importance rationales, example scenarios
   - See: `analysis/e3_retry_results.md`

6. **E4 (Guide-Based Queries)** âœ… - Query construction with/without guide
   - **Result**: âœ… **VALIDATED** - Guide reduces cost 30%, time 40%
   - **Condition A** (no guide): $0.1304, 59.4s, 6 LM calls
   - **Condition B** (with guide): $0.0911, 35.5s, 5 LM calls
   - **Savings**: $0.0393 per query, break-even at 6.3 queries
   - **Quality**: Guide produced better SPARQL (URI-based vs label-based)
   - **Key finding**: Two-phase workflow is cost-effective for 7+ queries
   - See: `analysis/e4_results.md`

7. **E4 Extension (Multiple Query Types)** âœ… - 3 queries of increasing complexity
   - **Result**: âœ… **STRONGLY VALIDATED** - Guide consistently reduces cost 44% across all query types
   - **Queries tested**: Simple (direct lookup), Relationship (multi-hop), Semantic (conceptual)
   - **Average per query**: $0.1545 â†’ $0.0860 (-44.3% cost), 58.6s â†’ 37.6s (-35.9% time)
   - **Surprising finding**: Relationship queries benefit MOST (-48.7%), not semantic queries
   - **Break-even**: 3.6 queries (improved from 6.3 in single-query E4)
   - **Key insight**: Guides most valuable for schema navigation (multi-hop property paths)
   - See: `analysis/e4_extension_results.md`

8. **E5 (UniProt + Real Endpoint + Bounded Tools)** âœ… - Production SPARQL endpoint with tool-based execution
   - **Result**: ðŸŽ‰ **PRODUCTION VALIDATED** - Guide reduces cost 53% with 100% success rate on real endpoint
   - **Environment**: UniProt endpoint (billions of triples), bounded SPARQL tools with auto-LIMIT
   - **Average per query**: $0.1609 â†’ $0.0751 (-53.3% cost), 59.6s â†’ 38.9s (-34.8% time)
   - **Key findings**:
     - **76% cost reduction** for schema navigation queries (property discovery)
     - **31% cost reduction** for instance data queries
     - **100% success rate** vs 100% failure without tools (timeouts, syntax errors)
     - **Guide caching works**: $0 cost for cached guide reuse
   - **Critical insight**: Bounded tools + guides enable safe, iterative refinement against production endpoints
   - See: `analysis/e5_results.md`

### Key Insights

1. **Format Shapes Behavior**: Structured output with "why_important" fields triggered delegation (E3), while explicit guidance didn't (E2)
2. **Delegation May Not Be Necessary**: E3-Retry achieved rich semantics WITHOUT llm_query via extended inline synthesis
3. **Iteration Budget Enables Quality**: 13 LM calls (vs 6) produced semantic categories, use cases, and comprehensive patterns
4. **Small Ontologies Don't Need Delegation**: For <3K triples, inline synthesis is preferred and more efficient
5. **Semantic Categorization Reduces Tokens**: Grouping classes by meaning (-28% tokens) while increasing semantic value
6. **Use Cases Bridge Structure and Semantics**: Connect "what exists" to "how to use it" with business value
7. **Two-Phase Workflow Validated**: Guides reduce query construction cost by 44-53% on average across all complexity levels
8. **Schema Navigation is Killer Use Case**: Relationship queries (multi-hop property paths) benefit MOST from guides (-48% to -76%), more than semantic queries
9. **Break-Even Faster Than Expected**: 3.6 queries to amortize guide creation cost, making guides practical for any ontology queried 4+ times
10. **Guide Compression Works**: 935-char summaries (2.3% of full guide) provide full benefit with no quality loss
11. **Bounded Tools Are Essential** (E5): Tool-based execution provides 100% success rate vs 100% failure without tools; auto-LIMIT prevents timeouts
12. **Production Endpoints Validated** (E5): Two-phase workflow works on real endpoints with billions of triples; guides + tools enable safe iterative refinement
13. **Guide Caching is Critical** (E5): $0 cost for cached guide reuse eliminates 80% of total workflow cost
14. **Manual Minimal Guides Sufficient** (E5): Hand-crafted guides with 15 classes provide same benefits as expensive generated comprehensive guides

### Evolution of Understanding

**E1/E2 (Prose Output)**:
- No delegation, generic synthesis
- Model described structure without deep semantics

**E3 (Structured + Low Limits)**:
- Triggered delegation attempt (llm_query_batched)
- Hit limit, fell back to generic labels
- Valid JSON but shallow semantics

**E3-Retry (Structured + High Limits)**:
- No delegation needed!
- Extended inline synthesis produced rich semantics
- Semantic categories, use cases, query patterns
- Python dict format (minor issue)

**Key Discovery**: For small ontologies (<3K triples), extended inline synthesis is more efficient than delegation. Higher iteration budgets enable quality without sub-LLM calls.

### When Would Delegation Be Necessary?

Based on E1-E3-Retry evidence, llm_query likely only needed for:

1. **Multi-ontology scenarios** - "Compare PROV and Dublin Core"
2. **Very large ontologies** - >10K triples exceeding context window
3. **Explicit sub-question tasks** - "Answer these 5 analysis questions"
4. **Cross-ontology reasoning** - Semantic alignment between schemas

For single-ontology exploration up to ~3K triples: **inline synthesis wins**.

### Next Steps

**Multi-Ontology Validation** (Recommended)
- Test E5 workflow on 3+ different ontologies (PROV, Dublin Core, FOAF, Schema.org)
- Validate that 40-50% cost reductions generalize across domains
- Compare manual minimal guides vs auto-generated comprehensive guides
- Measure guide creation cost vs query savings across different ontology sizes

**Complex Query Testing**
- Test with more challenging query patterns:
  - Multi-hop property paths (3+ hops)
  - OPTIONAL clauses and complex FILTER expressions
  - Aggregation queries (COUNT, GROUP BY, HAVING)
  - UNION and nested queries
- Hypothesis: Guide benefit increases with query complexity
- Measure if bounded tools prevent failures on complex queries

**Production Integration**
- Package bounded SPARQL tools as reusable library
- Create guide generation + caching workflow
- Build query construction module with guide injection
- Benchmark against baseline LLM query construction without guides

---

## Overall Conclusions

### Primary Research Question

**"How do ontologies improve an LLM's ability to query SPARQL endpoints?"**

### Answer (Validated by E1-E5)

Ontology guides reduce query construction cost by **44-53% on average**, with the greatest benefit (76%) for **schema navigation tasks** (discovering properties and relationships). The two-phase workflow is production-ready when combined with bounded SPARQL tools:

1. **Phase 1** (one-time): Create and cache ontology guide ($0 after caching)
2. **Phase 2** (repeated): Use guide + bounded tools for query construction (-53% cost per query)

### Critical Success Factors

1. **Bounded SPARQL Tools**: Auto-LIMIT injection, timeout handling, and iterative feedback enable 100% success rate vs 100% failure without tools
2. **Guide Caching**: Eliminates 80% of total workflow cost; makes workflow practical for production
3. **Minimal Guides Sufficient**: Hand-crafted guides with 15 classes provide same benefits as expensive comprehensive guides
4. **Schema Knowledge Most Valuable**: Guides compress property discovery from exploration to lookup (76% cost reduction)

### Break-Even Economics

- **Guide creation cost**: $0-0.67 (one-time)
- **Savings per query**: $0.04-0.09
- **Break-even point**: 3-7 queries
- **Production value**: For ontologies queried 10+ times, guides reduce total cost by 40-50%

### Production Recommendations

1. âœ… Use guides for any ontology queried 4+ times
2. âœ… Always use bounded SPARQL tools for endpoint queries
3. âœ… Start with manual minimal guides (sufficient for major cost reductions)
4. âœ… Cache guides aggressively (zero-cost reuse across thousands of queries)
5. âœ… Prioritize guides for schema-heavy workloads (relationship discovery, property navigation)

---

## Experiment Series

### E1: Can We Trigger Exploration?

**Hypothesis**: Given a loaded rdflib Graph in namespace, the model will write exploration code.

**Setup**:
- Use PROV ontology (small, 631 triples, well-understood)
- Load graph into namespace before RLM runs
- Simple prompt: "Explore this ontology and describe what you find"

**Measure**:
- Does model write rdflib code? (triples(), subjects(), etc.)
- What does it discover?
- How many iterations does it take?

**Success criteria**: Model writes at least 3 exploration code blocks using rdflib.

---

### E2: Does llm_query Get Used for Synthesis?

**Hypothesis**: With guidance, model will use llm_query to synthesize understanding.

**Setup**:
- Same as E1, but add guidance: "Use llm_query() to help you understand what the classes and properties mean"
- Track llm_query calls in trajectory

**Measure**:
- Number of llm_query calls
- What prompts does model send to llm_query?
- Does synthesis improve understanding vs E1?

**Success criteria**: At least 1 llm_query call for semantic synthesis.

---

### E3: Can We Materialize Understanding?

**Hypothesis**: Model can produce a structured guide from exploration.

**Setup**:
- Same as E2, but ask for structured output:
  - domain_summary (what is this ontology about?)
  - key_classes (list with URIs and descriptions)
  - key_properties (list with domains/ranges)
  - query_hints (how to query this ontology)

**Measure**:
- Does output match expected structure?
- Are URIs valid (exist in ontology)?
- Is the guide coherent and useful?

**Success criteria**: Structured guide with >80% valid URIs.

---

### E4: Does Guide Improve Query Construction?

**Hypothesis**: Query agent performs better with materialized guide than without.

**Setup**:
- Condition A: Query agent with NO guide (baseline)
- Condition B: Query agent WITH guide from E3

**Test queries** (on PROV):
1. "What activities can generate entities?" (simple)
2. "How are agents related to activities?" (relationship)
3. "What is the difference between Generation and Derivation?" (semantic)

**Measure**:
- Does CoT reference the guide? (Condition B)
- Query correctness (manual evaluation)
- Iterations to convergence
- Cost

**Success criteria**: Condition B shows guide references in CoT AND comparable/better query quality.

---

### E5: Scale to Larger Ontology

**Hypothesis**: Approach works on UniProt schema (larger, more complex).

**Setup**:
- Load UniProt core.ttl (~4K triples)
- Run E1-E4 sequence

**Measure**:
- Does exploration complete in reasonable iterations?
- Is guide comprehensive enough?
- Do queries work?

**Success criteria**: Complete workflow produces usable guide.

---

## Experiment Infrastructure

### What We Need

1. **Simple test harness** - Load ontology, run RLM, capture trajectory
2. **Trajectory analysis** - Count code blocks, llm_query calls, measure CoT
3. **Manual evaluation rubric** - For guide quality and query correctness

### What We Already Have

- DSPy RLM with namespace interpreter
- Trajectory logging (JSONL)
- PROV and UniProt ontologies

### Minimal New Code

```python
# experiments/ontology_exploration/run_experiment.py

def run_exploration_experiment(
    ontology_path: str,
    prompt: str,
    experiment_id: str,
    max_iterations: int = 10
) -> dict:
    """Run single exploration experiment and capture results."""

    # Load ontology into namespace BEFORE RLM runs
    ns = {}
    ns['ont'] = Graph().parse(ontology_path)
    ns['RDF'] = RDF
    ns['RDFS'] = RDFS
    ns['OWL'] = OWL

    # Run RLM with exploration prompt
    # ... (use existing DSPy RLM infrastructure)

    # Analyze trajectory
    results = {
        'experiment_id': experiment_id,
        'rdflib_code_blocks': count_rdflib_usage(trajectory),
        'llm_query_calls': count_llm_query_calls(trajectory),
        'iterations': len(trajectory),
        'final_output': ...,
    }

    return results
```

---

## Running the Experiments

### E1: Basic Exploration

```bash
# Run E1
python experiments/ontology_exploration/e1_basic_exploration.py

# Expected output:
# - Trajectory showing rdflib code execution
# - Model discovers classes, properties
# - ~5-8 iterations
```

### E2: With llm_query Guidance

```bash
# Run E2 (adds llm_query guidance)
python experiments/ontology_exploration/e2_llm_query_synthesis.py

# Expected output:
# - At least 1 llm_query call
# - Synthesis of what was discovered
```

### E3: Materialization

```bash
# Run E3 (asks for structured output)
python experiments/ontology_exploration/e3_materialize_guide.py

# Expected output:
# - Structured guide JSON
# - Validation results (URI checking)
```

### E4: Guide-Based Query

```bash
# Run E4 (compare with/without guide)
python experiments/ontology_exploration/e4_guide_comparison.py

# Expected output:
# - Side-by-side comparison
# - CoT analysis showing guide references
```

---

## Analysis Questions

After each experiment, we ask:

1. **What worked?** - What behaviors emerged as expected?
2. **What didn't work?** - Where did the model fail or surprise us?
3. **What guidance helped?** - Which prompts/instructions made a difference?
4. **What's the minimal effective prompt?** - Can we simplify?
5. **What's blocking progress?** - What needs to change for next experiment?

---

## Decision Points

After E1-E2:
- If model won't explore â†’ Need different prompt strategy
- If model explores but won't use llm_query â†’ Need stronger guidance or different approach

After E3:
- If guide is garbage â†’ Exploration isn't producing useful understanding
- If guide is good but not structured â†’ Need better output formatting

After E4:
- If guide doesn't help queries â†’ Guide format is wrong or query agent needs different prompt
- If guide helps â†’ Proceed to E5 scaling

---

## Timeline

- **E1**: Quick test, ~30 min
- **E2**: Add guidance, ~30 min
- **E3**: Structured output, ~1 hour (includes validation)
- **E4**: Comparison study, ~2 hours (multiple conditions)
- **E5**: Scale test, ~2 hours

**Total**: ~6 hours of focused experimentation before committing to implementation.

---

## Files

```
experiments/ontology_exploration/
â”œâ”€â”€ README.md                          # This file - Experiment overview and results
â”œâ”€â”€ e1_basic_exploration.py            # E1: Basic rdflib exploration
â”œâ”€â”€ e2_llm_query_synthesis.py          # E2: Testing llm_query delegation
â”œâ”€â”€ e2_large_ontology.py               # E2-Large: Scale test on UniProt
â”œâ”€â”€ e3_structured_materialization.py   # E3: JSON guide generation
â”œâ”€â”€ e3_retry_higher_limits.py          # E3-Retry: Extended inline synthesis
â”œâ”€â”€ e4_fixed.py                        # E4: Single query comparison
â”œâ”€â”€ e4_extension.py                    # E4 Extension: 3 query types
â”œâ”€â”€ e5_uniprot_endpoint.py             # E5: Production endpoint + tools
â”œâ”€â”€ e5_tool_test.py                    # E5 tool testing script
â”œâ”€â”€ e5_debug.py                        # E5 step-by-step debugging
â”œâ”€â”€ e3_retry_guide.json                # Cached PROV guide from E3-Retry
â”œâ”€â”€ e5_uniprot_guide_cache.json        # Cached UniProt guide for E5
â”œâ”€â”€ e4_results.json                    # E4 results data
â”œâ”€â”€ e4_extension_results.json          # E4 Extension results data
â”œâ”€â”€ e5_uniprot_results.json            # E5 results data
â”œâ”€â”€ analysis/                          # Detailed analysis documents
â”‚   â”œâ”€â”€ e1_results.md                  # E1 analysis
â”‚   â”œâ”€â”€ e2_results.md                  # E2 analysis
â”‚   â”œâ”€â”€ e2_large_results.md            # E2-Large analysis
â”‚   â”œâ”€â”€ e3_results.md                  # E3 analysis
â”‚   â”œâ”€â”€ e3_retry_results.md            # E3-Retry analysis
â”‚   â”œâ”€â”€ e4_results.md                  # E4 analysis
â”‚   â”œâ”€â”€ e4_extension_results.md        # E4 Extension analysis
â”‚   â””â”€â”€ e5_results.md                  # E5 analysis (production validation)
â””â”€â”€ compare_metrics.py                 # Utility for comparing experiment metrics
```
