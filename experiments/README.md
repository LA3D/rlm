# Experiments

This directory contains experiments validating and comparing different approaches for ontology-related tasks using Language Models.

## Quick Navigation

- **[Agent Guide Generation](#agent-guide-generation-experiment)** - Compare 4 approaches for generating AGENT_GUIDE.md documentation
- **[Reasoning Chain Validation](#reasoning-chain-validation-experiment)** - Test PDDL-INSTRUCT style reasoning chains for SPARQL construction
- **[Reasoning Test (L3-L4)](#reasoning-test-experiment)** - Test if complex reasoning triggers delegation in RLM
- **Results Summary** - [PROV](#prov-results-small-ontology) | [UniProt](#uniprot-results-large-ontology)
- **Code** - [agent_guide_generation/](./agent_guide_generation/) | [reasoning_chain_validation/](./reasoning_chain_validation/) | [reasoning_test/](./reasoning_test/)

---

## Agent Guide Generation Experiment

**Goal:** Compare different LM approaches for generating ontology documentation that helps AI agents understand HOW to use an ontology (affordances), not just WHAT's in it (schema).

### TL;DR Results

**For small ontologies (< 50K chars):**
- ðŸ¥‡ **Scratchpad** - Most comprehensive (2x more detail), builds incrementally
- âš¡ **Direct LLM** - Fastest (30s), good enough if speed matters

**For large ontologies (> 100K chars):**
- ðŸ¥‡ **Scratchpad** - Only approach that handles full ontology without truncation
- ðŸ¥ˆ **DSPy ReAct** - Best affordances, efficient (68s)
- âŒ **Direct LLM** - Truncated input, incomplete results

### The Four Approaches

| Approach | Method | Best For | Why |
|----------|--------|----------|-----|
| **Direct LLM** | Full ontology in single prompt | Speed, small ontologies | Fast, simple, good affordances |
| **RLM** | DSPy RLM with code execution + bounded tools | Comprehensive coverage | Systematic exploration, but slow |
| **DSPy ReAct** | Structured tool calls (no code exec) | Balanced speed/quality | Good affordances, no REPL overhead |
| **Scratchpad** | Persistent namespace + direct functions | Large ontologies | Context management, incremental assembly |

### Architecture Spectrum

```
Rich Upfront Context â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Minimal Context
Tool-Free â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Tool-Heavy

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Direct LLM  â”‚  â”‚  Scratchpad  â”‚  â”‚  DSPy ReAct  â”‚  â”‚     RLM      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Full ontologyâ”‚  â”‚ Rich metadataâ”‚  â”‚ Stats only   â”‚  â”‚ Stats only   â”‚
â”‚ Single shot  â”‚  â”‚ Persistent nsâ”‚  â”‚ Pure tools   â”‚  â”‚ Heavy REPL   â”‚
â”‚ ~30s         â”‚  â”‚ Incremental  â”‚  â”‚ Structured   â”‚  â”‚ Unbounded    â”‚
â”‚              â”‚  â”‚ ~160s        â”‚  â”‚ ~70s         â”‚  â”‚ trajectory   â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚ ~100-145s    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                   â–²                  â”‚                  â”‚
     â”‚                   â”‚                  â”‚                  â”‚
     â””â”€â”€â”€ Rails â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€ Current RLM â”€â”€â”€â”˜
         Pattern                                  Pattern
```

---

## Results Details

### PROV Results (Small Ontology)

**Ontology:** W3C PROV provenance standard
**Size:** 112K chars (fits in Direct LLM prompt)

| Approach | Time | Lines | Quality |
|----------|------|-------|---------|
| **Scratchpad** ðŸ¥‡ | 161s | **383** | Most comprehensive, excellent structure |
| RLM | 145s | 241 | Good coverage, but "overwhelming" |
| DSPy ReAct | 73s | 221 | Efficient, 0 tool calls (!), good affordances |
| Direct LLM âš¡ | 31s | 189 | Fast, good enough |

**Key Finding:** Scratchpad builds 2x more comprehensive guides by using persistent namespace to incrementally assemble sections.

**Generated Guides:**
- [Direct LLM](./results/latest/prov_direct.md) - Quick reference
- [DSPy ReAct](./results/latest/prov_react.md) - Balanced
- [RLM](./results/latest/prov_rlm.md) - Comprehensive but dense
- [Scratchpad](./results/latest/prov_scratchpad.md) - Most detailed with clear structure

---

### UniProt Results (Large Ontology)

**Ontology:** UniProt protein knowledge base
**Size:** 139K chars (DOESN'T fit in Direct LLM - truncated to 50K)

| Approach | Time | Lines | Quality |
|----------|------|-------|---------|
| **Scratchpad** ðŸ¥‡ | 155s | **379** | Most comprehensive, excellent patterns |
| DSPy ReAct ðŸ¥ˆ | 68s | 201 | Best affordances (LLM judge winner) |
| RLM | 101s | 190 | Comprehensive but "overwhelming" |
| Direct LLM âŒ | 24s | **129** | **Truncated input** - incomplete |

**Critical Insight:** Direct LLM guide was **32% shorter** due to truncated input. Only Scratchpad handled full ontology effectively.

**LLM Judge Winner:** DSPy ReAct
> "Practical utility trumps comprehensiveness. DSPy ReAct wins by focusing on HOW to use the ontology effectively."

**Generated Guides:**
- [Direct LLM](./results/latest/uniprot_direct.md) - Incomplete (truncated)
- [DSPy ReAct](./results/latest/uniprot_react.md) - Winner: best affordances â­
- [RLM](./results/latest/uniprot_rlm.md) - Comprehensive reference
- [Scratchpad](./results/latest/uniprot_scratchpad.md) - Most complete coverage

**LLM Comparison:** [uniprot_comparison.md](./results/analysis/uniprot_comparison.md)

---

## Key Findings

### 1. Context Management Matters for Large Ontologies

**Hypothesis Validated:** When ontologies exceed prompt limits, LLM-driven context management with persistent scratchpad state wins.

- **Direct LLM fails** - Truncated to 50K chars, missing 64% of UniProt content
- **Scratchpad succeeds** - Agent strategically chunks exploration, builds incrementally
- **39 namespace variables** on UniProt - Active context management

### 2. Scratchpad Design Wins on Comprehensiveness

**Why Scratchpad produces 1.6-2x more detail:**
- Persistent namespace across iterations (variables survive)
- Uses `llm_query()` for sub-LLM analysis of chunks
- Builds sections incrementally, then assembles
- Lightweight history (20K truncation) vs RLM's unbounded trajectory

### 3. Affordances > Completeness

**From LLM judge on UniProt:**
- DSPy ReAct: "Best affordance design with clear when/why guidance"
- RLM: "Comprehensive but overwhelming for practical agent use"
- Verdict: **Practical utility > exhaustive enumeration**

### 4. DSPy ReAct is Surprisingly Efficient

Despite having tools available:
- **0 tool calls** on both PROV and UniProt
- Still outperformed RLM on affordances
- 2x faster than RLM (68s vs 101s on UniProt)
- **Structured approach** > tool usage

### 5. Original RLM Design Works Better Than Current

**Scratchpad model** (from original `rlm/core.py`):
- Persistent namespace (scratchpad semantics)
- Direct function calls (no tool wrappers)
- Lightweight history (truncated at 20K)
- Rich upfront context (metadata in namespace)

**Current RLM** (DSPy RLM):
- Per-iteration namespace
- Wrapped tools
- Unbounded trajectory accumulation
- Minimal upfront context

**Result:** Scratchpad outperforms current RLM on both speed and quality.

---

## Design Patterns Identified

### Rails Pattern (Load Everything Upfront)
**Used by:** Direct LLM, Scratchpad
**Approach:** Put rich context in prompt/namespace from start
**Wins:** Small ontologies, speed
**Fails:** Large ontologies (truncation)

### Progressive Disclosure (Bounded Tools)
**Used by:** RLM, DSPy ReAct
**Approach:** Minimal context, explore via tool calls
**Wins:** Scalability, works on any size
**Fails:** Tool overhead, may miss patterns

### Scratchpad Pattern (Incremental Assembly)
**Used by:** Scratchpad (original RLM design)
**Approach:** Rich metadata + persistent state + direct functions
**Wins:** Large ontologies, comprehensive output
**Fails:** Slower than Direct LLM on small ontologies

---

## Running the Experiments

### Quick Test
```bash
# Test single approach on PROV
python experiments/agent_guide_generation/test_single.py scratchpad prov

# Test all 4 approaches on PROV
python experiments/agent_guide_generation/run_comparison.py prov
```

### Full Comparison
```bash
# Run all approaches on both ontologies
python experiments/agent_guide_generation/run_comparison.py prov
python experiments/agent_guide_generation/run_comparison.py uniprot

# Results saved to experiments/results/runs/<timestamp>/
```

### Files Generated
- `{ontology}_AGENT_GUIDE_{approach}_{timestamp}.md` - Generated guide
- `{ontology}_{approach}_metadata_{timestamp}.json` - Runtime metrics
- `{ontology}_comparison_{timestamp}.md` - LLM judge comparison
- `{ontology}_all_comparison_{timestamp}.json` - Summary stats

---

## Code Organization

```
experiments/
â”œâ”€â”€ README.md                           # This file
â”‚
â”œâ”€â”€ agent_guide_generation/             # AGENT_GUIDE.md generation experiment
â”‚   â”œâ”€â”€ README.md                       # Detailed experiment docs
â”‚   â”œâ”€â”€ agent_guide_generation.py       # Direct LLM + RLM approaches
â”‚   â”œâ”€â”€ agent_guide_dspy_react.py       # DSPy ReAct approach
â”‚   â”œâ”€â”€ agent_guide_scratchpad.py       # Scratchpad approach (original RLM)
â”‚   â”œâ”€â”€ agent_guide_comparison_all.py   # Run all 4 approaches
â”‚   â”œâ”€â”€ test_dspy_react.py              # Quick ReAct test
â”‚   â””â”€â”€ test_scratchpad.py              # Quick Scratchpad test
â”‚
â”œâ”€â”€ reasoning_chain_validation/         # PDDL-INSTRUCT style reasoning chains
â”‚   â”œâ”€â”€ README.md                       # Full experiment design
â”‚   â”œâ”€â”€ exemplars/                      # Reasoning chain exemplars (L1-L5)
â”‚   â”‚   â””â”€â”€ uniprot_l2_crossref.md     # Level 2 example
â”‚   â”œâ”€â”€ rc_001_exemplar_impact.py      # E-RC-001: Exemplar impact
â”‚   â”œâ”€â”€ behavior_analysis.py           # Shared behavior analysis module
â”‚   â””â”€â”€ results/                        # Generated outputs
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ latest/                         # Symlinks to latest successful runs
    â”‚   â”œâ”€â”€ prov_direct.md -> ../runs/20260124_110501/...
    â”‚   â”œâ”€â”€ prov_react.md
    â”‚   â”œâ”€â”€ prov_rlm.md
    â”‚   â”œâ”€â”€ prov_scratchpad.md
    â”‚   â”œâ”€â”€ uniprot_direct.md
    â”‚   â”œâ”€â”€ uniprot_react.md
    â”‚   â”œâ”€â”€ uniprot_rlm.md
    â”‚   â””â”€â”€ uniprot_scratchpad.md
    â”‚
    â”œâ”€â”€ runs/                           # Timestamped experiment runs
    â”‚   â”œâ”€â”€ 20260124_102636/            # Initial PROV run
    â”‚   â”œâ”€â”€ 20260124_110501/            # Fixed scratchpad PROV
    â”‚   â””â”€â”€ 20260124_110847/            # UniProt run
    â”‚
    â””â”€â”€ analysis/                       # Summary analysis
        â”œâ”€â”€ prov_comparison.md
        â””â”€â”€ uniprot_comparison.md
```

---

## Recommendations

### For Ontology Documentation Generation

| Ontology Size | Recommended Approach | Reason |
|---------------|---------------------|---------|
| < 50K chars | Direct LLM or Scratchpad | Speed vs comprehensiveness tradeoff |
| 50-150K chars | **Scratchpad** | Handles full ontology, most comprehensive |
| > 150K chars | **Scratchpad** | Only approach that can chunk strategically |

### For Query Construction Tasks

*(Future experiments needed to validate)*

**Hypothesis:** Scratchpad model should excel when:
- Ontology is large (> 100K chars)
- Need to explore multiple SHACL examples
- Building complex queries incrementally
- Context management is critical

### For Current RLM Implementation

**Consider:** Incorporating scratchpad patterns:
- Add persistent namespace option
- Provide direct function access (not just tool wrappers)
- Implement history truncation (configurable)
- Rich upfront context injection

---

## Related Documentation

- **Original RLM design:** `rlm/core.py` (claudette-based)
- **Current RLM design:** `rlm_runtime/engine/dspy_rlm.py` (DSPy-based)
- **Comparison analysis:** `docs/analysis/rails-doc-writer-pattern.md`
- **Architecture decisions:** `CLAUDE.md` - Project Context

---

---

## Reasoning Chain Validation Experiment

**NEW (2026-01-26)** - Validating PDDL-INSTRUCT style reasoning chains for SPARQL query construction.

### Research Question

**Do explicit reasoning chain exemplars improve SPARQL query construction?**

Based on the PDDL-INSTRUCT paper (arXiv:2509.13351v1), which achieved 28% â†’ 94% accuracy on planning tasks through:
- Decomposed state-action-state reasoning
- External verification
- Detailed feedback (not just binary pass/fail)

### Four Experiments

| Experiment | Question | Design |
|------------|----------|--------|
| **E-RC-001** | Do reasoning chains help? | Baseline vs 3/5 exemplars |
| **E-RC-002** | Which architecture for state tracking? | ReAct vs RLM vs Scratchpad vs Structured-State |
| **E-RC-003** | Does detailed feedback help? | None vs Binary vs Detailed |
| **E-RC-004** | Can we detect good reasoning? | Behavior analysis validation |

### Quick Start

```bash
# Run E-RC-001 with DSPy RLM implementation
python experiments/reasoning_chain_validation/rc_001_with_rlm.py --condition all

# Run single condition
python experiments/reasoning_chain_validation/rc_001_with_rlm.py --condition exemplar3

# Test behavior analysis
python experiments/reasoning_chain_validation/behavior_analysis.py

# Load exemplars into memory backend
python scripts/load_exemplars.py \
    --exemplar-dir experiments/reasoning_chain_validation/exemplars \
    --db-path memory.db \
    --ontology uniprot \
    --stats
```

### E-RC-001 Results (2026-01-27)

**Implementation:** DSPy RLM with verification feedback + curriculum-aware retrieval

**Test Conditions:**
- **baseline**: No exemplars, no schema (stats only)
- **schema**: Schema in context via AGENT_GUIDE.md
- **exemplar3**: L1-L3 exemplars + curriculum retrieval

| Condition | Convergence | Avg Iterations | Avg Reasoning Quality |
|-----------|-------------|----------------|---------------------|
| baseline | 3/3 (100%) | 6.7 | 0.52 |
| schema | 3/3 (100%) | 6.7 | **0.59** |
| exemplar3 | 3/3 (100%) | 7.0 | 0.48 |

**Key Findings:**
1. âœ… **System is functional** - All conditions achieved 100% convergence
2. âœ… **Verification feedback working** - Domain/range checks visible in traces
3. âœ… **State tracking adopted** - Strong scores (0.67-1.0) across all runs
4. âœ… **Schema metadata valuable** - Schema condition outperformed others (0.59 quality)
5. âš ï¸ **Exemplar impact unclear** - Need more exemplars (L3-L5) and harder tasks

**Implementation Status:**
- âœ… Phase 1: Foundation modules (exemplar_loader, verification_feedback, curriculum_retrieval)
- âœ… Phase 2: Interpreter enhancement with verification feedback
- âœ… Phase 3: DSPy RLM integration (CoT features, curriculum retrieval)
- âœ… Phase 4: Experiment integration (rc_001_with_rlm.py, load_exemplars.py)
- âœ… All 57 tests passing

**Next Steps:**
- Create L3-L5 exemplars for more complex query patterns
- Test on ontologies with instance data (not just schema)
- Design harder tasks (multi-hop joins, aggregations, complex filters)

**Results:** [experiments/reasoning_chain_validation/results/comparison_summary.md](./reasoning_chain_validation/results/comparison_summary.md)

### Behavior Analysis

The key innovation is analyzing reasoning traces for PDDL-INSTRUCT style indicators:

| Indicator | What it Measures |
|-----------|-----------------|
| **State tracking** | Does agent track discovered classes/properties? |
| **Verification** | Does agent verify constraints before/after? |
| **Anti-pattern avoidance** | Does agent avoid known mistakes? |
| **Step-by-step** | Is reasoning clearly structured? |

### Success Criteria

| Experiment | Success if... |
|------------|---------------|
| E-RC-001 | Exemplar-5 > 20% better pass rate than Baseline |
| E-RC-002 | One architecture clearly outperforms on state tracking |
| E-RC-003 | Detailed feedback > 25% better correction rate |
| E-RC-004 | Behavior analysis achieves F1 > 0.8 |

**If 3/4 succeed**, the reasoning chains approach is validated for investment in ReasoningBank integration.

### Files

```
reasoning_chain_validation/
â”œâ”€â”€ README.md                      # Full experiment design
â”œâ”€â”€ exemplars/                     # Reasoning chain exemplars
â”‚   â””â”€â”€ uniprot_l2_crossref.md    # Level 2 example
â”œâ”€â”€ rc_001_exemplar_impact.py      # E-RC-001 runner
â”œâ”€â”€ behavior_analysis.py           # Shared analysis module
â””â”€â”€ results/                       # Generated outputs
```

### Related Documents

- [ontology-kr-affordances-for-llm-reasoning.md](../docs/design/ontology-kr-affordances-for-llm-reasoning.md)
- [instruction-tuning-via-reasoning-chains.md](../docs/design/instruction-tuning-via-reasoning-chains.md)

---

## Reasoning Test Experiment

**Goal:** Determine if L3-L4 reasoning complexity triggers strategic delegation (`llm_query`) in RLM, or if tool-first pattern remains universal.

### Background

L1-L2 tests showed RLM uses a "tool-first" pattern:
- **0 delegation attempts** on simple queries
- Direct SPARQL construction via bounded tools
- AGENT_GUIDE.md provides sufficient scaffolding
- Cost: $0.11-0.13 per query (52% cheaper than ReAct)

**Question:** Does this pattern scale to complex reasoning?

### Test Queries (L3-L4 Complexity)

| ID | Level | Query | Reasoning Challenge |
|----|-------|-------|-------------------|
| L3-1 | Multi-entity | "Find reviewed human proteins with kinase activity" | Coordinate 4 concepts + GO hierarchy |
| L3-2 | Multi-hop | "What diseases involve enzymes in mitochondria?" | 2 annotation paths + transitive hierarchy |
| L4-1 | Spatial | "Find diseases from variants in active sites" | Position overlap reasoning with FALDO |
| L3-3 | Comparison | "How do human vs mouse proteins differ?" | Cross-organism aggregation |
| L4-2 | Integration | "Find proteins with disease + drug targets + membrane-bound" | 3+ constraint coordination |

### Metrics

| Metric | Target | Interpretation |
|--------|--------|----------------|
| **Delegation rate** | â‰¥2/5 queries | Delegation emerges for complex reasoning |
| **Cost** | < $0.25/query | Remains cheaper than ReAct baseline |
| **Convergence** | â‰¥80% | Reliable within budget |

### Expected Outcomes

**Scenario A: Delegation emerges** âœ…
- L3-L4 triggers `llm_query()` for disambiguation/validation
- Cost increases 20-40% but still cheaper than ReAct
- Document delegation patterns for production use

**Scenario B: Tool-first continues** âšª
- AGENT_GUIDE.md + tools handle L3-L4 directly
- Cost remains low ($0.15-0.20/query)
- Accept tool-first as optimal for RDF domain

**Scenario C: Mixed results** ðŸ”€
- Delegation for specific challenges (spatial, GO terms)
- Not needed for multi-entity coordination
- Document when delegation helps

### Files

```
reasoning_test/
â”œâ”€â”€ README.md                      # Full experiment design
â”œâ”€â”€ run_reasoning_test.py          # Test runner (5 queries)
â”œâ”€â”€ analyze_trajectory.py          # Trajectory visualization tool
â””â”€â”€ results/                       # Logs and summaries
```

### Related Analysis

- [rlm-behavior-l1-l2-queries.md](../docs/analysis/rlm-behavior-l1-l2-queries.md) - L1-L2 baseline results
- [rlm-execution-deep-dive.md](../docs/analysis/rlm-execution-deep-dive.md) - Technical execution details
- [rlm-system-behavior-summary.md](../docs/analysis/rlm-system-behavior-summary.md) - System overview

---

## Future Experiments

Potential areas to explore:

1. ~~**Query Construction** - Test scratchpad vs RLM on actual query construction tasks~~ â†’ **E-RC-002**
2. **Memory Retrieval** - Compare approaches for procedural memory retrieval
3. **SHACL Example Usage** - How do approaches leverage 1,228 UniProt examples?
4. **Scalability Tests** - Test on Schema.org (1MB), Wikidata ontology
5. **Hybrid Approaches** - Combine Direct LLM + Scratchpad refinement
6. **ReasoningBank Integration** - Store/retrieve reasoning chains (pending E-RC validation)

---

## Citation

If using these experiments, reference the comparison methodology:
```
4 approaches tested on 2 ontologies (PROV 112K, UniProt 139K):
- Direct LLM: Single prompt with full ontology
- RLM: DSPy RLM with bounded tools + code execution
- DSPy ReAct: Structured tool calls without REPL
- Scratchpad: Persistent namespace + direct functions (original rlm/core.py design)

Key finding: Scratchpad model wins on large ontologies through strategic
context management and incremental assembly.
```
